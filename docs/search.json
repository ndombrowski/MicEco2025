[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Microbial Ecology: Introduction into Data Analysis",
    "section": "",
    "text": "Nina Dombrowski\n\n\nInstitute for Biodiversity and Ecosystem Dynamics (IBED), University of Amsterdam (UVA)\n\n\nn.dombrowski@uva.nl\n\n\n\nIntroduction\nIn this tutorial, you will learn how to work on the command line and use a High-Performance Computing (HPC) environment. This will allow you to analyse 16S long-reads and turn this data into a count table of microbial taxa.\nThis course is designed for students in microbial ecology with little or no prior experience using the command line. During this tutorial, you will learn how to:\n\nInstall a Terminal\nDocument your Code\n\nDecide what software to use for code documentation\nHow to use markdown and document workflows\n\nNavigate the Command Line\n\nNavigate the filesystem (cd, ls, pwd)\nWork with files (mkdir, cp, mv, rm, cat, head, less)\nSearch and filter data (grep, wc)\nView and underrstand FASTQ files (zcat, wildcards)\n\nUnderstand the setup of an HPC\nWork with an HPC\n\nConnect to an HPC (SSH)\nUnderstand job schedulers (e.g., SLURM)\nRun command-line tools (e.g., fastqc, seqkit)\nSet up and activating conda environments\nRun workflows with Snakemake\n\nAnalyse long-read data\n\nPerform quality control\nCluster individual reads into consensus sequences\nPerform taxonomic classification\nGenerate a count table",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "source/terminal.html",
    "href": "source/terminal.html",
    "title": "1  Setting up a terminal",
    "section": "",
    "text": "1.1 Terminology\nThe command-line interface (CLI) is an alternative to a graphical user interface (GUI), with which you are likely more familiar. Both allow you to interact with your computer’s operating system but in a slightly different way:\nThe CLI is commonly called the shell, terminal, console, or prompt. These terms are related but not identical:\nThere are several types of shells — for example, bash or zsh (that use slightly different languages to issue commands). This tutorial was written on a computer that uses bash, which stands for Bourne Again Shell.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#terminology",
    "href": "source/terminal.html#terminology",
    "title": "1  Setting up a terminal",
    "section": "",
    "text": "In a GUI, you click buttons, open folders, and use menus\nIn the CLI, you type text to issue commands and see text output in the terminal\n\n\n\nThe terminal (or console) is the window or program that lets you type commands.\nThe shell is the program that interprets the commands you type inside the terminal and tells the operating system what to do.\nA prompt is the text displayed by the shell that indicates that it is ready to accept a command. The prompt often shows useful information, like your username, machine name, and current directory. This is one example for how a prompt can look like: user@machine:~$",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#installation-guides",
    "href": "source/terminal.html#installation-guides",
    "title": "1  Setting up a terminal",
    "section": "1.2 Installation guides",
    "text": "1.2 Installation guides\n\n1.2.1 Linux\nIf you’re using Linux, you already have everything you need and you don’t need to install anything. All Linux systems come with a terminal and a shell, and the default shell is usually Bash.\nYou can open a terminal from your applications menu or by searching for Gnome Terminal, KDE Konsole, or xterm, depending on your desktop environment.\nTo confirm which shell you’re using, type:\n\necho $SHELL\n\n\n\n1.2.2 Mac\nAll Mac computers also come with a built-in terminal and shell. To open the terminal:\n\nIn Finder, go to Go → Utilities, then open Terminal.\nOr use Spotlight Search (⌘ + Space), type Terminal, and press Return.\n\nThe default shell depends on your macOS version:\n\nmacOS Mojave (10.14) or earlier → Bash\nmacOS Catalina (10.15) or later → Zsh\n\nCheck which shell you’re currently using:\n\necho $SHELL\n\n\n\n1.2.3 Windows\nUnlike macOS or Linux, Windows doesn’t include a Unix-style terminal by default, so you will need to install one of the following:\n\nMobaXterm (recommended)\n\nProvides a terminal with Linux-like commands built-in\nInstallation guide: MobaXterm setup instructions\nEasiest option for beginners and lightweight to install\n\nWindows Subsystem for Linux (WSL2)\n\nGives you a full Linux environment directly on Windows\nRecommended if you’re comfortable installing software or already have some command-line experience\nUses Ubuntu by default, which includes Bash and all standard Linux tools\nInstallation guide:Microsoft WSL install instructions\n\n\nOnce installed, open your terminal (MobaXterm or Ubuntu via WSL2) and verify that Bash is available:\n\necho $SHELL",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#sanity-check",
    "href": "source/terminal.html#sanity-check",
    "title": "1  Setting up a terminal",
    "section": "1.3 Sanity check",
    "text": "1.3 Sanity check\nAfter setup, open your terminal and type echo $SHELL and press enter. You should see something like this:\n\nIf you see a prompt ending in $ (for example user@machine:~$), your shell is ready to go and you are all set to follow along with the tutorial!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/documentation.html",
    "href": "source/documentation.html",
    "title": "2  Documenting your code",
    "section": "",
    "text": "2.1 Choosing your editor\nDocumenting your code is crucial for both your future self and anyone else who might work with your code. Good documentation helps others (and your future self) understand the purpose, functionality, and usage of your scripts. You want to document your code in the same way that you would write a detailed lab notebook.\nFor more in-depth guidance, see A Guide to Reproducible Code in Ecology and Evolution. While examples are mainly in R, the principles are general and apply across programming languages.\nTo see how code documentation looks like in a real-world context, have a look at an example workflow I wrote for a previous analysis. It shows how to describe each analysis step, software dependency, and command used — in a way that allows someone else to reproduce the entire analysis from scratch and for me to remember what I did a month later:\nThis workflow documents a 16S rRNA amplicon analysis from Winogradsky columns, including:\nEven if you don’t understand all commands yet, notice how it reads almost like a lab notebook for code — each step has context, rationale, and expected results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/documentation.html#choosing-your-editor",
    "href": "source/documentation.html#choosing-your-editor",
    "title": "2  Documenting your code",
    "section": "",
    "text": "2.1.1 Plain text editor\nAvoid visual editors like Word, as they are not designed for code and can change syntax by for example replacing backticks (`) with apostrophes (').\nOne of the easiest solutions is to use a plain text editor, such as:\n\nTextEdit (Mac)\nNotepad (Windows)\n\nThese editors allow you to write and save code safely, but they lack advanced features like syntax highlighting or integrated code execution.\n\n\n2.1.2 Rmarkdown in RStudio\nRMarkdown combines plain text, code, and documentation in one document. You can write your analysis and explanatory text together, then “knit” the document to HTML, PDF, or Word.\nTo create an RMarkdown file in RStudio:\n\nGo to File → New File → R Markdown\nChoose a title, author, and output format\nWrite your code and text\nClick Knit to render the document\n\nMore info: RMarkdown tutorial\n\n\n2.1.3 Quarto in Rstudio\nQuarto is a next-generation alternative to RMarkdown. It supports R, Python, and other languages, and offers more output formats and customization options. This software was used to generate this tutorial.\nTo create a Quarto document:\n\nGo to File → New File → Quarto Document\nChoose a title, author, and output format\nClick Render to generate your document\n\nMore info: Quarto documentation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/documentation.html#markdown-for-documentation",
    "href": "source/documentation.html#markdown-for-documentation",
    "title": "2  Documenting your code",
    "section": "2.2 Markdown for Documentation",
    "text": "2.2 Markdown for Documentation\nMarkdown is a lightweight language for formatting text in your plain text, Rmarkdown or Quarto document. You can easily add headers, lists, links, code, images, and tables. You can use markdown inside a plain text editor or inside a Rmarkdown or Quarto document.\nHeaders:\nUse # to add a header and separate different sections of your documentation. The more # symbols you use after each other, the smaller the header will be. When writing a header make sure to always put a space between the # and the header name:\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers do not have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks followed by the computational language, i.e. bash or r, used.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nYou can easily add links to external resources as follows:\n[Link Text](https://www.example.com)\nEmphasis:\nYou can use * or _ to write italic and ** or __ for bold text.\n*italic*\n**bold**\nPictures\nYou can also add images to your documentation as follows:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/image.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/cli.html",
    "href": "source/cli.html",
    "title": "3  Navigating the command line",
    "section": "",
    "text": "3.1 pwd: Find out where we are\nOnce your terminal is open, you can get yourself oriented by typing your first command and then pressing enter:\npwd\nThe command pwd stands for print working directory. It tells you where you currently are in the file system, that is, which folder (directory) your shell is operating on right now. You should see something like:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#pwd-find-out-where-we-are",
    "href": "source/cli.html#pwd-find-out-where-we-are",
    "title": "3  Navigating the command line",
    "section": "",
    "text": "/Users/YourUserName\n\n\n\n\n\n\nTipTip: Finding the Desktop on Different Systems\n\n\n\n\n\nYour home directory path varies slightly across operating systems. Here’s how to locate yourself and connect to familiar locations like the Desktop:\nmacOS\n\nYour home directory is /Users/YourUserName\nTo open the folder you are currently in Finder:open .\nYour desktop is at /Users/YourUserName/Desktop\n\nMobaXterm (Windows)\n\nYour home directory is /home/mobaxterm\nBy default, this directory is temporary and is deleted when you close MobaXterm (when using the portable version). To make it permanent:\n\nGo to Settings –&gt; Configuration –&gt; General\nUnder Persistent home directory, choose a folder of your choice\n\nTo open the folder you are currently in the Windows File explorer: explorer.exe .\nYour Desktop is usually at: /mnt/c/Users/YourUserName/Desktop or /mnt/c/Users/YourUserName/OneDrive/Desktop (when using OneDrive)\n\nWSL2 (Windows)\n\nYour home directory is/home/YourUserName\nTo open the folder you are currently in the Windows File explorer: explorer.exe .\nYour Desktop is usually at: /mnt/c/Users/YourUserName/Desktop or /mnt/c/Users/YourUserName/OneDrive/Desktop (when using OneDrive)\n\nIf you want to access the Uva OneDrive folder and/or if your OneDrive folder name includes spaces (like OneDrive - UvA), use quotes around the path:\n\ncd \"/mnt/c/Users/YourUserName/OneDrive - UvA\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#ls-list-the-contents-of-a-directory",
    "href": "source/cli.html#ls-list-the-contents-of-a-directory",
    "title": "3  Navigating the command line",
    "section": "3.2 ls: List the contents of a directory",
    "text": "3.2 ls: List the contents of a directory\nNow that we know where we are, let’s find out what is inside that location, i.e. what files and folders can be found there. The command ls (short for list) shows the files and folders in your current directory. Type the following and press enter:\n\nls\n\nYou should see something like this (your output will vary depending on what’s in your directory):\n\nThe colors and formatting depend on your terminal settings, but typically:\n\nFolders (directories) appear in one color (often green or blue)\nFiles appear in another (often white or bold)\n\nIf your directory contains many items, the output can quickly become overwhelming. To make sense of it, we can use options and arguments to control how commands behave.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#the-structure-of-a-command",
    "href": "source/cli.html#the-structure-of-a-command",
    "title": "3  Navigating the command line",
    "section": "3.3 The structure of a command",
    "text": "3.3 The structure of a command\nA command generally has three parts:\n\nA command name: The program you want to run, i.e. ls\nAn option (or flag): A way to modify how the command behaves, i.e -l (long format)\nAn optional argument: The input, i.e. a file or folder\n\n\nTry the following command in your current directory to “List (ls) the contents of the current folder and show details in long format (-l)”:\n\nls -l\n\nAfter running this you should see a more detail list of the contents of your folder.In the example below we can see that we now print additional information about who owns the files (i.e. access modes), how large the files are, when they were last modified and of course the name:\n\n\n\n\n\n\n\nTipTip: Using ls in practice\n\n\n\n\n\nThroughout this tutorial, you’ll notice that we use ls frequently. There’s a reason for that:\nWhen working with data, sanity checks are essential, because it is easy to make mistakes, overwrite files, or lose track of where things are. Using simple commands like ls, pwd,wc, orgrep` helps you verify what’s happening to your files at every step of an analysis workflow.\nThese habits are not just for beginners, also experienced bioinformaticians rely on them constantly. For newcomers, practicing these checks early will help you build confidence and intuition when working on the command line.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#getting-help",
    "href": "source/cli.html#getting-help",
    "title": "3  Navigating the command line",
    "section": "3.4 Getting help",
    "text": "3.4 Getting help\nAt some point, you’ll want to know what options a command has or how it works. In this case, you can always check the manual pages (or man pages) by typing man followed by the command name:\n\nman ls\n\nThis opens the manual entry for the command ls. You can scroll through it using:\n\n↑ / ↓ arrows or the space bar to move down\nb to move back up\nq to quit the manual\n\nNot all commands come with such a manual. Depending on the program, there are a few common patterns you can try to get help:\n\nman ls\nls --help\nls -h",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#mkdir-make-a-new-folder",
    "href": "source/cli.html#mkdir-make-a-new-folder",
    "title": "3  Navigating the command line",
    "section": "3.5 mkdir: Make a new folder",
    "text": "3.5 mkdir: Make a new folder\nBefore we start moving around, let’s first learn how to create new folders (also called directories). This is something we will do often, for example, to keep raw data, results, and scripts organized in separate places. The command we use for that is mkdir, which stands for make directory.\nFor now, we will use mkdir to create a project folder with the name data_analysis for this tutorial. Don’t worry about how to move into the folder yet, we’ll cover that in the next section.\n\n# Move into the home directory (the starting point of your system)\ncd ~\n\n# Create a new folder called 'data_analysis'\nmkdir data_analysis\n\n# Check that the folder was created successfully\nls\n\nYou should see a new folder called data_analysis appear in the list. We will use this folder as our project folder for all exercises in this tutorial. Next, let’s make a new data folder inside the new data_analysis folder by typing the following:\n\n# Make a data folder inside the data_analysis folder\n# The `-p` option makes the parent directory (here: data), if it does not already exist\n# It is useful to by default add `-p` when generating a folder inside a folder\nmkdir -p data_analysis/data\n\n# Check that the folder was created successfully\n# Notice here, how we use ls with a flag and also with an optional argument?\nls -l data_analysis\n\n\n\n\n\n\n\nTipTip: Commenting your code\n\n\n\n\n\nNotice how we added # and some notes above each command?\nAnything written after # in Bash is a comment. A comment won’t be executed, but it helps you (and others) understand what the command does.\nIn your own work, add short, meaningful comments above key steps. Avoid restating the obvious, instead, explain why you’re doing something or what it achieves.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#cd-move-around-folders",
    "href": "source/cli.html#cd-move-around-folders",
    "title": "3  Navigating the command line",
    "section": "3.6 cd: Move around folders",
    "text": "3.6 cd: Move around folders\nNow that we have our own project folder, let’s learn how to move around the file system.\nThe file system is structured like a tree that starts from a single root directory (that is also denoted as / in bash). All other folders branch out from this root directory. For example, we can go from the root directory, to the users folder and from there into the john folder.\n\nThere are two ways to specify a path to go the the portfolio folder:\n\nAbsolute path: starts from the root (e.g. cd /users/john/portfolio)\nRelative path: starts from your current location (e.g. cd portfolio if you’re already in /users/john)\n\nIt is generally recommended to use the relative path from inside your project directory. That makes your code more portable and still allows you to run the code even if your computer setup changes.\nLet’s practice moving between folders (at each step, use pwd in case you feel that you get lost):\n\n# Move into the data analysis folder \ncd data_analysis \n\n# Check where we are \npwd\n\nWe now should see that we are in something like /Users/Name/data_analysis. We can use the cd command in multiple ways to move around:\n\n# Move into the data folder\ncd data\n\n# Move one level up (..), i.e. go back to the data_analysis folder\ncd ..\n\n# Move multiple levels down at once\ncd data_analysis/data\n\n# Move two levels up\ncd ../.. \n\n# Quickly go back home\ncd ~\n\n# And go back to the data_analysis folder \ncd data_analysis\n\nIn the code above, the tilde symbol (~) is a shortcut for your home directory. It’s equivalent to typing the full absolute path to your home (e.g. cd /Users/YourName) but it is much faster to type.\n\n\n\n\n\n\nTipTip: Command-line completion\n\n\n\n\n\nHere, are some other tips for faster navigation (and less typos):\n\nUse Tab for autocompletion: type the first few letters of a folder name and press Tab.\nIf there’s more than one match, press Tab twice to see all options.\nUse ↑ / ↓ arrows to scroll through previously entered commands\n\nHint: From now on try to use the Tab key once in a while so that you do not have to write everything yourself all the time.\n\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\nFamiliarize yourself with these first commands and:\n\nCreate a new folder inside your data_analysis directory called results\nMove into the results folder and confirm your location with pwd\nMove back inside the data_analysis folder\nUse ls to confirm both results and data are there\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\nmkdir results \ncd results \npwd \ncd ..\nls",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#wget-download-data",
    "href": "source/cli.html#wget-download-data",
    "title": "3  Navigating the command line",
    "section": "3.7 wget: Download data",
    "text": "3.7 wget: Download data\nNext, let’s download a fasta file that contains a 16S sequence to go through some other useful commands. We can use the wget command to fetch a fasta file from an online website as follows:\n\n# Download the example fasta file into the current directory\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fasta.gz\n\n\n\n\n\n\n\nTipTip: Where does our data come from?\n\n\n\n\n\nThe files comes from the Plant-associated bacterial culture collections database. This database contains information from microbial strains isolated from the roots and leaves of Arabidopsis thaliana and from roots and nodules from Lotus japonicus. Some of these strains are also used in your lab practicals.\nFor convenience (and security reasons) you will for this tutorial will download the data from Github. However, in practice you will often download data from institutional websites such as the one above.\n\n\n\nAfter you download data, it is always a good idea to do some sanity check to see if the file is present and know how large it is:\n\n# List files in long (-l) and human-readable (-h) format\n# combining these two commands becomes -lh\nls -lh LjRoot303.fasta.gz",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#cp-copy-files",
    "href": "source/cli.html#cp-copy-files",
    "title": "3  Navigating the command line",
    "section": "3.8 cp: Copy files",
    "text": "3.8 cp: Copy files\ncp duplicates files or directories. It is a useful feature to keep our files organized and not have every single file in a single folder but instead to organize your files into folder categories (useful folders can be: data, scripts and results).\nLet’s use cp to copy the downloaded file into data and to organize the data a bit better:\n\n# Copy the file into data \ncp LjRoot303.fasta.gz data/\n\n# Show the content of both locations\nls -l\nls -l data\n\nWhen running the two ls commands, we see that we now have two copies of LjRoot303.fasta.gz, one file is in our working directory and the other one is in our data folder. Having large files in multiple locations is not ideal since we will use unnecessary space. However, we can use another command to move the file into our data folder instead of copying it.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#mv-move-or-rename-files",
    "href": "source/cli.html#mv-move-or-rename-files",
    "title": "3  Navigating the command line",
    "section": "3.9 mv: Move (or rename) files",
    "text": "3.9 mv: Move (or rename) files\nmv moves or renames files without creating a second copy:\n\n# Move the file into data\nmv LjRoot303.fasta.gz data/\n\n# Verify\nls -l\nls -l data\n\nNotice that mv will move a file and, without asking, overwrite the existing file we had in the data folder when we ran cp. This means that if you run mv its best to make sure that you do not overwrite files by mistake.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#rm-remove-files-and-directories",
    "href": "source/cli.html#rm-remove-files-and-directories",
    "title": "3  Navigating the command line",
    "section": "3.10 rm: Remove files and directories",
    "text": "3.10 rm: Remove files and directories\nTo remove files and folders, we use the rm command. For example we could remove the LjRoot303.fasta.gz in case we don’t need it anymore:\n\n# Remove the fasta file from the data folder\nrm data/LjRoot303.fasta.gz\n\n# Check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an option. To do this, we use -r , which allows us to remove directories and their contents recursively.\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone. Therefore, use rm with care and check what you write twice before pressing enter!\nAlso, NEVER run rm -r or rm -rf on the root / folder or any important path. And yes, we have seen recommendations to use these combinations online. Therefore, always double check the path or file name before pressing enter when using the rm command.\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\nDownload another genome:\n\nDownload another fasta file (not LjRoot303) from this url: https://github.com/ndombrowski/MicEco2025/tree/main/data. To get the link,\n\nClick on the genome you want to download\nRight click on view raw and select copy link\nUse wget to download that file\n\nMake sure that the new genome stored in the data folder\nCheck the file size (hint: use the -h option with ls)\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Download the file\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/GCF_000714595.fasta.gz\n\n# Move the file to the data folder\nmv GCF_000714595.fasta.gz data\n\n# Check the file size\nls -lh data/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#gzip-uncompressing-files",
    "href": "source/cli.html#gzip-uncompressing-files",
    "title": "3  Navigating the command line",
    "section": "3.11 gzip: (Un)compressing files",
    "text": "3.11 gzip: (Un)compressing files\nYou might have noticed that the file we downloaded ends with .gz. This extension is used for files that are compressed to make the file smaller. This is useful for saving files, but this makes the file unreadable for a human. To be able to learn how to read the content of a file, let’s learn how to uncompress the file first.\n\n# Download another genome  \n# Here, we use the -P option to directly download the file into the data folder \nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fasta.gz -P data\n\n# Check that this worked \n# You now should see two files\nls data\n\n# Uncompress the file with gzip \n# Here, we use -d to decompress the file \n# (by default gzip will compress a file)\ngzip -d data/LjRoot303.fasta.gz \n\n# Check that this worked \n# We now should see that one file is uncompressed (i.e. it lost the gz extension)\nls data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#exploring-file-contents",
    "href": "source/cli.html#exploring-file-contents",
    "title": "3  Navigating the command line",
    "section": "3.12 Exploring file contents",
    "text": "3.12 Exploring file contents\nNow that we have the uncompressed file we can use different ways to explore the content of a file.\n\n3.12.1 cat: Print the full file\nWe can use cat print the entire file to the screen, this is fine for short files, but overwhelming for long ones where we would see hundreds of line just flying over the screen.\n\ncat data/LjRoot303.fasta\n\n\n\n3.12.2 head and tail: View parts of a file\nTo only print the first few or last few lines we can use the head and tail commands:\n\n# Show the first 10 lines\nhead data/LjRoot303.fasta\n\n# Show the last 5 lines\n# Here, the option -n allows us to control how many lines get printed\ntail -n 5 data/LjRoot303.fasta\n\n\n\n3.12.3 less: View the full file\nless let’s you view a file’s contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless -S data/LjRoot303.fasta\n\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\n\n\n3.12.4 zcat: Decompress and print to screen\nWhen we used gzip above, we decompressed the file and that allowed us to read the content of the fasta file. This is perfect for smaller files, but not ideal for sequence data since these files get large and we might not want to decompress these files as they would clutter our system.\nLuckily, there is one useful tool in bash to decompress the file and print the content to the screen. zcat will print the content of a file to the screen but leave the file as is.\n\n# Check the content of the data folder \n# We should have one compressed and one uncompressed file \nls data\n\n# Use zcat on the compressed file \nzcat data/GCF_000714595.fasta.gz\n\n# Check the content of the data folder \n# We still should have one compressed and one uncompressed file \nls data\n\n\n\n\n\n\n\nTipTip: Editing text files with nano\n\n\n\n\n\nYou can also edit the content of a text file and there are different programs available to do this on the command line, one such tool is nano, which should come with most command line interpreters. You can open any file as follows:\n\nnano data/LjRoot303.fasta\n\nOnce the document is open you can edit it however you want and then\n\nClose the document with control + X\nType y to save changes and press enter",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#wc-count-things",
    "href": "source/cli.html#wc-count-things",
    "title": "3  Navigating the command line",
    "section": "3.13 wc: Count things",
    "text": "3.13 wc: Count things\nAnother useful tool is the wc (short for wordcount) command that allows us to count the number of lines via -l in a file. It is an useful tool for sanity checking.\n\nwc -l data/LjRoot303.fasta\n\nWe see that this file does contain 20 lines of text. For fasta files this is not very informative, but if you for example work with a dataframe with rows and columns where you filter rows using certain conditions this can become very useful for sanity checking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#grep-print-lines-that-match-patterns",
    "href": "source/cli.html#grep-print-lines-that-match-patterns",
    "title": "3  Navigating the command line",
    "section": "3.14 grep : print lines that match patterns",
    "text": "3.14 grep : print lines that match patterns\nThe grep command searches for patterns in a file. We could for example use this to ask how many 16S rRNA gene sequences are in our fasta file by searching for the fasta header that always starts with a &gt;. The basic grep syntax is grep \"pattern\" file. Here, “pattern” is the text you want to search for, and file is the file you want to search in. The double quotes allow us to include special characters (like &gt;) safely.\n\ngrep \"&gt;\" data/LjRoot303.fasta\n\nThis prints all lines that match the pattern (here: &gt;). In a fasta file, &gt; is used to label the sequence headers. Depending on your terminal, the matches might be highlighted, which makes them easy to spot.\nIf we only care about the number of contigs, we can use the -c option:\n\n# Root303 contains 1 16S rRNA gene sequence\ngrep -c \"&gt;\" data/LjRoot303.fasta\n\n\n\n\n\n\n\nTipThe structure of a fasta file\n\n\n\n\n\nA sequence FASTA file is a text based format to store DNA or peptide sequences. It should always look something like this:\n\nThe header always starts with a &gt; followed by descriptive information. In a new line the sequence data gets stored in either a single line or multiple lines. A fasta file can continue single sequence or multiple sequences.\nBy convention the extensions .fna is used to store fasta sequences from nucleotides and .faa is used to store fasta sequences from proteins. The extension .fasta can contain either nucleotide or protein sequences.\n\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\n\nUncompress the file for GCF_000714595\nCompare the number of 16S rRNA gene sequences for GCF_000714595 and LjRoot303\nWhat does it mean if one genome has more 16S rRNA gene sequences than another?\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Uncompress \ngzip -d data/GCF_000714595.fasta.gz\n\n# Count the number of contigs \n# We work with genomes with 1 and 9 16S rRNA gene sequences\ngrep -c \"&gt;\" data/LjRoot303.fasta\ngrep -c \"&gt;\" data/GCF_000714595.fasta\n\nAnswer for question 3:\nMany bacteria carry multiple copies of the rRNA operon (16S–23S–5S). A higher copy number is often associated with fast-responding, fast-growing life strategies (more ribosomes → faster protein synthesis under rich conditions).\nThis can cause problems in, for example, 16S amplicon analyses:\n\na single organism may produce multiple distinct 16S sequences, inflating apparent diversity\nspecies with higher 16S copy number will produce more amplicon reads per cell. So raw read counts overestimate their true cell abundance unless you correct for copy number",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#working-with-multiple-files",
    "href": "source/cli.html#working-with-multiple-files",
    "title": "3  Navigating the command line",
    "section": "3.15 Working with multiple files",
    "text": "3.15 Working with multiple files\nSo far, we’ve worked with single files. In practice, sequencing data often comes as multiple files, for example, one FASTQ file per barcode. Let’s practice handling several files at once. Let’s begin by getting more genomes and not only download genomes from human-background(files with Lj prefix) but also from a human background (files with GCF prefix):\nHint: If you downloaded one of the genomes in the exercise before, then you don’t need to run this wget command.\n\n# Download some more data \nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/GCF_000009425.fasta.gz -P data\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/GCF_000253315.fasta.gz -P data\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot59.fasta.gz -P data\n\n# Check what was done\nls -lh data\n\nWe now should have 5 files, 3 of which are still compressed.\n\n\n\n\n\n\nTipHint: Downloading many files with a for loop\n\n\n\n\n\nOnce you understand how to use wget, you can easily scale it up to download multiple files automatically using a for loop.\nFor example, suppose you have a list of genome IDs in a file called genomes.txt, one ID per line:\nLjRoot303\nGCF_000714595\nLjRoot59\nLjRoot60\nGCF_000009425\nGCF_000253315\nRoot935\nYou can then loop through all URLs and download them into the data folder like this:\n\n# make a playground folder\nmkdir playground \n\n# Download all genomes at once\nfor genome in $(cat genomes.txt); do\n    wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/${genome}.fasta.gz -P playground\ndone\n\nHere’s what happens:\n\n$(cat genomes.txt) reads all lines from the file\nfor genome in ... goes through each line one by one\nwget github_path/${genome}.fasta.gz -P playground downloads each genome into the playground folder. Here, the ${genome} will get replaced with one line in genomes.txt\n\nYou can use this same loop structure to run any command on multiple files automatically. This is one of the main reasons the CLI is so powerful. For example, you could in one command count the number of contigs in all your genomes:\n\n# Download all genomes at once\nfor genome in $(cat genomes.txt); do\n    echo $genome\n    zcat playground/${genome}.fasta.gz | grep -c \"&gt;\"\ndone\n\nYou can also format this more nicely if you really want to but this is outside of the scope of this tutorial. A more detailed explanation about for-loops can be found here.\n\n\n\n\n3.15.1 Wildcards (*): Match multiple files\nImagine we want to uncompress all the new files. Typing every filename can get tedious. Wildcards are another tool that you can use to work with groups of files using pattern matching. One of the most useful wildcard is *, which is used to search for a particular character(s) for zero or more times. For example, *.txt would find all files with the .txt extension.\n\n# List all files inside the data folder that end with gz\n# We should see only 3 files\nls data/*gz\n\n# List all uncompressed Lotus fasta files inside the data folder \n# using `Lj*.fasta*` means we look for filenames that \n# start with Lj, are followed by any number of characters and end with .fasta\nls data/Lj*.fasta\n\nWe can use Wildcards with every bash command and, for example, use it to unzip every file at once:\n\ngzip -d data/*gz\n\n# Check if that worked\nls -lh data\n\n\n\n3.15.2 cat and &gt; : Combining and saving files\nThe cat command doesn’t just print files, it can also concatenate (join) multiple files into one. For example, we might want to combine all contigs for the Lotus (Lj) genomes and store the output in a new file called lotus.fasta in the results folder (a folder you should have generated in an earlier exercise).\n\n# Combine the Lotus fasta files into one \n# and store the output of the cat command in a new file \ncat data/Lj*.fasta &gt; results/lotus.fasta\n\nHere:\n\ncat data/Lj* selects all files in the data folder that start with Lj and end with .fasta.\n&gt; tells the shell to write the combined output into a new file called lotus.fasta\n\n\n\n\n\n\n\nCautionImportant: Be careful with &gt;\n\n\n\n\n\nThe &gt; operator overwrites files without asking. If you want to add (append) to an existing file instead of replacing it, use &gt;&gt;:\n\n\n\nWhenever you combine or modify files it is a good idea to do some sanity checks. Luckily, we already learned about useful ways to do this:\n\n# Check how many protein files are in the individual faa files \ngrep -c \"&gt;\" data/Lj*\n\n# Check how many protein files are in concatenated file\n# Hopefully the numbers add up\ngrep -c \"&gt;\" results/lotus.fasta\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\n\nCombine the 16S rRNA gene sequences from human-associated strains (GCF Prefix) into a new file that should be stored in the results folder\nCount the total number of contigs in the individual and the combined file\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Combine\ncat data/GCF*.fasta &gt; results/human.fasta\n\n# Count \ngrep -c \"&gt;\" data/GCF*.fasta \ngrep -c \"&gt;\" results/human.fasta",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#pipes",
    "href": "source/cli.html#pipes",
    "title": "3  Navigating the command line",
    "section": "3.16 Pipes",
    "text": "3.16 Pipes\nSo far, we’ve run one command at a time, for example. But often, you might want to combine commands so that the output of one becomes the input of another. That’s what the pipe (|) does. It allows us to chain simple commands together to do more complex operations.\nFor example, we might want to ask how many sequences all human-associated strains have together but might not want to store a concatenated file to do so. Here, we could do the following instead:\n\n# Combine two files and count the total number of contigs\ncat data/GCF*.fasta | grep -c \"&gt;\"\n\nHere:\n\ncat data/GCF*.fasta concatenates fasta files from all human-associated strains\nThe pipe (|) sends the combined contigs directly to the grep command\ngrep -c \"&gt;\" only counts how many headers are found in the two genomes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/hpc_intro.html",
    "href": "source/hpc_intro.html",
    "title": "4  HPC introduction",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\nVery briefly, you can log into an HPC where you then have access to the login node, sometimes also called head node. The purpose of a login node is to prepare to run a program (e.g., moving and editing files as well as compiling and preparing a job script). You then submit a job script from the head to the compute nodes via a job manager called SLURM. The compute nodes are used to actually run a program and SLURM is used to provide the users access to the resources (CPUs, memory) on the compute nodes for a certain amount of time.\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind that you are sharing the system with other users. Some rules of thumb:\n\nDo NOT run jobs that request many CPUs and lots of memory on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005) for this\nDo NOT allocate more than 20% (CPU or memory) of the cluster for more than a day.\nDo not leave allocations unused and set reasonable time limits on you jobs\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use this system during the tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn Crunchomics you:\n\nAre granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nIn your home directory, /home/$USER , you have 25 G of storage\nIn your personal directory, /zfs/omics/personal/$USER , you can store up to 500 GB data\nFor larger, collaborative projects you can contact the Crunchomics team and ask for a shared folder to which several team members can have access\nYou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nA manual with more information and documentation about the cluster can be found here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nAccess to two directories:\n\nThe home directory with 25 GB of storage\nyour personal directory, with 500 GB of storage\n\n\nSome information about snapshots:\n\nSnapshots are made daily at 00.00.00 and kept for 2 weeks. This means that if you accidentally remove a file it can be restored up to 2 weeks after removal. This also means that even if you remove files to make space, these files will still count towards your quota for two weeks\nData on Crunchomics is stored on multiple disks. Therefore, there is protection against disk failure. However, the data is not replicated and you are responsible for backing up and/or archiving your data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HPC introduction</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html",
    "href": "source/hpc_howto.html",
    "title": "5  Working on an HPC",
    "section": "",
    "text": "5.1 Connecting to the HPC\nNow that you are more comfortable with the command line, we will start working on the UVA Crunchomics HPC. In this section, you will go through the following steps:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working on an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#connecting-to-the-hpc",
    "href": "source/hpc_howto.html#connecting-to-the-hpc",
    "title": "5  Working on an HPC",
    "section": "",
    "text": "5.1.1 ssh: Connecting to a sever\nSSH (Secure Shell) is a network protocol that allows you to securely connect to a remote computer such as the Crunchomics HPC. The general command looks like this:\n\nssh -X username@server\n\nHere:\n\nusername is your account name on the HPC, i.e. your UvanetID\nserver is the address of the HPC you want to connect to, for Crunchomics this is omics-h0.science.uva.nl\n-X enables X11 forwarding, which allows graphical applications from the server (like plotting or viewing images) to appear on your local machine. “Untrusted” X11 forwarding means the server can send graphical output, but it has limited access to your local machine\n\nFor Crunchomics at UVA, you would use:\n\nssh -X uvanetid@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nImportant tips for connecting:\n\nIf you want to log into Crunchomics while working from UvA use the eduroam network, not the open Amsterdam Science Park network\n\nIf you are connecting from outside UVA, you must be on the VPN. Contact ICT if you have not set it up or encounter issues\nAlways double-check your username and server address to avoid login errors\n\n\n\n\n\n5.1.2 Crunchomics: Preparing your account\nIf this is your first time using Crunchomics, you need to run a small Bash script to set up your account. This script will:\n\nAdd /zfs/omics/software/bin to your PATH. This basically allows Bash to locate and use the system-wide installed software available on Crunchomic\nSet up a Python 3 environment with some useful Python packages pre-installed\nCreate a link to your 500 GB personal directory inside your home directory, giving you plenty of space to store data and results\n\nTo set up your account, run the following command:\n\n# First orient yourself by typing \npwd \nls -lh \n\n# Run the Crunchomics installation script\n/zfs/omics/software/script/omics_install_script\n\n# Check if something changed\n# You now should see the personal folder inside your home directory \nls -lh\n\n\n\n5.1.3 conda: Setting up your own software environment\nMany bioinformatics tools are already installed on Crunchomics, but sometimes you’ll need additional ones (like NanoPlot for analysing long-read data). To manage and install your own tools, we use conda/mamba, an environment manager that lets you create isolated environments for different software.\n\n5.1.3.1 Install conda/mamba\nMany systems already include conda or mamba. Before installing a new copy, check if it’s already available:\n\nwhich conda\nwhich mamba\n\nIf one of these commands returns “command not found”, then you can follow the next steps to install conda/mamba yourself.\nTo install Miniforge (which includes conda and mamba by default), run:\n\n# Download the miniforge installation script\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n\n# Execute the miniforge installation script\nbash Miniforge3-$(uname)-$(uname -m).sh\n\nDuring installation:\n\nRead the license (scroll by pressing enter) and say “Yes” when asking for confirmation\nChoose the installation location\n\nOn Crunchomics, your home directory has only 25 GB of space. Therefore install everything into the personal folder that comes with 500 GB: /zfs/omics/personal/uvanetID/miniforge3\n\nSay “yes” when asked if conda should be initialized\nRestart your shell by typing source ~/.bashrc. You now should see a (base) in front of your prompt indicating that you are now inside conda’s base environment\nVerify the installation:\n\n\nconda -h\nmamba -h\n\n\n\n5.1.3.2 Installing NanoPlot\nNanoPlot is a visualization tool for long-read sequencing data. We can easily install it, and other software, using mamba (or conda if mamba is unavailable).\n\n# Check if NanoPlot is already available\nNanoPlot -h\n\n# If not installed, create a new environment called nanoplot and install it\n# Press \"Y\" when prompted about confirming the installation\nmamba create -n nanoplot -c bioconda nanoplot\n\n# Activate the new environment\nconda activate nanoplot \n\n# Check if the tool is installed\nNanoPlot -h\n\n# Exit the environment\nconda deactivate\n\nRecommendations:\n\nWhenever possible, use mamba instead of conda, it resolves dependencies faster\nKeep your base environment clean (the base environment is the conda environment you start with when you log into the HPC). Always create new environments for each tool (conda create -n toolname …) instead of installing everything in base\nCheck the tool’s documentation for specific installation notes or version requirements\nYou can install specific versions when needed with mamba create -n nanoplot_v1.42 -c bioconda nanoplot=1.42\nYou can remove environments you no longer need with conda env remove -n nanoplot\n\n\n\n\n5.1.4 Preparing your working directory\nNext, you can start organizing your project folder. You always want to generate project folders inside your personal directory, which provides more storage than the home directory:\n\n# Go into the personal folder\ncd personal \n\n# Make and go into a new project folder\nmkdir data_analysis\ncd data_analysis \n\n# Ensure that we are in the right folder \npwd\n\nYou now have a clean workspace for all the analyses that you will run during this tutorial on Crunchomics. As in your local machine, it’s good practice to keep your raw data, results, and scripts organized in separate folders.\n\n\n5.1.5 scp: Transferring data from/to a server\nTo learn how to transfer data to Crunchomics, we want to transfer the data folder (the one you have generated in previous part of the tutorial) to the HPC. We can do this with the scp(Secure Copy Protocol) command, which securely transfers files or directories between your computer and a remote server.\nThe basic syntax is:\n\nscp [options] SOURCE DESTINATION\nFor connecting to any HPC the syntax is server:file_location\n\nWhen transferring data you must run the command from a terminal session on your own computer, not from a terminal session that runs on the HPC.\nTo copy the entire data folder into your Crunchomics project folder, use:\n\n# Run this from your local terminal (not while logged into the HPC)\n# Replace 'username' with your UvAnetID\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/data_analysis\n\n# Then check on the HPC that the files arrived:\nll data/*.fna\n\nNotes:\n\nThe -r option copies directories recursively, i.e. you want to copy everything in the directory(recursively)\nIt’s helpful to keep two terminal windows open: one for your local computer and one for the HPC\nRun this command from inside your local data_analysis folder (the one you created earlier)\n\n\n\n\n\n\n\nTipTip: Moving data from the HPC to our own computer\n\n\n\n\n\nYou can also move results from the HPC to your computer. For example, to copy a single genome file from Crunchomics:\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/data_analysis/data/LjRoot303.fna .\n\nHere, the . at the end means “copy to the current directory” on your local machine. If you want to copy to another location, replace . with a path.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working on an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#submitting-jobs-with-slurm",
    "href": "source/hpc_howto.html#submitting-jobs-with-slurm",
    "title": "5  Working on an HPC",
    "section": "5.2 Submitting jobs with SLURM",
    "text": "5.2 Submitting jobs with SLURM\nIn the HPC introduction, you learned that there are two main “sections” on an HPC:\n\nLogin node: where you log in and do light work, such as moving files, editing scripts, or preparing jobs.\nCompute nodes: where the actual analyses run. These nodes have more CPUs and memory and are managed by a scheduling system.\n\nTo use these compute nodes efficiently, we communicate with them through SLURM, a workload manager that decides when, where, and how your jobs run based on the available resources.\n\n5.2.1 squeue: View info about jobs in the queue\nThe command below shows you which jobs are currently running or waiting in the queue:\n\nsqueue\n\nThis displays all jobs submitted to the system, and might look something like this:\n\n\n\n\n\nSome useful columns to know:\n\nJOBID: a unique number assigned to each job; you can use it to check or cancel a job later.\nST: the current state of the job (e.g., R = running, PD = pending).\nA full list of SLURM job state codes can be found here\n\nIf you have already submitted a job, it will appear in this list. To see only your own jobs, you can add the -u flag followed by your username:\n\nsqueue -u $USER\n\n\n\n5.2.2 srun:Submitting a job interactively\nNow that we’ve seen how to check the job queue, let’s learn how to actually run jobs on the compute nodes. There are two main ways to submit jobs: with srun and with sbatch. We’ll start with srun, which you typically use when:\n\nYou want to run tasks interactively and see the output directly on your screen\nYou are testing or debugging your commands before writing a job script\nYou have short jobs (usually less than a few hours)\n\n\nImportant: Jobs started with srun will stop if you disconnect from the HPC (unless you use tools like screen or tmux, which we won’t cover in this tutorial).\n\nLet’s start with a very simple example to see how srun works:\n\nsrun --cpus-per-task=1 --mem=1G --time=00:10:00 echo \"Hello world\"\n\nHere’s what each part means:\n\nsrun → communicate that you want to run something on the compute node\n--cpus-per-task=1 → request 1 CPU core\n--mem=1G → request 1 GB of memory\n--time=00:10:00 → set a maximum runtime of 10 minutes\necho \"Hello world\" → the actual command to run (prints text to the screen)\n\nSo the arguments after srun and before echo are where you tell SLURM what resources to allocate on the compute node.\n\n\n5.2.3 Running a analysis with Seqkit interactively\nNext, let’s use srun for something more useful. We’ll analyze our genomes using seqkit, a fast toolkit for inspecting FASTA/FASTQ files.\n\nTip: Whenever you use a tool for the first time, check its help page (e.g., seqkit -h) to see all available options.\n\n\n# Create a results folder\nmkdir -p results/seqkit \n\n# Run seqkit interactively\nsrun --cpus-per-task=1 --mem=5G seqkit stats data/*fna -Tao results/seqkit/genome_stats.tsv --threads 1\n\n# View the results (press 'q' to exit less)\nless -S results/seqkit/genome_stats.tsv \n\nHere:\n\nseqkit stats data/*fna → analyzes all FASTA files in the data folder ending in fna\n-Tao → output in tabular format (-T), include all stats (-a), and write to file (-o)\n--threads 1 → run on one thread (should match --cpus-per-task=1)\nThe results are stored in results/seqkit/genome_stats.tsv\n\nThis job runs very quickly — so fast that it might not even appear in the queue when you type squeue.\nWhen you open the results file, you’ll see:\n\nThe number of contigs (num_seqs), this should match what we saw earlier\nThe total genome size in bp (sum_len)\nAdditional metrics such as the minimum, maximum, average sequence length, and N50, a common measure of genome fragmentation.\n\n\n\n\n\n\n\nTipTip: Choosing the right amount of resources\n\n\n\n\n\nWhen starting out, choosing the right amount of CPUs, memory, or runtime can be tricky. Here are a few rules of thumb:\n\nStart small, most tools don’t need huge resources for small test runs\nCheck the tool’s documentation, many list recommended resource settings\nTest on a subset of your data first. It runs faster and helps you debug\nIf your job fails or runs out of memory, increase the resources gradually\nOnce you have a stable workflow, you can scale up\n\nRemember: over-requesting resources can make jobs wait longer in the queue — and can block others from running.\n\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\nNow it’s your turn to run an srun job interactively. Follow these steps:\nQuestion 1: Inspect the data\nWithout uncompressing the FASTQ file, view the first few lines of barcodeX.fastq.gz using zcat and head.\nYou can get the FASTQ file as follows:\n\nIf you are following the Microbial Ecology course:\n\n# Replace X with the barcode you used in the practical\ncp &lt;tba&gt;/barcodeX.fastq.gz data\n\nIf you are following this tutorial independently:\n\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/barcode01_merged.fastq.gz -P data/\n\n\nQuestion 2: Run seqkit interactively\nUse srun to analyze barcodeX.fastq.gz with seqkit stats. Request 1 CPU, 5 GB memory, and 10 minutes of runtime. Save the output in the results/seqkit/ folder.\nQuestion 3: Check the results\nOpen the output file and answer the following:\n\nWhat is the average sequence length (avg_len)? Does it match the expected amplicon size from the lab?\nHow many sequences are in the file (num_seqs)? Does this align with your expectations from the experiment?\n\n\nReminder: Run all commands on Crunchomics in your project folder. srun jobs may finish too quickly to appear in squeue.\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Question 1\nzcat data/barcodeX.fastq.gz | head\n\n# Question 2 \nsrun --cpus-per-task=1 --mem=5G seqkit stats data/barcodeX.fastq.gz  \\\n    -Tao results/seqkit/barcodeX_stats.tsv --threads 1\n\n# Question 3\nless -S results/seqkit/barcodeX_stats.tsv\n\nAnswers:\n\nYour average sequence length should be ~ 1400 bp and this should correspond to the amplicon size of your PCR reaction.\nnum_seqs should correspond to the number of sequences in your FASTQ file (depends on your library).\n\n\nTip: Use \\ to split long commands across lines for readability.\n\n\n\n\n\n\n\n\n\n5.2.4 sbatch: submitting a long-running job\nWhile srun is great for quick, interactive runs, most analyses on the HPC are submitted with sbatch, which lets jobs run in the background even after you log out.\nUse sbatch when:\n\nYou have long or resource-intensive analyses\nYou want jobs to run unattended\nYou plan to run multiple jobs in sequence or parallel\n\n\n5.2.4.1 Step 1: Create folders for organization\nWe’ll keep our project tidy by creating dedicated folders for scripts and logs:\n\nmkdir -p scripts logs\n\n\n\n5.2.4.2 Step 2: Write a job script\nUse nano (or another editor) to create a script file:\n\nnano scripts/run_seqkit.sh\n\nThen add the following content:\n\n#!/bin/bash\n#SBATCH --job-name=seqkit\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n#SBATCH --time=1:00:00\n\necho \"Seqkit job started on:\" \ndate\n\nsrun --cpus-per-task=1 --mem=5G seqkit stats data/*fna -Tao results/seqkit/genome_stats_2.tsv --threads 1\n\necho \"Seqkit job finished on:\"\ndate\n\nSave and exit:\n\nPress Ctrl+X to exit, then Y to confirm that you want to safe the file, then Enter.\n\n\n\n5.2.4.3 Step 3: Submit and monitor your job\n\n# Submit \nsbatch scripts/run_seqkit.sh\n\n# Check job status (may run too fast to appear)\nsqueue -u $USER\n\nWhen submitted successfully, you’ll see something like Submitted batch job 754 and new log files will appear in your logs folder:\n\nseqkit_&lt;jobID&gt;.out → standard output (results and messages)\nseqkit_&lt;jobID&gt;.err → error log (check this if your job fails)\n\n\n\n5.2.4.4 Step 4: Understanding your job script\nIn the job script above:\n\n#!/bin/bash: Runs the script with the Bash shell\n#SBATCH --... Define SLURM options: resources, runtime, output names.\n\nThese are the same options that you have used with srun\nThe %j variable in your log filenames automatically expands to the job ID, making it easier to keep track of multiple submissions.\n\necho / date: Prints status messages to track job progress\nsrun ...: The actual analysis command to run on the compute node\n\n\n\n\n\n\n\nTipTip: Debugging your first sbatch jobs\n\n\n\n\n\nIf your job fails:\n\nCheck your .err and .out files in the logs folder, these files usually tell you exactly what went wrong\nConfirm that your input files and paths exist\nTry running the main command interactively with srun first, if it works there, it will work in a script.\n\n\n\n\n\n\n5.2.4.5 Step 5: scancel: Cancelling a job\nSometimes you realize a job is stuck, misconfigured, or using the wrong resources. You can easily cancel it with:\n\nscancel &lt;jobID&gt;\n\nFor example, if your job ID was 754:\n\nscancel 754\n\n\n\n\n\n\n\nTipTip: Good HPC etiquette\n\n\n\nAlways cancel jobs that are running incorrectly or stuck in a queue too long. This frees resources for others and avoids unnecessary load on the cluster.\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nExercise: Run NanoPlot with sbatch\nNow you’ll write your own sbatch script to generate quality plots from barcodeX.fastq.gz using NanoPlot, which you installed earlier in your conda environment.\nBelow is a template for a script called scripts/nanoplot.sh. Replace all ... with the correct code.\nIn your script, make sure to:\n\nGive your job, output, and error files meaningful names\n\nRequest 2 CPUs, 10 GB of memory, and 30 minutes of runtime\n\nActivate your nanoplot conda environment\n\nCreate an appropriate output directory under results/\n\nUse NanoPlot options to:\n\nIndicate the input is in FASTQ format\n\nOutput a TSV stats file\n\nProduce bivariate dot plots\n\nSave all results in the new results folder\n\n\n\n#!/bin/bash\n#SBATCH --job-name=nanoplot\n#SBATCH --output=logs/nanoplot_%j.out\n#SBATCH --error=logs/nanoplot_%j.err\n...\n\n# Activate the conda environment\nsource ~/.bashrc # this is needed so that conda is initialized inside the compute node\nconda activate nanoplot\n\n# Add start date (optional)\n...\n\n# Make an output directory \n...\n\n# Run Nanoplot\nNanoPlot ...\n\n# Add end date (optional)\n...\n\nThen:\n\nSubmit your job with sbatch scripts/nanoplot.sh\nUse squeue to check whether it’s running\nInspect your log files (logs/nanoplot_.out and logs/nanoplot_.err)\nUse scp to copy the results folder to your own computer for viewing\nIn your NanoPlot output, open:\n\nNanoStats.txt (text summary)\nLengthvsQualityScatterPlot_dot.html (interactive plot)\n\nRecord:\n\nHow many reads were processed\nThe mean read length\nThe mean read quality\n\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\nThe final content of scripts/nanoplot.sh should look like this:\n\n#!/bin/bash\n#SBATCH --job-name=nanoplot\n#SBATCH --output=logs/nanoplot_%j.out\n#SBATCH --error=logs/nanoplot_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=10G\n#SBATCH --time=00:30:00\n\n# Activate the conda environment\nsource ~/.bashrc # this is needed so that conda is initialized inside the compute node\nconda activate nanoplot\n\n# Add start date (optional)\necho \"Job started: \"\ndate\n\n# Make an output directory \nmkdir -p results/nanoplot/raw\n\n# Run Nanoplot\nNanoPlot --fastq data/barcodeX.fastq.gz \\\n    -t 2 \\\n    --tsv_stats \\\n    --plots dot \\\n    -o results/nanoplot/raw\n\n# Add end date (optional)\necho \"Job ended: \"\ndate\n\nCheck your results:\n\n# Submit the job \nsbatch scripts/nanoplot.sh\n\n# Check job status\nsqueue\n\n# Explore the log files \nls -l logs/nanoplot* \ntail logs/nanoplot_*.out\n\n# Copy results to your computer\n# Run this command inside the data_analysis folder on my own computer \nscp -r uvanetid@omics-h0.science.uva.nl:/zfs/omics/personal/ndombro/data_analysis/results/nanoplot results\n\n# Explore the stats file \nless -S results/nanoplot/NanoStats.txt\n\nWhat to look for:\n\nMean read length ≈ your expected amplicon size\nMean sequence quality ≈ 12–15 (depends on sequencing kit)\nPlots usually show:\n\nA peak around the expected read length\nA tail of short, incomplete reads\nFewer, unusually long reads (often chimeras)\n\n\nUse these plots to guide quality filtering to removing reads that are too short, too long, or low quality, while keeping the majority of good data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working on an HPC</span>"
    ]
  },
  {
    "objectID": "source/applied_workflow.html",
    "href": "source/applied_workflow.html",
    "title": "6  Amplicon long-read analysis",
    "section": "",
    "text": "6.1 chopper: Quality cleaning\nSections that are still in progress:\nIn this section, you will learn how to perform quality filtering of your amplicon long-read data using chopper. Quality control is an essential first step to remove low quality reads that introduce errors in downstream analyses such as consensus generation and taxonomic assignment. Typical things you want to do are:\nchopper is designed for filtering and trimming of long-read data and you can install chopper with mamba:\nmamba create -n chopper_0.11.0 -c bioconda chopper=0.11.0\nIn this tutorial, you will always receive example code as a starting point, with sections containing ... for you to fill in. If you get stuck, check chopper’s documentation or use the built-in help via the CLI (chopper -h).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Amplicon long-read analysis</span>"
    ]
  },
  {
    "objectID": "source/applied_workflow.html#chopper-quality-cleaning",
    "href": "source/applied_workflow.html#chopper-quality-cleaning",
    "title": "6  Amplicon long-read analysis",
    "section": "",
    "text": "Removing sequence adapters, barcodes, primer sequences. This already has been done for your data.\nRemoving reads with a too low quality score\nRemoving reads that are too long or too short\nTrimming read ends if they have low quality scores\n\n\n\n\n\n\n\n\n\n\nQuestionTasks\n\n\n\n\n\nUse your NanoPlot quality plots from the previous section and choose some quality and length cutoffs. Specifically, you want to:\n\nPerform quality filtering with chopper:\n\nStore the output of chopper in a separate results folder\nRequest 2 CPUs and 20G of memory with srun, and ensure chopper itself also uses 2 threads\nDiscard sequences that are too long or too short\nDiscard reads, where the read’s average quality score is lower than a quality score of Q15\nCheck the tools manual carefully with how to deal with .gz files. Hint: Closely check the example section\n\nInvestigate the quality of the filtered FASTQ file with NanoPlot\n\nuse srun, 2 CPUs and 10G of memory\n\nafterwards open NanoPlots LengthvsQualityScatterPlot_dot.html and NanoStats.txt outputs and answer:\n\nThe total number of sequences that went into the analysis\nThe total number of sequences that passed the quality filtering\nAt which step most reads were lost (and how many)\nHow many basepairs were lost due to the trimming\n\n\n\n# Generate an output folder for the results\n... \n\n# Activate the chopper conda environment\n...\n\n# Run chopper\nsrun --cpus-per-task 2 --mem=20G chopper ...\n\n# Deactivate the conda environment \n...\n\n# Make a folder for the Nanoplot results and run Nanoplot\n...\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Generate an output folder for the results\nmkdir results/chopper\n\n# Activate the chopper conda environment\nconda activate chopper_0.11.0\n\n# Run chopper\nsrun --cpus-per-task 2 --mem=20G chopper \\\n    -i data/barcodeX.fastq.gz \\\n    -q 16 \\\n    --minlength 1300 --maxlength 1600 \\\n    --threads 2 | gzip \\\n    &gt; results/chopper/barcodeX_filtered.fastq.gz\n\n# Deactivate the conda environment \nconda deactivate\n\n# Run Nanoplot\nconda activate nanoplot\n\nmkdir -p results/nanoplot/cleaned\n\nsrun --cpus-per-task 2 --mem=10G NanoPlot \\\n    --fastq results/chopper/barcodeX_filtered.fastq.gz \\\n    -t 2 \\\n    --tsv_stats \\\n    --plots dot \\\n    -o results/nanoplot/cleaned\n\nconda deactivate\n\nBelow are example results from one of my analyses. These numbers will be different for you, however, this process will make you more familiar for what to look for in your own analyses.\n\nInput: 43,170 reads\nPassed filtering: 28665 reads (66%). For this data this is reasonable given the relatively high Phred score we use for quality filtering",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Amplicon long-read analysis</span>"
    ]
  },
  {
    "objectID": "source/applied_workflow.html#ngspeciesid-read-clustering",
    "href": "source/applied_workflow.html#ngspeciesid-read-clustering",
    "title": "6  Amplicon long-read analysis",
    "section": "6.2 NGSpeciesID: Read clustering",
    "text": "6.2 NGSpeciesID: Read clustering\nNanopore long-read sequencing can produce reads that span nearly the entire 16S rRNA gene, making it powerful for identifying microbial species. However, these long reads come with a trade-off: they have a higher error rate compared to short-read technologies.\nIn our dataset, the average read quality is Q15. The Phred score (Q) is a measure of how confident the sequencer is in each base call. The probability of a base being called incorrectly can be calculated as:\n\\[P=10^{-Q/10}\\]\nIn R we can compute this with:\n\nQ &lt;- 16\nP_error &lt;- 10^(-Q/10)\npercent_error &lt;- P_error * 100\nmessage(\"Error rate: \", round(percent_error, 2), \"%\")\n\nError rate: 2.51%\n\n\nThis shows that with a mean Phred quality of 15, each base has roughly a 2.5% chance of being incorrect. Across a 1,500 bp amplicon, that’s about 38 errors per sequence, which can be too many for accurate species-level identification.\nTo address this, we can cluster similar reads together, meaning we group sequences that likely come from the same species. Tools like NGSpeciesID cluster reads and then build a consensus sequence for each cluster, correcting random sequencing errors by comparing all reads within a cluster.\nYou can install NGSpeciesID with mamba:\n\n# Create a base environment\nmamba create -n NGSpeciesID_0.3.1  python=3.11 pip\n\n# Activate and install dependencies\nconda activate NGSpeciesID_0.3.1\nmamba install --yes -c conda-forge -c bioconda medaka==2.0.1 openblas==0.3.3 spoa racon minimap2  samtools\npip install NGSpeciesID\nconda deactivate\n\n\n\n\n\n\n\nQuestionTask\n\n\n\n\n\nFamiliarize yourself with the NGSpeciesID documentation and finish the code below:\nStep1: Prepare the input files as NGSpecies ID can not work with compressed FASTQ files.\n\n# Uncompress the chopper output\n...\n\nStep2: Run NGSpeciesID\nHere, you want to:\n\nUse srun with with 6 CPUs, 50G of memory\nSpecify that you’re working with ONT data\nUse the cleaned uncompressed FASTQ file as input\nOutput results to a new results folder\nEnable consensus generation (--consensus) and further polishing (--medaka)\nLower the abundance ratio to 0.01 (captures low-abundance sequences)\nRequire reads to align at least 95% (--aligned_threshold 0.95)\n\n\n# Activate the conda environment\nconda activate NGSpeciesID_0.3.1\n\n# Run NGSpeciesID\nsrun ... NGSpeciesID ...\n\n# Deactivate the conda environment \n...\n\nStep3: Explore the output\nIn your results folder, you should find multiple subfolders (e.g., medaka_cl_id_*). Each folder contains a consensus.fasta file representing a consensus sequence.\nAnswer the following:\n\nHow many consensus sequences were formed?\nIs this number higher or lower than what you expected based on your experimental design (number of microbial species)?\nIf it differs, what could explain this discrepancy? Hint: Think about sequencing errors, read depth, and 16S sequence similarity.\n\n\n# How many consensus sequences are formed?\ngrep ...\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\nThe scripts/ngspeciesid.sh should look as follows:\n\n# Uncompress the chopper output as NGSPeciesID only works with uncompressed files \ngzip -d results/chopper/barcodeX_filtered.fastq.gz\n\n# Activate the conda environment\nconda activate NGSpeciesID_0.3.1\n\n# Run NGSpeciesID\nsrun --cpus-per-task 6 --mem=50G NGSpeciesID \\\n        --ont \\\n        --fastq results/chopper/barcodeX_filtered.fastq \\\n        --outfolder results/ngspeciesid  \\\n        --consensus --medaka \\\n        --t 6 \\\n        --aligned_threshold 0.95 \\\n        --abundance_ratio 0.01 \n\nconda deactivate\n\n# Count how many consensus sequences we work with\ngrep \"&gt;\" results/ngspeciesid/medaka*/*fasta\n\nAnswer:\n\nNumber of consensus sequences formed: 9\nExpected number: 14 (based on 14 bacterial strains used)\nPossible reasons for less consensus sequences:\n\nThe high Nanopore error rate can blur true biological differences\nHighly similar 16S rRNA genes (95–99.9% identity) may have merged into a single cluster\nLower-abundance taxa may not have had enough reads to form a distinct cluster\n\nIf you observe more consensus sequences than expected, it could indicate incomplete clustering due to sequencing errors or uneven read coverage",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Amplicon long-read analysis</span>"
    ]
  }
]