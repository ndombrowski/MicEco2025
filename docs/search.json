[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Microbial Ecology: Introduction into Data Analysis",
    "section": "",
    "text": "Nina Dombrowski\n\n\nInstitute for Biodiversity and Ecosystem Dynamics (IBED), University of Amsterdam (UVA)\n\n\nn.dombrowski@uva.nl\n\n\n\nIntroduction\nWelcome to the Microbial Ecology Data Analysis Course!\nIn this short tutorial, you will learn how to use the command line and a High-Performance Computing (HPC) environment in order to move from raw 16S nanopore reads to a basic count table of microbial taxa.\nThis course is designed for students in microbial ecology with little or no prior experience using the command line. During this tutorial, you will learn how to:\n\nInstall a Terminal\n\nHow to access the command line\nHow to run basic commands\n\nDocument your Code\n\nDecide what software to use for code documentation\nHow to use markdown and document workflows\n\nNavigate the Command Line\n\nNavigate the filesystem (cd, ls, pwd)\nWork with files (mkdir, cp, mv, rm, cat, head, less)\nSearch and filter data (grep, wc, cut)\nView and underrstand FASTQ files\n\nWork with an HPC\n\nConnect to an HPC (SSH)\nUnderstand job schedulers (e.g., SLURM)\nRun command-line tools (e.g., fastqc, seqkit)\nSet up and activating conda environments\nRun workflows with Snakemake",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "source/terminal.html",
    "href": "source/terminal.html",
    "title": "1  Setting up a terminal",
    "section": "",
    "text": "1.1 Terminology\nThe command-line interface (CLI) is an alternative to a graphical user interface (GUI), with which you are likely more familiar. Both allow you to interact with your computer’s operating system but in a slightly different way:\nThe CLI is commonly called the shell, terminal, console, or prompt. These terms are related but not identical:\nThere are several types of shells — for example, bash or zsh (that use slightly different languages to issue commands). This tutorial was written on a computer that uses bash, which stands for Bourne Again Shell.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#terminology",
    "href": "source/terminal.html#terminology",
    "title": "1  Setting up a terminal",
    "section": "",
    "text": "In a GUI, you click buttons, open folders, and use menus\nIn the CLI, you type text to issue commands and see text output in the terminal\n\n\n\nThe terminal (or console) is the window or program that lets you type commands.\nThe shell is the program that interprets the commands you type inside the terminal and tells the operating system what to do.\nA prompt is the text displayed by the shell that indicates that it is ready to accept a command. The prompt often shows useful information, like your username, machine name, and current directory. This is one example for how a prompt can look like: user@machine:~$",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#installation-guides",
    "href": "source/terminal.html#installation-guides",
    "title": "1  Setting up a terminal",
    "section": "1.2 Installation guides",
    "text": "1.2 Installation guides\n\n1.2.1 Linux\nIf you’re using Linux, you already have everything you need and you don’t need to install anything. All Linux systems come with a terminal and a shell, and the default shell is usually Bash.\nYou can open a terminal from your applications menu or by searching for Gnome Terminal, KDE Konsole, or xterm, depending on your desktop environment.\nTo confirm which shell you’re using, type:\necho $SHELL\n\n\n1.2.2 Mac\nAll Mac computers also come with a built-in terminal and shell. To open the terminal:\n\nIn Finder, go to Go → Utilities, then open Terminal.\nOr use Spotlight Search (⌘ + Space), type Terminal, and press Return.\n\nThe default shell depends on your macOS version:\n\nmacOS Mojave (10.14) or earlier → Bash\nmacOS Catalina (10.15) or later → Zsh\n\nCheck which shell you’re currently using:\necho $SHELL\nIf the output does not end in /bash, you can start Bash manually by typing:\nbash\nThen check again with echo $SHELL. If you have trouble switching, don’t worry — nearly all commands in this tutorial will also work in Zsh.\n\n\n1.2.3 Windows\nUnlike macOS or Linux, Windows doesn’t include a Unix-style terminal by default, so you will need to install one of the following:\n\nMobaXterm (recommended)\n\nProvides a terminal with Linux-like commands built-in\nInstallation guide: MobaXterm setup instructions\nEasiest option for beginners and lightweight to install\n\nWindows Subsystem for Linux (WSL2)\n\nGives you a full Linux environment directly on Windows\nRecommended if you’re comfortable installing software or already have some command-line experience\nUses Ubuntu by default, which includes Bash and all standard Linux tools\nInstallation guide:Microsoft WSL install instructions\n\n\nOnce installed, open your terminal (MobaXterm or Ubuntu via WSL2) and verify that Bash is available:\necho $SHELL",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#sanity-check",
    "href": "source/terminal.html#sanity-check",
    "title": "1  Setting up a terminal",
    "section": "1.3 Sanity check",
    "text": "1.3 Sanity check\nAfter setup, open your terminal and type echo $SHELL and press enter. You should see something like this:\n\nIf you see a prompt ending in $ (for example user@machine:~$), your shell is ready to go and you are all set to follow along with the tutorial!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/documentation.html",
    "href": "source/documentation.html",
    "title": "2  Documenting your code",
    "section": "",
    "text": "2.1 Choosing your editor\nDocumenting your code is crucial for both your future self and anyone else who might work with your code. Good documentation helps others (and your future self) understand the purpose, functionality, and usage of your scripts. You want to document your code in the same way that you would write a detailed lab notebook.\nFor more in-depth guidance, see A Guide to Reproducible Code in Ecology and Evolution. While examples are mainly in R, the principles are general and apply across programming languages.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/documentation.html#choosing-your-editor",
    "href": "source/documentation.html#choosing-your-editor",
    "title": "2  Documenting your code",
    "section": "",
    "text": "2.1.1 Plain text editor\nAvoid visual editors like Word, as they are not designed for code and can change syntax by for example replacing backticks (`) with apostrophes (').\nOne of the easiest solutions is to use a plain text editor, such as:\n\nTextEdit (Mac)\nNotepad (Windows)\n\nThese editors allow you to write and save code safely, but they lack advanced features like syntax highlighting or integrated code execution.\n\n\n2.1.2 Rmarkdown in RStudio\nRMarkdown combines plain text, code, and documentation in one document. You can write your analysis and explanatory text together, then “knit” the document to HTML, PDF, or Word.\nTo create an RMarkdown file in RStudio:\n\nGo to File → New File → R Markdown\nChoose a title, author, and output format\nWrite your code and text\nClick Knit to render the document\n\nMore info: RMarkdown tutorial\n\n\n2.1.3 Quarto in Rstudio\nQuarto is a next-generation alternative to RMarkdown. It supports R, Python, and other languages, and offers more output formats and customization options. This software was used to generate this tutorial.\nTo create a Quarto document:\n\nGo to File → New File → Quarto Document\nChoose a title, author, and output format\nClick Render to generate your document\n\nMore info: Quarto documentation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/documentation.html#markdown-for-documentation",
    "href": "source/documentation.html#markdown-for-documentation",
    "title": "2  Documenting your code",
    "section": "2.2 Markdown for Documentation",
    "text": "2.2 Markdown for Documentation\nMarkdown is a lightweight language for formatting text. You can easily add headers, lists, links, code, images, and tables. You can use markdown inside a plain text editor or inside a Rmarkdown or Quarto document.\nHeaders:\nUse # to add a header and separate different sections of your documentation. The more # symbols you use after each other, the smaller the header will be. When writing a header make sure to always put a space between the # and the header name:\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers do not have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks followed by the computational language, i.e. bash or r, used.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nYou can easily add links to external resources as follows:\n[Link Text](https://www.example.com)\nEmphasis:\nYou can use * or _ to write italic and ** or __ for bold text.\n*italic*\n**bold**\nPictures\nYou can also add images to your documentation as follows:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/image.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/cli.html",
    "href": "source/cli.html",
    "title": "3  Navigating the command line",
    "section": "",
    "text": "3.1 pwd: Find out where we are\nOnce your terminal is open, let’s get oriented by typing your first command and then pressing enter:\npwd\nThe command pwd stands for print working directory. It tells you where you currently are in the file system, that is, which folder (directory) your shell is operating on right now. You should see something like:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#pwd-find-out-where-we-are",
    "href": "source/cli.html#pwd-find-out-where-we-are",
    "title": "3  Navigating the command line",
    "section": "",
    "text": "/Users/YourUserName\n\n\n\n\n\n\nTipTip: Finding the Desktop on Different Systems\n\n\n\n\n\nYour home directory path varies slightly across operating systems. Here’s how to locate yourself and connect to familiar locations like the Desktop:\nmacOS\n\nYour home directory is /Users/YourUserName\nTo open the folder you are currently in Finder:open .\nYour desktop is at /Users/YourUserName/Desktop\n\nMobaXterm (Windows)\n\nYour home directory is /home/mobaxterm\nBy default, this is temporary and is deleted when you close MobaXterm. To make it permanent:\n\nGo to Settings –&gt; Configuration –&gt; General\nUnder Persistent home directory, choose a folder of your choice\n\nTo open the folder you are currently in the Windows File explorer: explorer.exe .\nYour Desktop is usually at: /mnt/c/Users/YourUserName/Desktop or /mnt/c/Users/YourUserName/OneDrive/Desktop (when using OneDrive)\n\nWSL2 (Windows)\n\nYour home directory is/home/YourUserName\nTo open the folder you are currently in the Windows File explorer: explorer.exe .\nYour Desktop is usually at: /mnt/c/Users/YourUserName/Desktop or /mnt/c/Users/YourUserName/OneDrive/Desktop (when using OneDrive)\n\nIf you want to access the Uva OneDrive folder:\nIf your OneDrive folder name includes spaces (like OneDrive - UvA), use quotes around the path:\n\ncd \"/mnt/c/Users/YourUserName/OneDrive - UvA\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#ls-list-the-contents-of-a-directory",
    "href": "source/cli.html#ls-list-the-contents-of-a-directory",
    "title": "3  Navigating the command line",
    "section": "3.2 ls: List the contents of a directory",
    "text": "3.2 ls: List the contents of a directory\nNow that we know where we are, let’s find out what is inside that location, i.e. what files and folders can be found there. The command ls (short for list) shows the files and folders in your current directory. Type the following and press enter:\n\nls\n\nYou should see something like this (your output will vary depending on what’s in your directory):\n\nThe colors and formatting depend on your terminal settings, but typically:\n\nFolders (directories) appear in one color (often green or blue)\nFiles appear in another (often white or bold)\n\nIf your directory contains many items, the output can quickly become overwhelming. To make sense of it, we can use options and arguments to control how commands behave.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#the-structure-of-a-command",
    "href": "source/cli.html#the-structure-of-a-command",
    "title": "3  Navigating the command line",
    "section": "3.3 The structure of a command",
    "text": "3.3 The structure of a command\nA command generally has three parts:\n\nA command: The program you want to run, i.e. ls\nAn option (or flag): A way to modify how the command behaves, i.e -l (long format)\nAn optional argument: The input, i.e. a file or folder\n\n\nTry the following command in your current directory to “List (ls) the contents of the current folder and show details in long format (-l)”:\n\nls -l\n\nAfter running this you should see a more detail list of the contents of your folder.In the example below we can see that we now print additional information about who owns the files (i.e. access modes), how large the files are, when they were last modified and of course the name:\n\n\n\n\n\n\n\nTipTip: Using ls in practice\n\n\n\n\n\nThroughout this tutorial, you’ll notice that we use ls frequently. There’s a reason for that:\nWhen working with data, sanity checks are essential, because it is easy to make mistakes, overwrite files, or lose track of where things are. Using simple commands like ls, wc, or grep helps you verify what’s happening to your files at every step of an analysis workflow.\nThese habits are not just for beginners, also experienced bioinformaticians rely on them constantly. For newcomers, practicing these checks early will help you build confidence and intuition when working on the command line.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#getting-help",
    "href": "source/cli.html#getting-help",
    "title": "3  Navigating the command line",
    "section": "3.4 Getting help",
    "text": "3.4 Getting help\nAt some point, you’ll want to know what options a command has or how it works. In this case, you can always check the manual pages (or man pages) by typing man followed by the command name:\n\nman ls\n\nThis opens the manual entry for the command ls. You can scroll through it using:\n\n↑ / ↓ arrows or the space bar to move down\nb to move back up\nq to quit the manual\n\nNot all commands come with such a manual. Depending on the program, there are a few common patterns you can try to get help:\n\nman ls\nls --help\nls -h",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#mkdir-make-a-new-folder",
    "href": "source/cli.html#mkdir-make-a-new-folder",
    "title": "3  Navigating the command line",
    "section": "3.5 mkdir: Make a new folder",
    "text": "3.5 mkdir: Make a new folder\nBefore we start moving around, let’s first learn how to create new folders (also called directories).This is something we will do often, for example, to keep raw data, results, and scripts organized in separate places. The command we use for that is mkdir, which stands for make directory.\nFor now, we will use mkdir to create a shared working folder with the name data_analysis for this tutorial. Don’t worry about how to move into the folder yet, we’ll cover that in the next section.\n\n# Move into the home directory (the starting point of your system)\ncd ~\n\n# Create a new folder called 'data_analysis'\nmkdir data_analysis\n\n# Check that the folder was created successfully\nls\n\nYou should see a new folder called data_analysis appear in the list. We will use this folder as our project space for all exercises in this tutorial. You can easily make a new data folder inside the new data_analysis folder by typing the following:\n\n# Make a data folder inside the data_analysis folder\n# The `-p` option makes the parent directory, if it does not already exist\n# It is useful to by default add `-p` when generating a folder inside a folder\nmkdir -p data_analysis/data\n\n# Check that the folder was created successfully\n# Notice here, how we use ls with a flag and also with an optional argument?\nls -l data_analysis\n\n\n\n\n\n\n\nTipTip: Commenting your code\n\n\n\n\n\nNotice how we added # and some notes above each command?\nAnything written after # in Bash is a comment. A comment won’t be executed, but it helps you (and others) understand what the command does.\nIn your own work, add short, meaningful comments above key steps. Avoid restating the obvious, instead, explain why you’re doing something or what it achieves.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#cd-move-around-folders",
    "href": "source/cli.html#cd-move-around-folders",
    "title": "3  Navigating the command line",
    "section": "3.6 cd: Move around folders",
    "text": "3.6 cd: Move around folders\nNow that we have our own project folder, let’s learn how to move around the file system.\nThe file system is structured like a tree that starts from a single root directory (that is also denoted as / in bash). All other folders branch out from this root directory. For example, we can go from the root directory, to the users folder and from there into the john folder.\n\nThere are two ways to specify a path to go the the portfolio folder:\n\nAbsolute path: starts from the root (e.g. cd /users/john/portfolio)\nRelative path: starts from your current location (e.g. cd portfolio if you’re already in /users/john)\n\nIt is generally recommended to use the relative path from inside your project directory. That makes your code more portable and still allows you to run the code even if your computer setup changes.\nLet’s practice moving between folders (at each step, use pwd in case you feel that you get lost):\n\n# Move into the data analysis folder \ncd data_analysis \n\n# Check where we are \npwd\n\nWe now should see that we are in something like /Users/Name/data_analysis. We can use the cd command in multiple ways to move around:\n\n# Move into the data folder\ncd data\n\n# Move one level up, i.e. go back to the data_analysis folder\ncd ..\n\n# Move multiple levels at once\ncd data_analysis/data\n\n# Move two levels up\ncd ../.. \n\n# Quickly go back home\ncd ~\n\n# And go back to the data_analysis folder \ncd data_analysis\n\nIn the code above, the tilde symbol (~) is a shortcut for your home directory. It’s equivalent to typing the full absolute path to your home (e.g. cd /Users/YourName) but it is much faster to type.\n\n\n\n\n\n\nTipTip: Command-line completion\n\n\n\n\n\nHere, are some other tips for faster navigation (and less typos):\n\nUse Tab for autocompletion: type the first few letters of a folder name and press Tab.\nIf there’s more than one match, press Tab twice to see all options.\nUse ↑ / ↓ arrows to scroll through previously entered commands\n\nHint: From now on try to use the Tab key once in a while so that you do not have to write everything yourself all the time.\n\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\nFamiliarize yourself with these first commands and:\n\nCreate a new folder inside your data_analysis directory called results\nMove into the results folder and confirm your location with pwd\nMove back inside the data_analysis folder\nUse ls to confirm both results and data are there\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\nmkdir results \ncd results \npwd \ncd ..\nls",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#wget-download-data",
    "href": "source/cli.html#wget-download-data",
    "title": "3  Navigating the command line",
    "section": "3.7 wget: Download data",
    "text": "3.7 wget: Download data\nNext, let’s download a genome fasta file to go through some other useful commands. We can use the wget command to fetch a genome fasta from an online website as follows:\n\n# Download the example fasta file into the current directory\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fna.gz\n\n\n\n\n\n\n\nTipTip: Where does our data come from?\n\n\n\n\n\nThe genome comes from the Plant-associated bacterial culture collections database. This database contains information from microbial strains isolated from the roots and leaves of Arabidopsis thaliana and from roots and nodules from Lotus japonicus. Some of these strains are also used in your lab practicals.\nFor convenience (and security reasons) you will for this tutorial will download the data from Github. However, in practice you will often download data from institutional websites such as the one above.\n\n\n\nAfter you download data, it is always a good idea to do some sanity check to see if the file is present and know how large it is:\n\n# List files in long (-l) and human-readable (-h) format\n# combining these two commands becomes -lh\nls -lh LjRoot303.fna.gz",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#cp-copy-files",
    "href": "source/cli.html#cp-copy-files",
    "title": "3  Navigating the command line",
    "section": "3.8 cp: Copy files",
    "text": "3.8 cp: Copy files\ncp duplicates files or directories. It is a useful feature to keep our files organized and not have every single file in a single folder but instead to organize your files into folder categories (useful folders can be: data, scripts and results).\nLet’s use cp to copy the downloaded file into data and to organize the data a bit better:\n\n# Copy file into data \ncp LjRoot303.fna.gz data/\n\n# Show content of both locations\nls -l\nls -l data\n\nWhen running the two ls commands, we see that we now have two copies of LjRoot303.fna.gz, one file is in our working directory and the other one is in our data folder. Having large files in multiple locations is not ideal since we will use unneccassary space. However, we can use another command to move the file into our data folder instead of copying it.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#mv-move-or-rename-files",
    "href": "source/cli.html#mv-move-or-rename-files",
    "title": "3  Navigating the command line",
    "section": "3.9 mv: Move (or rename) files",
    "text": "3.9 mv: Move (or rename) files\nmv moves or renames files without creating a second copy:\n\n# Move the file into data\nmv LjRoot303.fna.gz data/\n\n# Verify\nls -l\nls -l data\n\nNotice that mv will move a file and, without asking, overwrite the existing file we had in the data folder when we ran cp. This means that if you run mv its best to make sure that you do not overwrite files by mistake.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#rm-remove-files-and-directories",
    "href": "source/cli.html#rm-remove-files-and-directories",
    "title": "3  Navigating the command line",
    "section": "3.10 rm: Remove files and directories",
    "text": "3.10 rm: Remove files and directories\nTo remove files and folders, we use the rm command. For example we could remove the LjRoot303.fna.gz in case we don’t need it anymore:\n\n# Remove the genome file from the data folder\nrm data/LjRoot303.fna.gz\n\n# Check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an option. To do this, we use -r , which allows us to remove directories and their contents recursively.\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone. Therefore, use rm with care and check what you write twice before pressing enter!\nAlso, NEVER run rm -r or rm -rf on the root / folder or any important path. And yes, we have seen recommendations to use these combinations online. Therefore, always double check the path or file name before pressing enter when using the rm command.\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\nDownload another genome:\n\nDownload another genome (not LjRoot303) from this url: https://github.com/ndombrowski/MicEco2025/tree/main/data. To get the link,\n\nClick on the genome you want to download\nRight click on view raw and select copy link\nUse wget to download that file\n\nMake sure that the new genome stored in the data folder\nCheck the file size (hint: use the -h option with ls)\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Download the genome\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot44.fna.gz\n\n# Move the genome file to the data folder\nmv LjRoot44.fna.gz data\n\n# Check the file size\nls -lh data/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#gzip-uncompressing-files",
    "href": "source/cli.html#gzip-uncompressing-files",
    "title": "3  Navigating the command line",
    "section": "3.11 gzip: (Un)compressing files",
    "text": "3.11 gzip: (Un)compressing files\nYou might have noticed that the file we downloaded ends with .gz. This extension is used for files that are compressed to make the file smaller. This is useful for saving files, but this makes the file unreadable for a human. To be able to learn how to read the content of a file, let’s learn how to uncompress the file first.\n\n# Download another genome  \n# Here, we use the -P option to directly download the file into the data folder \nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fna.gz -P data\n\n# Check that this worked \n# You now should see two genomes\nls data\n\n# Uncompress the file with gzip \n# Here, we use -d to decompress the file \n# (by default gzip will compress a file)\ngzip -d data/LjRoot303.fna.gz \n\n# Check that this worked \n# We now should see that one file is uncompressed (i.e. it lost the gz extension)\nls data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#exploring-file-contents",
    "href": "source/cli.html#exploring-file-contents",
    "title": "3  Navigating the command line",
    "section": "3.12 Exploring file contents",
    "text": "3.12 Exploring file contents\nNow that we have the uncompressed file we can use different ways to explore the content of a file.\n\n3.12.1 cat: Print the full file\nWe can use cat print the entire file to the screen, this is fine for short files, but overwhelming for long ones, which for our file is the case as we will see the nucleotides just running across the screen.\n\ncat data/LjRoot303.fna\n\n\n\n3.12.2 head and tail: View parts of a file\nTo only print the first few or last few lines we can use the head and tail commands:\n\n# Show the first 10 lines\nhead data/LjRoot303.fna\n\n# Show the last 5 lines\n# Here, the option -n allows us to control how many lines get printed\ntail -n 5 data/LjRoot303.fna\n\n\n\n3.12.3 less: View the full file\nless let’s you view a file’s contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless -S data/LjRoot303.fna\n\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\n\n\n3.12.4 zcat: Decompress and print to screen\nWhen we used gzip above, we decompressed the file and that allowed us to read the content of the fasta file. This is perfect for smaller files, but not ideal for sequence data since these files get large and we might not want to decompress these files as they would clutter our system.\nLuckily, there is one useful tool in bash to decompress the file and print the content to the screen. zcat will print the content of a file to the screen but leave the file as is. Since the file is quite large, we see a lot of content flying across the screen, that is fine, we will see how to improve in a bit.\n\n# Check the content of the data folder \n# We should have one compressed and one uncompressed file \nls data\n\n# Use zcat on the compressed file \nzcat data/LjRoot44.fna.gz\n\n# Check the content of the data folder \n# We still should have one compressed and one uncompressed file \nls data\n\n\n\n\n\n\n\nTipTip: Editing text files\n\n\n\n\n\nYou can also edit the content of a text file and there are different programs available to do this on the command line, the most commonly used tool is nano, which should come with most command line interpreters. You can open any file as follows:\n\nnano data/LjRoot303.fna\n\nOnce the document is open you can edit it however you want and then\n\nClose the document with control + X\nType y to save changes and press enter",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#wc-count-things",
    "href": "source/cli.html#wc-count-things",
    "title": "3  Navigating the command line",
    "section": "3.13 wc: Count things",
    "text": "3.13 wc: Count things\nAnother useful tool is the wc (short for wordcount) command that allows us to count the number of lines via -l in a file. It is an useful tool for sanity checking.\n\nwc -l data/LjRoot303.fna\n\nWe see that this file does contain 112,262 lines of text. For fasta files this is not very informative, but if you for example work with a dataframe with rows and columns where you filter rows using certain conditions this can become very useful for sanity checking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#grep-print-lines-that-match-patterns",
    "href": "source/cli.html#grep-print-lines-that-match-patterns",
    "title": "3  Navigating the command line",
    "section": "3.14 grep : print lines that match patterns",
    "text": "3.14 grep : print lines that match patterns\nThe grep command searches for patterns in a file. We could for example use this to ask how many contigs are in our fasta file by searching for the fasta header that always starts with a &gt;. The basic grep syntax is grep \"pattern\" file. Here, “pattern” is the text you want to search for, and file is the file you want to search in. The double quotes allow us to include special characters (like &gt;) safely.\n\ngrep \"&gt;\" data/LjRoot303.fna\n\nThis prints all lines that match the pattern. In a fasta file, this means you see all the contig headers. Depending on your terminal, the matches might be highlighted, which makes them easy to spot..\nIf we only care about the number of contigs, we can use the -c option:\n\n# Root303 contains 85 contigs\ngrep -c \"&gt;\" data/LjRoot303.fna\n\n\n\n\n\n\n\nTipThe structure of a fasta file\n\n\n\n\n\nA sequence FASTA file is a text based format to store DNA or peptide sequences. It should always look something like this:\n\nThe header always starts with a &gt; followed by descriptive information. In a new line the sequence data gets stored in either a single line or multiple lines. A fasta file can continue single sequence or multiple sequences.\nBy convention the extensions .fna is used to store fasta sequences from nucleotides and .faa is used to store fasta sequences from proteins.\n\n\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\n\nUncompress the file for LjRoot44\nCompare the number of contigs for the LjRoot44 and LjRoot303 genomes\nWhat does it mean if one genome has more contigs than another? Should this raise any concerns when analyzing the genome?\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Uncompress \ngzip -d data/LjRoot44.fna.gz\n\n# Count the number of contigs \n# We work with genomes with 85 and 12 contigs\ngrep -c \"&gt;\" data/LjRoot303.fna\ngrep -c \"&gt;\" data/LjRoot44.fna\n\nAnswer for question 3:\nLjRoot44 appears more fragmented (more contigs) than LjRoot303. A fragmented genome can split interesting genes across contigs making them harder to find and annotate. When evaluating genomes, consider not just contig counts but also file size, genome size, and completeness.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#pipes",
    "href": "source/cli.html#pipes",
    "title": "3  Navigating the command line",
    "section": "3.15 Pipes",
    "text": "3.15 Pipes\nSo far, we’ve run one command at a time, for example. But often, you might want to combine commands so that the output of one becomes the input of another. That’s what the pipe (|) does. It allows us to chain simple commands together to do more complex operations.\n\n# Example: show only the first 100 lines and then\n# count how often a sequence motif occurs in these lines\nhead -n 100 data/LjRoot303.fna | grep -c \"&gt;\" \n\nHere:\n\nhead -n 100 data/GCF_000005845.2_ASM584v2_genomic.fna prints the first 100 lines\nThe pipe (|) sends these 100 lines directly to the grep command\ngrep -c \"&gt;\" only counts how many headers are found in the first 100 lines of the fasta file",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#working-with-multiple-files",
    "href": "source/cli.html#working-with-multiple-files",
    "title": "3  Navigating the command line",
    "section": "3.16 Working with multiple files",
    "text": "3.16 Working with multiple files\nSo far, we’ve worked with single files. In practice, sequencing data often comes as multiple filesm for example, one FASTQ file per barcode. Let’s practice handling several files at once. Let’s begin by getting more genomes and not only get genomes from Lotus japonicus (files with Lj prefix) but also some genomes from Arabidopsis thaliana (files with Root prefix):\nHint: If you downloaded one of the genomes in the exercise before, then you don’t need to run this wget command.\n\n# Download some more data \nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/Root401.fna.gz -P data\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/Root68.fna.gz -P data\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot59.fna.gz -P data\n\n# Check what was done\nls -lh data\n\nWe now should have 5 files, 3 of which are still compressed.\n\n\n\n\n\n\nTipHint: Downloading many files with a for loop\n\n\n\n\n\nOnce you understand how to use wget, you can easily scale it up to download multiple files automatically using a simple for loop.\nFor example, suppose you have a list of genome IDs in a file called genomes.txt, one ID per line:\nLjRoot303\nLjRoot44\nLjRoot59\nLjRoot60\nRoot401\nRoot68\nRoot935\nYou can then loop through all URLs and download them into the data folder like this:\n\n# make a playground folder\nmkdir playground \n\n# Download all genomes at once\nfor genome in $(cat genomes.txt); do\n    wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/${genome}.fna.gz -P playground\ndone\n\nHere’s what happens:\n\n$(cat genomes.txt) reads all lines from the file\nfor genome in ... goes through each line one by one\nwget github_path/${genome}.fna.gz -P playground downloads each genome into the playground folder. Here, the ${genome} will get replaced with one line in genomes.txt\n\nYou can use this same loop structure to run any command on multiple files automatically. This is one of the main reasons the CLI is so powerful. For example, you could in one command count the number of contigs in all your genomes:\n\n# Download all genomes at once\nfor genome in $(cat genomes.txt); do\n    echo $genome\n    zcat playground/${genome}.fna.gz | grep -c \"&gt;\"\ndone\n\nYou can also format this more nicely if you really want to but this is outside of the scope of this tutorial. A more detailed explanation about for-loops can be found here.\n\n\n\n\n3.16.1 Wildcards (*): Match multiple files\nImagine we want to uncompress all the new files. Typing every filename can get tedious. Wildcards are another tool that you can use to work with groups of files using pattern matching.\n\n# List all files inside the data folder that end with gz\n# We should see only 3 files\nls data/*gz\n\n# List all Lotus fasta files inside the data folder \n# using `Lj*.fna*` means we look for filenames that \n# Start with Lj, followed by any number of characters before and ending with .fna\nls data/Lj*.fna\n\nWe can use Wildcards with every bash command and use it to unzip every file at once with the following command:\n\ngzip -d data/*gz\n\n# Check if that worked\nls -lh data\n\n\n\n3.16.2 cat and &gt; : Combining and saving files\nThe cat command doesn’t just print files, it can also concatenate (join) multiple files into one. For example, we might want to combine all contigs for the Lotus (Lj) genome and store the output in a new file called lotus.fna in the results folder (a folder you should have generated in an earlier exercise).\n\n# Combine the Lotus fasta files into one\ncat data/Lj*.fna &gt; results/lotus.fna\n\nHere:\n\ncat data/Lj* selects all files in the data folder that start with Lj and end with .fna.\n&gt; tells the shell to write the combined output into a new file called lotus.fna\n\n\n\n\n\n\n\nCautionImportant: Be careful with &gt;\n\n\n\n\n\nThe &gt; operator overwrites files without asking. If you want to add (append) to an existing file instead of replacing it, use &gt;&gt;:\n\n\n\nWhenever you modify files it is a good idea to do some sanity checks. Luckily, we already learned about useful ways to do this:\n\n# Check how many protein files are in the individual faa files \ngrep -c \"&gt;\" data/Lj*\n\n# Check how many protein files are in concatenated file\n# Hopefully the numbers add up\ngrep -c \"&gt;\" results/lotus.fna\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\n\nCombine the Arabidopsis contigs (Root Prefix) into a new file that should be stored in the results folder\nCount the total number of contigs in the individual and the combined file\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Combine\ncat data/Root*.fna &gt; results/arabidopsis.fna\n\n# Count \ngrep -c \"&gt;\" data/Root*.fna \ngrep -c \"&gt;\" results/arabidopsis.fna",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/hpc_intro.html",
    "href": "source/hpc_intro.html",
    "title": "4  HPC introduction",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\nVery briefly, you can log into an HPC onto a login node. The purpose of a login node, sometimes also called head node, is to prepare to run a program (e.g., moving and editing files as well as compiling and preparing a job script). You then submit a job script from the head to the compute nodes via Slurm. The compute nodes are used to actually run a program and Slurm is an open-source workload manager/scheduler that is used on many big HPCs. Slurm has three key functions:\n\nprovide the users access to the resources on the compute nodes for a certain amount of time to perform any computation\nprovide a framework to start, execute, and check the work on the set of allocated compute nodes\nmange the queue of submitted jobs based on the availability of resources\n\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind that you are sharing the system with other users. Some rules of thumb:\n\nDo NOT run jobs that request many CPUs and lots of memory on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005) for this\nDo NOT allocate more than 20% (CPU or memory) of the cluster for more than a day. If you have large jobs, contact Wim de Leeuw\nDo not leave allocations unused and set reasonable time limits on you jobs\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use the queue during this tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn crunchomics you:\n\nAre granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nIn your home directory, /home/$USER , you have 25 G quotum\nIn your personal directory, /zfs/omics/personal/$USER , you can store up to 500 GB data\nFor larger, collaborative projects you can contact the Crunchomics team and ask for a shared folder to which several team members can have access\nYou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nA manual with more information and documentation about the cluster can be found here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nIf you need more resources (or access to GPUs), visit the website of the computational support team for more information\nAccess to two directories:\n\nThe home directory with 25 GB of storage\nyour personal directory, with 500 GB of storage\n\n\nSome information about storage:\n\nSnapshots are made daily at 00.00.00 and kept for 2 weeks. This means that if you accidentally remove a file it can be restored up to 2 weeks after removal. This also means that even if you remove files to make space, these files will still count towards your quota for two weeks\nData on Crunchomics is stored on multiple disks. Therefore, there is protection against disk failure. However, the data is not replicated and you are responsible for backing up and/or archiving your data.\n\nOn the next page you find a tutorial to move the sequencing data that we worked on during the introduction into bash onto the server and run software to assess the quality of our sequencing data. We will learn to use some pre-installed software and also install software ourselves with conda and we will learn different ways to submit jobs to the compute nodes using SLURM.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HPC introduction</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html",
    "href": "source/hpc_howto.html",
    "title": "5  Using an HPC",
    "section": "",
    "text": "5.1 Connecting to the HPC\nNow that you are more comfortable with the command line, we will start working on the UVA Crunchomics HPC. In this section, you will go through the following steps:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#ssh-connecting-to-a-sever",
    "href": "source/hpc_howto.html#ssh-connecting-to-a-sever",
    "title": "5  Using an HPC",
    "section": "",
    "text": "-X option enables untrusted X11 forwarding in SSH. Untrusted means = your local client sends a command to the remote machine and receives the graphical output. Put simply this option enables us to run graphical applications on a remote server and this for example allows us to view a pdf.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to log into Crunchomics while working from UvA use the eduroam network, not the open Amsterdam Science Park network.\nIf you want to log into Crunchomics from outside of UvA you need to be connected to the VPN. If you have not set that up and/or have trouble doing so, please contact ICT.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#crunchomics-preparing-your-account",
    "href": "source/hpc_howto.html#crunchomics-preparing-your-account",
    "title": "5  Using an HPC",
    "section": "5.2 Crunchomics: Preparing your account",
    "text": "5.2 Crunchomics: Preparing your account\nIgnore this section if you already did this but if this is your first time on Crunchomics, then you want to first run a small Bash script that:\n\nAllows you to use system-wide installed software, by adding /zfs/omics/software/bin to your path. This basically means that bash knows that there is an extra folder in which to look for any software that is pre-installed on the HPC\nSets up a python3 environment and some useful python packages\nHave a link for your 500 GB personal directory in your home directory\n\nTo set this up, run the following command in the cli:\n\n/zfs/omics/software/script/omics_install_script",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#scp-transferring-data-fromto-a-server",
    "href": "source/hpc_howto.html#scp-transferring-data-fromto-a-server",
    "title": "5  Using an HPC",
    "section": "5.3 scp: Transferring data from/to a server",
    "text": "5.3 scp: Transferring data from/to a server\nscp stands for Secure Copy Protocol and allows you to securely copy files and directories between remote hosts. When transferring data the transfer is always prepared from the terminal of your personal computer and not from the HPCs login node.\nThe basic syntax we use looks like this:\nscp [options] SOURCE DESTINATION\nTo start analysing our data, we want to move the fastq.gz files that we have worked with before from our personal folder, the source, to a folder on Cruncomics, the destination. Let’s start setting up a project folder from which we want to run our analyses by:\n\nMoving from our home directory into our personal directory on the Crunchomics HPC. We move there since we have more space in the personal directory. Notice, for the command below to work, we need to have already executed the omics_install_script script above\nMake a project folder with a descriptive file name, i.e. projectX\n\n\ncd personal/\nmkdir projectX\ncd projectX\n\nNow that we have organized our working directory on the HPC, we next want to move the data folder (which right now is only on your own, personal computer) with the sequencing that you have downloaded before to Crunchomics. We do this by moving the whole data folder from our own computer to the new project folder on the HPC. Therefore, it is important that:\n\nwe run the following command from the cli on our own computer and not from the cli while being logged into the HPC!\nyou exchange the two instances of username in the code below with your username/uvanetid\nIn the example below, I am running the code from inside of the data_analysis folder that we have generated in the previous tutorial since that is where I have the data folder that I want to move. I use the -r option in order to move the whole data folder and not a single file.\n\n\n#run the command below from your local computer, \n#ensure that the data folder is inside the directory from which you run this command\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/projectX\n\n#run ls on crunchomics to check if you moved the data successfully\nll data/seq_project/*/*fastq.gz\n\n\n\n\n\n\n\nTipTip: Moving data from the HPC to our own computer\n\n\n\n\n\nWe can also move data from the HPC to our own computer. For example, let’s assume we want to move a single sequencing file from crunchomics back to our computer. In this case,\n\nWe do not need -r since we only move a single file\nWe again run this command from a terminal on our computer, not while being logged in the HPC\nWe use . to indicate that we want to move the file into the directory we are currently in. If we want to move the file elsewhere we can use any absolute or relative path as needed\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/data/seq_project/barcode001_User1_ITS_1_L001/Sample-DUMMY1_R1.fastq.gz .\n\n\n\n\n\n\n\n\n\n\nTipTip: Moving data from the HPC using wildcards\n\n\n\n\n\nWe can also move several files at once and ignore the whole folder structure by using wildcards when using scp.\n\n#make a random directory in our Crunchomics working directory to move our data into\n#again, we generate this folder on Crunchomics\nmkdir transfer_test\n\n#move files from crunchomics to our local computer\n#again, always run scp from your local computer\nscp data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\n#view the data, see how the folder structure is different compared to our first example? \n#since we moved the data TO Crunchomics, we run ls while being logged into Crunchomics\nll transfer_test/*\n\nNotice for MAC users:\nFor Mac users that work with an zsh shell the command above might not work and you might get an error like “file not found”, “no matches found” or something the like. Without going into details zsh has a slightly different way of handling wildcards and tries to interpret the wildcard literally and thus does not find our files. If you see the error and you are sure the file exists it should work to edit your line of code as follows:\n\n\\scp  data/seq_project/barcode00*/*fastq.gz username@omics-h0.science.uva.nl:/home/username/personal/projectX/transfer_test\n\nIf that does not work, these are some other things to try (different zsh environments might need slightly different solutions):\n\nnoglob scp data/seq_project/barcode00*/*fastq.gz ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\nscp 'data/seq_project/barcode00*/*fastq.gz' ndombro@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/transfer_test\n\n\n\n\n\n\n\n\n\n\nTipTip: Moving files using the FileZilla GUI\n\n\n\n\n\nscp takes some getting used to and there are several graphical user interface programs that are available which provide the same functionality. Here, we will look at FileZilla, as it is compatible with Windows, Mac, and Linux.\nTo use FileZilla, do the following:\n\nDownload the FileZilla Client here. The free version is all we need.\nFollow the download instructions\nClick the FileZilla app to start it\nAdd Uva user credentials\n\nEnter omics-h0.science.uva.nl in host\nEnter user_name in user\nEnter psswd chosen when making surf account\nPort should be 22 to open a sftp connection\n\nTrust the host\nYou land in your home directory (on the right hand side)\nYou can copy-paste data by drag and dropping",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#slurm-basics",
    "href": "source/hpc_howto.html#slurm-basics",
    "title": "5  Using an HPC",
    "section": "5.4 Slurm basics",
    "text": "5.4 Slurm basics\n\n5.4.1 Get information about the cluster\nNow, that we have our data prepared we want to run a tool to assess the quality of our reads. Before doing that, let’s talk about submitting jobs to an HPC.\nAs a reminder: We do not run big jobs on the login node but need to submit such jobs to the compute nodes via SLURM. Login nodes are for preparing your programs to run and you run your actual jobs by submitting them to the compute nodes using SLURM.\nIn the next few sections you will get to know the basics steps to be able to do this.\nBefore doing this it is, however, useful when getting started on a new HPC to know how to get basic information about what nodes are available on a cluster and how busy an HPC is. This allows us to better know how many resources to request in order to have our job run efficiently but also get started in a timely manner.\nOne way to get a list of available compute nodes is by typing the following command into the cli:\n\nsinfo\n\nWe will see something like this:\n\n\n\n\n\nHere, the different columns you see are:\n\npartition: tells us what queues that are available. There are few partitions on Crunchomics and you do not need to define them in our case. However, other systems use partitions to group a subset of nodes with different type of hardwares or a specific maximum wall time. For example, you might have specific partitions for memory-heavy versus time-intensive jobs.\nstate: tells you if a node is busy or not. Here:\n\nmix : consumable resources partially allocated\nidle : available to requests consumable resources\ndrain : unavailable for use per system administrator request\nalloc : consumable resources fully allocated\ndown : unavailable for use.\n\nNodes: The number of nodes available for each partition\nNodeList: the names of the compute nodes (i.e. omics-cn001 to omics-cn005)\n\n\n\n5.4.2 View info about jobs in the queue\nThe following command gives us some information about how busy the HPC is:\n\nsqueue\n\nAfter running this, you can see all jobs scheduled on the HPC, which might look something like this:\n\n\n\n\n\n\nJOBID: every job gets a number and you can manipulate jobs via this number\nST: Job state codes that describe the current state of the job. The full list of abbreviations can be found here\n\nIf we would have submitted a job, we also should see the job listed there.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#srunsubmitting-a-job-interactively",
    "href": "source/hpc_howto.html#srunsubmitting-a-job-interactively",
    "title": "5  Using an HPC",
    "section": "5.5 srun:Submitting a job interactively",
    "text": "5.5 srun:Submitting a job interactively\nsrun is used when you want to run tasks interactively or want to have more control over the execution of a job. You directly issue srun commands in the terminal and you at the same time are able to specify the tasks to be executed and their resource requirements. For example, you might want to run softwareX and request that this job requires 10 CPUs and 5 GB of memory.\nUse srun when:\n\nYou want to run tasks interactively and need immediate feedback printed to the screen\nYou are testing or debugging your commands before incorporating them into a script\nYou need more control over the execution of tasks\nTypically, you use srun for smaller jobs that do not run for too long, i.e. a few hours\n\nLet’s submit a very simple example for which we would not even need to submit a job, but just to get you started.\n\nsrun echo \"Hello interactively\"\n\nYou should see the output of echo printed to the screen and if you would run squeue you won’t even see your job since everything ran so fast. But congrats, you communicated the first time with the compute node.\nNow assume you want to run a more complex interactive task with srun that might run longer and benefit from using more CPUs. In this case you need to specify the resources your job needs by adding flags, i.e. some of which you see here:\n\nsrun --cpus-per-task=1 --mem=1G --time=00:10:00 echo \"Hello interactively\"\n\nThe different flags mean the following:\n\n--cpus-per-task=1: Specifies the number of CPUs per task. Adjust this based on the computational requirements of your task\n--mem=1G: Sets the memory requirement for the task. Modify this based on your task’s memory needs\n--time=00:10:00 Sets the time limit to 10 minutes\necho \"Hello interactively: The actual command you want to run interactively\n\nOther flags:\n\n--nodes=1: Specifies the number of nodes. In this case, it’s set to 1 and tells slurm that we want to use a full node. Only use this if you make use of all resources on that node, otherwise omit. In our case this definitely is over-kill to request a full node with 64 CPUs to print a single line of text to the screen.\n--ntasks=1: Defines the number of tasks to run. Here, we would set it to 1 since we want to use echo once\n\n\n5.5.1 Choosing the right amount of resources\nWhen you’re just starting, deciding on the right resources to request for your computational job can be a bit challenging. The resource requirements can vary significantly based on the specific tool or workflow you are using. Here are some general guidelines to help you make informed choices:\n\nUse default settings: Begin by working with the default settings provided by the HPC cluster or recommended by the tool itself. These are often set to provide a balanced resource allocation for a wide range of tasks\nCheck the software documentation: Consult the documentation of the software or tool you are using. Many tools provide recommendations for resource allocation based on the nature of the computation.\nTest with small datasets: For initial testing and debugging especially when working with large datasets, consider working with a smaller subset of your data. This allows for faster job turnaround times, helping you identify and resolve issues more efficiently.\nMonitor the resources usage:\n\nUse sacct to check what resources a finished job has used and see whether you can optimize a run if you plan to run similar jobs over and over again. An example command would be sacct -j 419847 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,ncpus. In the report, look for columns like MaxRSS (maximum resident set size) to check if the amount of memory allocated (–mem) was appropriate.\nEnsuer that the job used the resources you requested. For instance, if you would have used --cpus-per-task=4 --mem=4G, you would expect to use a total of 16 GB of memory (4 CPUs * 4 GB). You can verify this with sacct to ensure your job’s resource requirements align with its actual usage.\n\nFine-Tuning Resource Requests: If you encounter performance issues or your jobs are not completing successfully, consider iteratively adjusting resource requests. This might involve increasing or decreasing the number of CPUs, memory allocation, or other relevant parameters.\n\n\n\n5.5.2 Run FastQC with srun\nFastQC is a quality control tool for high throughput sequence data that is already installed on Crunchomics. This will be the actual software that we now want to run to look more closely at the quality of the sequencing data that we just uploaded to Crunchomics.\nBy the way: Whenever running a software for a first time, it is useful to check the manual, for example with fastqc -h.\nLet’s start by setting up a clean folder structure to keep our files organized. I start with making a folder in which I want to store all results generated in this analysis and by using the -p argument I generate a fastqc folder inside the results folder at the same time. Then, we can submit our job using srun.\n\nmkdir -p results/fastqc \n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\nll results/fastqc/*\n\nSince we work with little data this will run extremely fast despite only using 1 CPU (or thread, in our case these two words can be used interchangeably). However, if you would be logged into Crunchomics via a second window and run squeue you should see that your job is actively running (in the example below, the job we submitted is named after the software and got the jobid 746):\n\n\n\n\n\nAdditionally, after the run is completed, you should see that several HTML files were generated in our fastqc folder.\n\n\n\n\n\n\nCautionExercise\n\n\n\n\n\nUse scp to download the HTML files generated by fastqc to your own computer and view one of the HTML files.\n\n\nClick me to see an answer\n\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/projectX/results/fastqc/*html results/fastqc/\n\nYou could also open a file on Crunchomics with firefox results/fastqc/Sample-DUMMY1_R1_fastqc.html. However, connecting to the internet via a cli on a remote server tends to be rather slow and its often better to view files on your own computer especially if they are large files.\nIf you want to know more about how to to interpret the output, you can visit the fastqc website, which gives some examples for interpreting good and bad reports.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#screen-submitting-long-running-jobs-via-srun",
    "href": "source/hpc_howto.html#screen-submitting-long-running-jobs-via-srun",
    "title": "5  Using an HPC",
    "section": "5.6 screen: Submitting long running jobs via srun",
    "text": "5.6 screen: Submitting long running jobs via srun\nOne down-side of srun for long running jobs is that your terminal gets “blocked” as long as the job is running and your job will be aborted if you loose the ssh connection to the HPC. For long running jobs, there are two ways to deal with this:\n\nsubmit a srun job in a screen\nuse sbatch\n\nIn this section, we will cover how to use screen. Screen or GNU Screen is a terminal multiplexer. This means that you can start a screen session and then open any number of windows (virtual terminals) inside that session.\nProcesses running in Screen will continue to run when their window is not visible even if you get disconnected. This is perfect, if we start longer running processes on the server and want to shut down our computer when leaving for the day. As long as the server is still connected to the internet, your process will continue running.\nWe start a screen as follows:\n\nscreen\n\nWe detach (go outside of a screen but keep the screen running in the background) from a screen by pressing control+a+d.\nIf you want to run multiple analyses in multiple screens at the same time, then can be useful to give your screens more descriptive names. You can give screens a name using the -S option:\n\nscreen -S run_fastqc\n\nAfter detaching from this screen again with control+a+d you can create a list of all currently running screens with:\n\nscreen -ls\n\nYou can re-connect to an existing screen like this:\n\nscreen -r run_fastqc\n\nNow inside our screen, we can run fastqc same as we did before (but now don’t risk to loose a long-running job):\n\nsrun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1 \n\nFor long-running jobs we can start multiple screens at once or even if we just have one screen open, close it and leave for the day or simply work on other things on the cli outside of the screen.\nIf you want to completely close and remove a screen, type the following and press enter while being inside of the screen:\n\nexit",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#sbatch-submitting-a-long-running-job",
    "href": "source/hpc_howto.html#sbatch-submitting-a-long-running-job",
    "title": "5  Using an HPC",
    "section": "5.7 sbatch: submitting a long-running job",
    "text": "5.7 sbatch: submitting a long-running job\nsbatch is your go-to command when you have a script (i.e. a batch script) that needs to be executed without direct user interaction.\nUse sbatch when:\n\nYou have long-running or resource-intensive tasks\nYou want to submit jobs that can run independently without your immediate supervision\nYou want to submit multiple jobs at once\n\nTo run a job script, you:\n\ncreate a script that contains all the commands and configurations needed for your job\nuse sbatch to submit this script to the Slurm scheduler, and it takes care of the rest.\n\nLet’s start with generating some new folders to keep our project folder organized:\n\nmkdir scripts \nmkdir logs\n\nTo get started, assume we have created a script in the scripts folder named run_fastqc.sh with the content that is shown below. Notice, how in this script I added some additional echo commands? I just use these to print some information about the progress which could be printed to a log file but if you have several commands that should be executed after each other that is how you would do it.\n\n#!/bin/bash\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G      \n#SBATCH --time=1:00:00\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\nIn the job script above:\n\n#!/bin/bash . This so-called Shebang line tells the shell to interpret and run the Slurm script using the bash shell. This line should always be added at the very top of your SBATCH/Slurm script.\nThe lines that follow and start with # are the lines in which we define the amount of resources required for our job to run. In our case, we request 1 CPU, 5G of memory and 1 hour time limit.\nIf your code needs any dependencies, such as conda environments, you would add these dependencies here. We do not need this for our example here, but you might need to add something like this conda activate my_env if you have installed your own software. We will touch upon conda environments and installing software a bit later.\nThe lines afterwards are the actually commands that we want to run on the compute nodes, We also call this the job steps.\n\nTo prepare the script and run it:\n\nRun nano scripts/run_fastqc.sh to generate an empty jobscript file\nAdd the code from above into the file we just opened\nPress ctrl+x to exit nano\nType Y when prompted if the changes should be saved\nConfirm that the file name is good by pressing enter\n\nAfterwards, you can submit run_fastqc.sh as follows:\n\n#submit job: 754\nsbatch scripts/run_fastqc.sh\n\n#check if job is running correctly\nsqueue\n\nAfter running this, you should see that the job was submitted and something like this printed to the screen Submitted batch job 754. You will also see that a new file is generated that will look something like this slurm-754.out.\nWhen you submit a batch job using sbatch, Slurm redirects the standard output and standard error messages, which you have seen printed to the screen when you used srun, to a file named in the format slurm-JOBID.out, where JOBID is the unique identifier assigned to your job.\nThis file is useful as it:\n\nCaptures the output of our batch scripts and stores them in a file\nCan be used for debugging, since if something goes wrong with your job, examining the contents of this file can provide valuable insights into the issue. Error messages, warnings, or unexpected outputs are often recorded here\n\nFeel free to explore the content of the log file, do you see how the echo commands are used as well?\n\n\n\n\n\n\nTipTip: sbatch and better log files\n\n\n\n\n\nWe have seen that by default sbatch redirects the standard output and error to our working directory and that it decides itself how to name the files. Since file organization is very important especially if you generate lots of files, you find below an example to:\n\nStore the standard output and error in two separate files\nRedirect the output into another folder, the logs folder\nIn the code below, the %j is replaced with the job allocation number once the log files are generated\n\n\n#!/bin/bash\n#SBATCH --job-name=our_fastqc_job\n#SBATCH --output=logs/fastqc_%j.out\n#SBATCH --error=logs/fastqc_%j.err\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n#SBATCH --time=1:00:00\n\necho \"Start fastqc\"\n\nfastqc data/seq_project/*/*gz -o results/fastqc  --threads 1\n\necho \"fastqc finished\"\n\n\n\n\n\n\n\n\n\n\nTipAdvanced tip: sbatch arrays to run multiple files in parallel\n\n\n\n\n\nWith fastqc we are very lucky that the tool can identify all the fastq files in the directory we specify with -o by making use of the wildcard. This is extremely useful for us but by far not all programs work this way.\nFor this section, lets assume that we need to provide each individual file we want to analyse, one by one, and can not provide a folder name. How would we run such a job effectively?\nWhat we want to do is created what is called a job array that allows us to:\n\nRun multiple jobs that have the same job definition, i.e. cpus, memory and software used\nRun these job in the most optimal way. I.e. we do not want to run one job after each other but we also want to run jobs in parallel at the same time to optimize resource usage.\n\nLet’s start with making a list with files we want to work with based on what we have already learned:\n\nls data/seq_project/*/*.gz | cut -f4 -d \"/\" &gt; samples.txt\n\nNext, we can use this text file in our job array, the content of which we store in scripts/array.sh:\n\n#!/bin/bash\n\n#SBATCH --job-name=my_array\n#SBATCH --output=logs/array_%A_%a.out\n#SBATCH --error=logs/array_%A_%a.err\n#SBATCH --array=1-8\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=5G\n\n#calculate the index of the current job within the batch\n#in our case the index will store the values 1 to 8 for our 8 files\nINDEX=$((SLURM_ARRAY_TASK_ID ))\n\n#build an array structure that stores the fastq.gz file names\nCURRENT_SAMPLE=$(cat samples.txt |  sed -n \"${INDEX}p\")\n\n#print what is actually happening\necho \"Now Job${INDEX} runs on ${CURRENT_SAMPLE}\"\n\n#run the actual job\nfastqc data/seq_project/*/${CURRENT_SAMPLE} -o results/fastqc  --threads 1\n\nIn the script we use some new SLURM arguments:\n\n#SBATCH --array=1-8: Sets up a job array, specifying the range (1-8). We choose 1-8 because we have exactly 8 fastq.gz files we want to analyse\n#SBATCH --output=logs/array_%A_%a.out: Store the standard output and error. %A represents the job ID assigned by Slurm, and %a represents the array task ID\nImportant: Are your individual arrays large jobs that use a lot of CPUs/memory? Then you might not want to submit all at the same time but submit them in a step-wise manner. With #SBATCH --array=1-8%2 you would only submit the first two jobs and only after these have finished the next jobs. Remember, try to use not more than ~20% of the crunchomics cluster at the same time.\n\nThe job does the following:\n\nThe INDEX variable is storing the value of the current SLURM_ARRAY_TASK_ID. This represents the ID of the current job within the array. In our case this will be first 1, then 2, …, and finally 8.\nNext, we build the array structure in which the CURRENT_SAMPLE variable is created by:\n\nReading the sample_list.txt file with cat\nUsing a pipe to extract the file name at the calculated index using sed. Sed is an extremely powerful way to edit text that we have not covered here but -n 1p is a option that allows us to print one specific line of a file, in our case the first one when running array 1. So for the first array the actual code run is the following cat samples.txt |  sed -n \"1p\". For the next array, we would run cat samples.txt |  sed -n \"2p\" and so forth.\nThe output of the pipe is stored in a variable, called CURRENT_SAMPLE. For our first sample this will be Sample-DUMMY1_R1.fastq.gz\n\nWe use echo to record what was executed when and store this information in the standard output\nWe run our actual fastqc job on the file name that is currently stored in the CURRENT_SAMPLE variable.\n\nIf we check what is happening right after submitting the job with squeue we should see something like this:\n\n\n\n\n\nWe see that jobs 1-4 are already running and the jobs 5-8 are currently waiting for space. That is one of the useful things using a job manager such as SLURM. It takes care of finding the appropriate resources on all nodes for us as long as we defined the required cpus and memory sensibly.\nIf we check the log files we should see something like this:\n\n\n\n\n\nWe see that we get an individual output and error file for each job. In the output we see what value is stored in the INDEX, here 1, and the CURRENT_SAMPLE, here Sample-DUMMY1_R1.fastq.gz and that the analysis finished successfully.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#installing-software",
    "href": "source/hpc_howto.html#installing-software",
    "title": "5  Using an HPC",
    "section": "5.3 Installing software",
    "text": "5.3 Installing software\nThere might be cases where the software you are interested in is not installed on the HPC you are working with (or on your own computer).\nIn the majority of cases, you should be able to install software by using a package management system, such as conda or mamba. These systems allow you to find and install packages in their own environment without administrator privileges. Let’s have a look at a very brief example:\n\n5.3.1 Install conda/mamba\nA lot of systems already come with conda/mamba installed, however, if possible we recommend working with mamba instead of conda. mamba is a replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.\nIf you have conda installed and do not want to install anything else, that is fine. Just replace all instances of mamba with conda below.\nThis command should work in most cases to setup conda together with mamba:\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\nWhen running the bash command, you get asked a few questions:\n\nRead the license and use arrow down to scroll down. Don’t scroll too fast, so that you see the next question\nDecide where conda gets installed. You can say yes to the default location in your home. But don’t forget that for Crunchomics your home only has 25G of space. You could also install the miniforge/mambaforge folder in your personal folder instead.\nSay yes, when you get asked whether conda should be initialized during start up\nRestart the shell (exit the terminal and use ssh to log back in) for the changes to take effect\nCheck if conda is accessible by running conda -h\n\n\n\n5.3.2 Setting up an environment\nLet’s assume we want to install seqkit, a tool that allows us to calculate some statistics for sequencing data such as the number of sequences or average sequence length per sample.\nWe can install seqkit into a separate environment, which we can give a descriptive name, as follows:\n\n#check if the tool is installed (should return \"command not\" found if the software is not installed)\nseqkit -h\n\n#create an empty environment and name it seqkit and we add the version number to the name\n#this basically sets up seqkit separate from our default working environment\n#this is useful whenever software require complicated dependencies allowing us to have a separate install away from software that could conflict with each other\nmamba create -n seqkit_2.6.1\n\n#install seqkit, into the environment we just created\nmamba install -n seqkit_2.6.1 -c bioconda seqkit=2.6.1\n\n#to run seqkit, we need activate the environment first\nmamba activate seqkit_2.6.1\n\n#check if tool is installed, \n#if installed properly this should return some detailed information on how to run seqkit\nseqkit -h\n\n#run the tool via srun\nmkdir results/seqkit\nsrun --cpus-per-task 2 --mem=4G seqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\nless -S results/seqkit/seqkit_stats.tsv \n\n#close the environment\nconda deactivate\n\nWhen installing seqkit, we:\n\nspecify the exact version we want to download with =2.6.1. We could also install the newest version that conda/mamba can find by running mamba install -n seqkit -c bioconda seqkit.\nspecify that we want to look for seqkit in the bioconda channel with the option -c. Channels are the locations where packages are stored. They serve as the base for hosting and managing packages. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. If you are unable to find a package it might be that you need to specify a channel.\n\nForgot what conda environments you installed in the past? You can run conda env list to generate a list of all existing environments.\nUnsure if a software can be installed with conda? Google conda together with the software name, which should lead you do a conda web page. This page should inform you whether you need to add a specific channel to install the software as well as the version numbers available.\nA full set of mamba/conda commands can be found here.\n\n\n\n\n\n\nCautionExercise\n\n\n\n\n\n\nDownload and view the file results/seqkit/seqkit_stats.tsv on your own computer\nRun the seqkit again but this time submit the job via a sbatch script instead of using srun. Notice, that you need to tell SLURM how it can activate the conda environment that has seqkit installed. You might need to google how to do that, since this requires some extra line of code that we have not covered yet but see this as a good exercise for how to handle error messages that you see in the log files for your own analyses\n\n\n\nClick me to see an answer\n\n\n#question 1\nscp username@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/results/seqkit/seqkit_stats.tsv .\n\n#question 2 \nsbatch scripts/seqkit.sh\n\nContent of scripts/seqkit.sh:\n\n#!/bin/bash\n#SBATCH --job-name=seqkit_job\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=5G\n\n#activate dependencies\nsource ~/.bashrc\nmamba activate seqkit_2.6.1\n\n#run seqkit\necho \"Start seqkit\"\n\nseqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2\n\necho \"seqkit finished\"\n\nIn the script above, we see that we need to add two lines of code to activate the seqkit conda environment:\nsource ~/.bashrc\nmamba activate seqkit_2.6.1\nWhen you run a script or a command, it operates in its own environment. The source command is like telling the script to look into another file, in this case, ~/.bashrc, and execute the commands in that file as if they were written directly into the script.\nHere, source ~/.bashrc is telling the script to execute the commands in the ~/.bashrc file. This is typically done to set up environment variables, paths, and activate any software or tools that are required for the script to run successfully. In our case this tells Slurm where we have installed conda and thus enables Slurm to use conda itself.\nThis allows slurm to, after executing source ~/.bashrc, activates a Conda environment using mamba activate seqkit_2.6.1. This ensures that the SeqKit tool and its dependencies are available and properly configured for use in the subsequent part of the script.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#connecting-to-the-hpc",
    "href": "source/hpc_howto.html#connecting-to-the-hpc",
    "title": "5  Using an HPC",
    "section": "",
    "text": "5.1.1 ssh: Connecting to a sever\nSSH (Secure Shell) is a network protocol that allows you to securely connect to a remote computer. On an HPC, you use SSH to log in and run commands on the server. The general command looks like this:\n\nssh -X username@server\n\nHere:\n\nusername is your account name on the HPC, i.e. your UvanetID\nserver is the address of the HPC you want to connect to, for Crunchomics this is omics-h0.science.uva.nl\n-X enables X11 forwarding, which allows graphical applications from the server (like plotting or viewing images) to appear on your local machine. “Untrusted” X11 forwarding means the server can send graphical output, but it has limited access to your local machine\n\nFor Crunchomics at UVA, you would use:\n\nssh -X uvanetid@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nImportant tips for connecting:\n\nIf you want to log into Crunchomics while working from UvA use the eduroam network, not the open Amsterdam Science Park network\n\nIf you are connecting from outside UVA, you must be on the VPN. Contact ICT if you have not set it up or encounter issues\nAlways double-check your username and server address to avoid login errors\n\n\n\n\n\n5.1.2 Crunchomics: Preparing your account\nIf this is your first time using Crunchomics, you need to run a small Bash script to set up your account. This script will:\n\nAdd /zfs/omics/software/bin to your PATH. This basically allows Bash to locate and use the system-wide installed software available on Crunchomic\nSet up a Python 3 environment with some useful Python packages pre-installed\nCreate a link to your 500 GB personal directory inside your home directory, giving you plenty of space to store data and results\n\nTo set up your account, run the following command:\n\n# First orient yourself by typing \npwd \nls -lh \n\n# Run the Crunchomics installation script\n/zfs/omics/software/script/omics_install_script\n\n# Check if something changed\n# You now should see the personal folder inside your home directory \nls -lh\n\n\n\n5.1.3 conda: Setting up your own software environment\nMany bioinformatics tools are already installed on Crunchomics, but sometimes you’ll need additional ones (like NanoPlot for analysing long-read data). To manage and install your own tools, we use conda, a popular environment manager that lets you create isolated environments.\n\n5.1.3.1 Install conda/mamba\nMany systems already include conda or mamba. Before installing a new copy, check if it’s already available:\n\nwhich conda\nwhich mamba\n\nIf either command returns a path (not “command not found”), you can skip installation and move on.\nTo install Miniforge (which includes conda and mamba by default), run:\n\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n\nDuring installation:\n\nRead the license (scroll by pressing enter)\nChoose the installation location\n\nOn Crunchomics, your home directory has only ~25 GB. Therefore install everything into the personal folder: /zfs/omics/personal/uvanetID/miniforge3\nThe default (/home//miniforge3) is fine for other setups\n\nSay “yes” when asked if conda should be initialized\nRestart your shell (exit and log back in)\nVerify the installation:\n\n\nconda -h\nmamba -h\n\n\n\n5.1.3.2 Installing NanoPlot\nNanoPlot is a visualization tool for long-read sequencing data. We can easily install it using mamba (or conda if mamba is unavailable).\n\n# Check if NanoPlot is already available\nNanoPlot -h\n\n# If not installed, create a new environment called nanoplot and install it\n# Press \"Y\" when prompted about confirming the installation\nmamba create -n nanoplot -c bioconda nanoplot\n\n# Activate the new environment\nconda activate nanoplot \n\n# Check if the tool is installed\nNanoPlot -h\n\n# Exit the \nconda deactivate\n\nRecommendations:\n\nWhenever possible, use mamba instead of conda, it resolves dependencies faster\nKeep your base environment clean (the base environment is the conda environment you start with when you log into the HPC). Always create new environments for each tool (conda create -n toolname …) instead of installing everything in base\nCheck the tool’s documentation for specific installation notes or version requirements\nYou can install specific versions when needed with mamba create -n nanoplot_v1.42 -c bioconda nanoplot=1.42\nYou can remove environments you no longer need with conda env remove -n nanoplot\n\n\n\n\n5.1.4 Preparing your working directory\nNext, you can start organizing your project files. You always want to generate project folders inside your personal directory, which provides more storage than the home directory:\n\n# Go into the personal folder\n# You want generally generate data here, since the personal folder comes with more space than the home directory\ncd personal \n\n# Make and go into a project folder \nmkdir data_analysis\ncd data_analysis \n\n# Ensure that we are in the right folder \npwd\n\nYou now have a clean workspace for all your analyses on Crunchomics. As in your local machine, it’s good practice to keep your raw data, results, and scripts organized in separate folders.\n\n\n5.1.5 scp: Transferring data from/to a server\nTo learn how to transfer data to Crunchomics, we first want to copy our local data folder (from your personal computer) to the HPC. We can do this with the scp(Secure Copy Protocol) command, which securely transfers files or directories between your computer and a remote server.\nThe basic syntax is:\n\nscp [options] SOURCE DESTINATION\nFor connecting to any HPC the syntax is server:file_location\n\nWhen transferring data you must run the command from your own computer, not from inside the HPC login session.\nTo copy the entire data folder into your Crunchomics project folder, use:\n\n# Run this from your local terminal (not while logged into the HPC)\n# Replace 'username' with your UvAnetID\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/data_analysis\n\n# Then check on the HPC that the files arrived:\nll data/*.fna\n\nNotes:\n\nThe -r option copies directories recursively, i.e. you want to copy everything in the directory(recursively)\nIt’s helpful to keep two terminal windows open: one for your local computer and one for the HPC\nRun this command from inside your local data_analysis folder (the one you created earlier)\n\n\n\n\n\n\n\nTipTip: Moving data from the HPC to our own computer\n\n\n\n\n\nYou can also move results back to your computer. For example, to copy a single genome file from Crunchomics:\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/data_analysis/data/LjRoot303.fna .\n\nHere, the . at the end means “copy to the current directory” on your local machine. If you want to copy to another location, replace . with a path.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#submitting-jobs-with-slurm",
    "href": "source/hpc_howto.html#submitting-jobs-with-slurm",
    "title": "5  Using an HPC",
    "section": "5.2 Submitting jobs with SLURM",
    "text": "5.2 Submitting jobs with SLURM\nIn the HPC introduction, you learned that there are two main “sections” on an HPC:\n\nLogin node: where you log in and do light work, such as moving files, editing scripts, or preparing jobs.\nCompute nodes: where the actual analyses run. These nodes have more CPUs and memory and are managed by a scheduling system.\n\nTo use these compute nodes efficiently, we communicate with them through SLURM, a workload manager that decides when, where, and how your jobs run based on the available resources.\n\n5.2.1 squeue: View info about jobs in the queue\nThe command below shows you which jobs are currently running or waiting in the queue:\n\nsqueue\n\nThis displays all jobs submitted to the system, and might look something like this:\n\n\n\n\n\nSome useful columns to know:\n\nJOBID – a unique number assigned to each job; you can use it to check or cancel a job later.\nST – the current state of the job (e.g., R = running, PD = pending).\nA full list of SLURM job state codes can be found here\n\nIf you have already submitted a job, it will appear in this list. To see only your own jobs, you can add the -u flag followed by your username:\n\nsqueue -u $USER\n\n\n\n5.2.2 srun:Submitting a job interactively\nNow that we’ve seen how to check the job queue, let’s learn how to actually run jobs on the compute nodes. There are two main ways to submit jobs: with srun and with sbatch. We’ll start with srun, which you typically use when:\n\nYou want to run tasks interactively and see the output directly on your screen\nYou are testing or debugging your commands before writing a job script\nYou have short jobs (usually less than a few hours)\n\n\nImportant: Jobs started with srun will stop if you disconnect from the HPC (unless you use tools like screen or tmux, which we won’t cover in this tutorial).\n\nLet’s start with a very simple example — a “Hello world” job — just to see how srun works:\n\nsrun --cpus-per-task=1 --mem=1G --time=00:10:00 echo \"Hello world\"\n\nHere’s what each part means:\n\n--cpus-per-task=1 → request 1 CPU core\n--mem=1G → request 1 GB of memory\n--time=00:10:00 → set a maximum runtime of 10 minutes\necho \"Hello world\" → the actual command to run (prints text to the screen)\n\nSo the arguments after srun and before echo tell SLURM what resources to allocate on the compute node.\n\n\n5.2.3 Running a analysis with Seqkit interactively\nNext, let’s use srun for something more useful. We’ll analyze our genomes using seqkit, a fast toolkit for inspecting FASTA/FASTQ files.\n\nTip: Whenever you use a tool for the first time, check its help page (e.g., seqkit -h) to see all available options.\n\n\n# Create a results folder\nmkdir -p results/seqkit \n\n# Run seqkit interactively\nsrun --cpus-per-task=1 --mem=5G seqkit stats data/*fna -Tao results/seqkit/genome_stats.tsv --threads 1\n\n# View the results (press 'q' to exit less)\nless -S results/seqkit/genome_stats.tsv \n\nHere:\n\nseqkit stats data/*fna → analyzes all FASTA files in the data folder ending in .fna\n-Tao → output in tabular format (-T), include all stats (-a), and write to file (-o)\n--threads 1 → run on one thread (should match --cpus-per-task=1)\nThe results are stored in results/seqkit/genome_stats.tsv\n\nThis job runs very quickly — so fast that it might not even appear in the queue when you type squeue.\nWhen you open the results file, you’ll see:\n\nThe number of contigs (num_seqs), this should match what we saw earlier\nThe total genome size in bp (sum_len)\nAdditional metrics such as the minimum, maximum, average sequence length, and N50, a common measure of genome fragmentation.\n\n\n\n\n\n\n\nTipTip: Choosing the right amount of resources\n\n\n\n\n\nWhen starting out, choosing the right amount of CPUs, memory, or runtime can be tricky. Here are a few rules of thumb:\n\nStart small, most tools don’t need huge resources for small test runs\nCheck the tool’s documentation, many list recommended resource settings\nTest on a subset of your data first. It runs faster and helps you debug\nIf your job fails or runs out of memory, increase the resources gradually\nOnce you have a stable workflow, you can scale up\n\nRemember: over-requesting resources can make jobs wait longer in the queue — and can block others from running.\n\n\n\n\n\n5.2.4 sbatch: submitting a long-running job\nWhile srun is great for quick, interactive runs, most analyses on the HPC are submitted with sbatch, which lets jobs run in the background even after you log out.\nUse sbatch when:\n\nYou have long or resource-intensive analyses\nYou want jobs to run unattended\nYou plan to run multiple jobs in sequence or parallel\n\n\n5.2.4.1 Step 1: Create folders for organization\nWe’ll keep our project tidy by creating dedicated folders for scripts and logs:\n\nmkdir -p scripts logs\n\n\n\n5.2.4.2 Step 2: Write a job script\nUse nano (or another editor) to create a script file:\n\nnano scripts/run_seqkit.sh\n\nThen add the following content:\n\n#!/bin/bash\n#SBATCH --job-name=seqkit\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n#SBATCH --time=1:00:00\n\necho \"Seqkit job started on:\" \ndate\n\nsrun --cpus-per-task=1 --mem=5G seqkit stats data/*fna -Tao results/seqkit/genome_stats_2.tsv --threads 1\n\necho \"Seqkit job finished on:\"\ndate\n\nSave and exit:\n\nPress Ctrl+X to exit, then Y to confirm that you want to safe the file, then Enter.\n\n\n\n5.2.4.3 Step 3: Submit and monitor your job\n\n# Submit \nsbatch scripts/run_seqkit.sh\n\n# Check job status (may run too fast to appear)\nsqueue -u $USER\n\nWhen submitted successfully, you’ll see something like Submitted batch job 754 and new log files will appear in your logs folder:\n\nseqkit_&lt;jobID&gt;.out → standard output (results and messages)\nseqkit_&lt;jobID&gt;.err → error log (check this if your job fails)\n\n\n\n5.2.4.4 Step 4: Understanding your job script\nIn the job script above:\n\n#!/bin/bash: Runs the script with the Bash shell\n#SBATCH --... SLURM options: resources, runtime, output names.\n\nThese are the same options that you have used with srun\nThe %j variable in your log filenames automatically expands to the job ID, making it easier to keep track of multiple submissions.\n\necho / date: Prints status messages to track job progress\nsrun ...: The actual analysis command to run on the compute node\n\n:: {.callout-tip title=“Tip: Debugging your first sbatch jobs” collapse=true}\nIf your job fails:\n\nCheck your .err/.out file in the logs/ folder, these files usually tell you exactly what went wrong\nConfirm that your input files and paths exist\nTry running the main command interactively with srun first, if it works there, it will work in a script.\n\n\n\n5.2.4.5 Step 5: scancel: Cancelling a job\nSometimes you realize a job is stuck, misconfigured, or using the wrong resources. You can easily cancel it with:\n\nscancel &lt;jobID&gt;\n\nFor example, if your job ID was 754:\n\nscancel 754\n\n\n\n\n\n\n\nTipTip: Good HPC etiquette\n\n\n\nAlways cancel jobs that are running incorrectly or stuck in a queue too long. This frees resources for others and avoids unnecessary load on the cluster.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Using an HPC</span>"
    ]
  }
]