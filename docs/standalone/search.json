[
  {
    "objectID": "source/terminal.html",
    "href": "source/terminal.html",
    "title": "1  Setting up a terminal",
    "section": "",
    "text": "1.1 Terminology\nThe command-line interface (CLI) is an alternative to a graphical user interface (GUI), with which you are likely more familiar. Both allow you to interact with your computer’s operating system but in a slightly different way:\nThe CLI is commonly called the shell, terminal, console, or prompt. These terms are related but not identical:\nThere are several types of shells — for example, bash or zsh (that use slightly different languages to issue commands). This tutorial was written on a computer that uses bash, which stands for Bourne Again Shell.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#terminology",
    "href": "source/terminal.html#terminology",
    "title": "1  Setting up a terminal",
    "section": "",
    "text": "In a GUI, you click buttons, open folders, and use menus\nIn the CLI, you type text to issue commands and see text output in the terminal\n\n\n\nThe terminal (or console) is the window or program that lets you type commands.\nThe shell is the program that interprets the commands you type inside the terminal and tells the operating system what to do.\nA prompt is the text displayed by the shell that indicates that it is ready to accept a command. The prompt often shows useful information, like your username, machine name, and current directory. This is one example for how a prompt can look like: user@machine:~$",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#installation-guides",
    "href": "source/terminal.html#installation-guides",
    "title": "1  Setting up a terminal",
    "section": "1.2 Installation guides",
    "text": "1.2 Installation guides\n\n1.2.1 Linux\nIf you’re using Linux, you already have everything you need and you don’t need to install anything. All Linux systems come with a terminal and a shell, and the default shell is usually Bash.\nYou can open a terminal from your applications menu or by searching for Gnome Terminal, KDE Konsole, or xterm, depending on your desktop environment.\nTo confirm which shell you’re using, type:\n\necho $SHELL\n\n\n\n1.2.2 Mac\nAll Mac computers also come with a built-in terminal and shell. To open the terminal:\n\nIn Finder, go to Go → Utilities, then open Terminal.\nOr use Spotlight Search (⌘ + Space), type Terminal, and press Return.\n\nThe default shell depends on your macOS version:\n\nmacOS Mojave (10.14) or earlier → Bash\nmacOS Catalina (10.15) or later → Zsh\n\nCheck which shell you’re currently using:\n\necho $SHELL\n\n\n\n1.2.3 Windows\nUnlike macOS or Linux, Windows doesn’t include a Unix-style terminal by default, so you will need to install one of the following:\n\nMobaXterm (recommended)\n\nProvides a terminal with Linux-like commands built-in\nInstallation guide: MobaXterm setup instructions\nEasiest option for beginners and lightweight to install\n\nWindows Subsystem for Linux (WSL2)\n\nGives you a full Linux environment directly on Windows\nRecommended if you’re comfortable installing software or already have some command-line experience\nUses Ubuntu by default, which includes Bash and all standard Linux tools\nInstallation guide: Microsoft WSL install instructions\n\n\nOnce installed, open your terminal (MobaXterm or Ubuntu via WSL2) and verify that Bash is available:\n\necho $SHELL",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#sanity-check",
    "href": "source/terminal.html#sanity-check",
    "title": "1  Setting up a terminal",
    "section": "1.3 Sanity check",
    "text": "1.3 Sanity check\nAfter setup, open your terminal and type echo $SHELL and press enter. You should see something like this:\n\nIf you see a prompt ending in $ (for example user@machine:~$), your shell is ready to go and you are all set to follow along with the tutorial!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/documentation.html",
    "href": "source/documentation.html",
    "title": "2  Documenting your code",
    "section": "",
    "text": "2.1 Choosing your editor\nDocumenting your code is crucial for both your future self and anyone else who might work with your code. Good documentation helps others (and your future self) understand the purpose, functionality, and usage of your scripts. You want to document your code in the same way that you would write a detailed lab notebook.\nFor more in-depth guidance, see A Guide to Reproducible Code in Ecology and Evolution. While the examples are mainly in R, the principles are general and apply across programming languages.\nTo see how code documentation looks like in a real-world context, have a look at an example workflow I wrote for a previous analysis. It shows how to describe each analysis step, software dependency, and command used in a way that allows someone else to reproduce the entire analysis from scratch and for me to remember what I did a month later:\nThis workflow documents a 16S rRNA gene amplicon analysis from Winogradsky columns, including:\nEven if you don’t understand all commands yet, notice how it reads almost like a lab notebook for code where each step has context, rationale, and description of the results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/documentation.html#choosing-your-editor",
    "href": "source/documentation.html#choosing-your-editor",
    "title": "2  Documenting your code",
    "section": "",
    "text": "2.1.1 Plain text editor\nAvoid visual editors like Word, as they are not designed for code and can change syntax by for example replacing backticks (`) with apostrophes (').\nOne of the easiest solutions is to use a plain text editor, such as:\n\nTextEdit (Mac)\nNotepad++ (Windows)\n\nThese editors allow you to write and save code safely, but they lack advanced features like syntax highlighting or integrated code execution.\n\n\n2.1.2 Rmarkdown in RStudio\nRMarkdown combines plain text, code, and documentation in one document. You can write your analysis and explanatory text together, then “knit” the document to HTML, PDF, or Word.\nTo create an RMarkdown file in RStudio:\n\nGo to File → New File → R Markdown\nChoose a title, author, and output format\nWrite your code and text\nClick Knit to render the document\n\nMore info: RMarkdown tutorial\n\n\n2.1.3 Quarto in Rstudio\nQuarto is a next-generation alternative to RMarkdown. It supports R, Python, and other languages, and offers more output formats and customization options. Quarto was also used to generate this tutorial.\nTo create a Quarto document:\n\nGo to File → New File → Quarto Document\nChoose a title, author, and output format\nClick Render to generate your document\n\nMore info: Quarto documentation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/documentation.html#markdown-for-documentation",
    "href": "source/documentation.html#markdown-for-documentation",
    "title": "2  Documenting your code",
    "section": "2.2 Markdown for Documentation",
    "text": "2.2 Markdown for Documentation\nMarkdown is a lightweight language for formatting text in your plain text, Rmarkdown or Quarto document. You can easily add headers, lists, links, code, images, and tables.\nHeaders:\nUse # to add a header and separate different sections of your documentation. The more # symbols you use after each other, the smaller the header will be. When writing a header make sure to always put a space between the # and the header name:\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers do not have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks followed by the computational language, i.e. bash or r, used.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nYou can easily add links to external resources as follows:\n[Link Text](https://www.example.com)\nEmphasis:\nYou can use * or _ to write italic and ** or __ for bold text.\n*italic*\n**bold**\nPictures\nYou can also add images to your documentation as follows:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/image.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html",
    "href": "source/hpc_howto.html",
    "title": "5  Working on an HPC",
    "section": "",
    "text": "5.1 Connecting to the HPC\nNow that you are more comfortable with the command line, we will start working on the UVA Crunchomics HPC. In this section, you will go through the following steps:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working on an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#connecting-to-the-hpc",
    "href": "source/hpc_howto.html#connecting-to-the-hpc",
    "title": "5  Working on an HPC",
    "section": "",
    "text": "5.1.1 ssh: Connecting to a sever\nSSH (Secure Shell) is a network protocol that allows you to securely connect to a remote computer such as the Crunchomics HPC. The general command looks like this:\n\nssh -X username@server\n\nHere:\n\nusername is your account name on the HPC, i.e. your UvanetID\nserver is the address of the HPC you want to connect to, for Crunchomics this is omics-h0.science.uva.nl\n-X enables X11 forwarding, which allows graphical applications from the server (like plotting or viewing images) to appear on your local machine. “Untrusted” X11 forwarding means the server can send graphical output, but it has limited access to your local machine\n\nFor Crunchomics at UVA, you would use:\n\nssh -X uvanetid@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nImportant tips for connecting:\n\nIf you want to log into Crunchomics while working from UvA use the eduroam network, not the open Amsterdam Science Park network\n\nIf you are connecting from outside UVA, you must be on the VPN. Contact ICT if you have not set it up or encounter issues\nAlways double-check your username and server address to avoid login errors\n\n\n\n\n\n5.1.2 Crunchomics: Preparing your account\nIf this is your first time using Crunchomics, you need to run a small Bash script to set up your account. This script will:\n\nAdd /zfs/omics/software/bin to your PATH variable. This basically allows Bash to locate and use the system-wide installed software available on Crunchomics\nSet up a Python 3 environment with some useful Python packages pre-installed\nCreate a link to your 500 GB personal directory inside your home directory, giving you plenty of space to store data and results\n\nTo set up your account, run the following commands:\n\n# First orient yourself by typing \npwd \nls -lh \n\n# Run the Crunchomics installation script\n/zfs/omics/software/script/omics_install_script\n\n# Check if something changed\n# You now should see the personal folder inside your home directory \nls -lh\n\n\n\n5.1.3 conda: Setting up your own software environment\nMany bioinformatics tools are already installed on Crunchomics, but sometimes you’ll need additional ones (like NanoPlot for analysing long-read data). To manage and install your own tools, we use conda/mamba, an environment manager that lets you create isolated environments for different software.\n\n5.1.3.1 Install conda/mamba\nMany systems already include conda or mamba. Before installing a new copy, check if it’s already available:\n\nwhich conda\nwhich mamba\n\nIf one of these commands returns “command not found”, then you can follow the next steps to install conda/mamba yourself.\nTo install Miniforge (which includes conda and mamba by default), run:\n\n# Download the miniforge installation script\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n\n# Execute the miniforge installation script\nbash Miniforge3-$(uname)-$(uname -m).sh\n\nDuring installation:\n\nRead the license (scroll by pressing enter) and say “Yes” when asking for confirmation\nChoose the installation location\n\nOn Crunchomics, your home directory has only 25 GB of space. Therefore install everything into the personal folder that comes with 500 GB: /zfs/omics/personal/uvanetID/miniforge3\n\nSay “yes” when asked if conda should be initialized\nRestart your shell by typing source ~/.bashrc. You now should see a (base) in front of your prompt indicating that you are now inside conda’s base environment\nVerify the installation:\n\n\nconda -h\nmamba -h\n\n\n\n5.1.3.2 Installing NanoPlot\nNanoPlot is a visualization tool for long-read sequencing data. We can install it, and other software, using mamba (or conda if mamba is unavailable).\n\n# Check if NanoPlot is already available\nNanoPlot -h\n\n# If not installed, create a new environment called nanoplot and install it\n# Press \"Y\" when prompted about confirming the installation\nmamba create -n nanoplot -c bioconda nanoplot\n\n# Activate the new environment\nconda activate nanoplot \n\n# Check if the tool is installed\nNanoPlot -h\n\n# Exit the environment\nconda deactivate\n\nRecommendations:\n\nWhenever possible, use mamba instead of conda, it resolves dependencies faster\nKeep your base environment clean (the base environment is the conda environment you start with when you log into the HPC). Always create new environments for each tool (conda create -n toolname …) instead of installing everything in base\nCheck the tool’s documentation for specific installation notes or version requirements\nYou can install specific versions when needed with mamba create -n nanoplot_v1.42 -c bioconda nanoplot=1.42\nYou can remove environments you no longer need with conda env remove -n nanoplot\n\n\n\n\n5.1.4 Preparing your working directory\nNext, you can start organizing your project folder. You always want to generate project folders inside your personal directory, which provides more storage than the home directory:\n\n# Go into the personal folder\ncd personal \n\n# Make and go into a new project folder\nmkdir data_analysis\ncd data_analysis \n\n# Ensure that you are in the right folder \npwd\n\nYou now have a clean workspace for all the analyses that you will run during this tutorial on Crunchomics. As in your local machine, it’s good practice to keep your raw data, results, and scripts organized in separate folders.\n\n\n5.1.5 scp: Transferring data from/to a server\nTo learn how to transfer data to Crunchomics, we want to transfer the data folder (the one you have generated in previous part of the tutorial) to the HPC. We can do this with the scp(Secure Copy Protocol) command, which securely transfers files or directories between your computer and a remote server.\nThe basic syntax is:\n\nscp [options] SOURCE DESTINATION\nFor connecting to any HPC the syntax is server:file_location\n\nWhen transferring data you must run the command from a terminal session on your own computer, not from a terminal session that runs on the HPC.\nTo copy the entire data folder into your Crunchomics project folder, use:\n\n# Run this from your local terminal (not while logged into the HPC)\n# Replace 'username' with your UvAnetID\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/data_analysis\n\n# Then check on the HPC that the files arrived:\nll data/*.fasta\n\nNotes:\n\nThe -r option copies directories recursively, i.e. you want to copy everything in the directory and the directory itself (recursively)\nIt’s helpful to keep two terminal windows open: one for your local computer and one for the HPC\nRun this command from inside your local data_analysis folder (the one you created earlier)\n\n\n\n\n\n\n\nTipTip: Moving data from the HPC to our own computer\n\n\n\n\n\nYou can also move results from the HPC to your computer. For example, to copy a single genome file from Crunchomics:\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/data_analysis/data/LjRoot303.fasta .\n\nHere, the . at the end means “copy to the current directory” on your local machine. If you want to copy to another location, replace . with a path.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working on an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#submitting-jobs-with-slurm",
    "href": "source/hpc_howto.html#submitting-jobs-with-slurm",
    "title": "5  Working on an HPC",
    "section": "5.2 Submitting jobs with SLURM",
    "text": "5.2 Submitting jobs with SLURM\nIn the HPC introduction, you learned that there are two main “sections” on an HPC:\n\nLogin node: where you log in and do light work, such as moving files, editing scripts, or preparing jobs.\nCompute nodes: where the actual analyses run. These nodes have more CPUs and memory and are managed by a scheduling system.\n\nTo use these compute nodes efficiently, we communicate with them through SLURM, a workload manager that decides when, where, and how your jobs run based on the available resources.\n\n5.2.1 squeue: View info about jobs in the queue\nThe command below shows you which jobs are currently running or waiting in the queue:\n\nsqueue\n\nThis displays all jobs submitted to the system, and might look something like this:\n\n\n\n\n\nSome useful columns to know:\n\nJOBID: a unique number assigned to each job; you can use it to check or cancel a job later.\nST: the current state of the job (e.g., R = running, PD = pending).\nA full list of SLURM job state codes can be found here\n\nIf you have already submitted a job, it will appear in this list. To see only your own jobs, you can add the -u flag followed by your username:\n\nsqueue -u $USER\n\n\n\n5.2.2 srun:Submitting a job interactively\nNow that we’ve seen how to check the job queue, let’s learn how to actually run jobs on the compute nodes. There are two main ways to submit jobs: srun and with sbatch. We’ll start with srun, which you typically use when:\n\nYou want to run tasks interactively and see the output directly on your screen\nYou are testing or debugging your commands before writing a job script\nYou have short jobs (usually less than a few hours)\n\n\nImportant: Jobs started with srun will stop if you disconnect from the HPC (unless you use tools like screen or tmux, which we won’t cover in this tutorial).\n\nLet’s start with a very simple example to see how srun works:\n\nsrun --cpus-per-task=1 --mem=1G --time=00:10:00 echo \"Hello world\"\n\nHere’s what each part means:\n\nsrun → communicate that you want to run something on the compute node\n--cpus-per-task=1 → request 1 CPU core\n--mem=1G → request 1 GB of memory\n--time=00:10:00 → set a maximum runtime of 10 minutes\necho \"Hello world\" → the actual command to run (prints text to the screen)\n\nSo the arguments after srun and before echo are where you tell SLURM what resources to allocate on the compute node.\n\n\n5.2.3 Running a analysis with Seqkit interactively\nNext, let’s use srun for something more useful. First, we will analyze the fasta files that we have explored using seqkit, a fast toolkit for inspecting FASTA/FASTQ files.\n\nTip: Whenever you use a tool for the first time, check its help page (e.g., seqkit -h) to see all available options.\n\n\n# Create a results folder\nmkdir -p results/seqkit \n\n# Run seqkit interactively\nsrun --cpus-per-task=1 --mem=5G seqkit stats data/*fasta -Tao results/seqkit/16s_stats.tsv --threads 1\n\n# View the results (press 'q' to exit less)\nless -S results/seqkit/16s_stats.tsv \n\nHere:\n\nseqkit stats data/*fasta → analyzes all FASTA files in the data folder ending in fna\n-Tao → output in tabular format (-T), include all stats (-a), and write to file (-o)\n--threads 1 → run on one thread (should match --cpus-per-task=1)\nThe results are stored in results/seqkit/genome_stats.tsv\n\nThis job runs very quickly — so fast that it might not even appear in the queue when you type squeue.\nWhen you open the results file, you’ll see:\n\nThe number of sequences (num_seqs), this should match what we saw earlier\nThe average length of the sequences in bp (avg_len)\nAdditional metrics such as the minimum, maximum or total sequence length\n\n\n\n\n\n\n\nTipTip: Choosing the right amount of resources\n\n\n\n\n\nWhen starting out, choosing the right amount of CPUs, memory, or runtime can be tricky. Here are a few rules of thumb:\n\nStart small, a lot of tools don’t need huge resources for small test runs\nCheck the tool’s documentation, many list recommended resource settings\nTest on a subset of your data first. It runs faster and helps you debug\nIf your job fails or runs out of memory, increase the resources gradually\nOnce you have a stable workflow, you can scale up\n\nRemember: Over-requesting resources can make jobs wait longer in the queue — and can block others from running.\n\n\n\n\n\n\n\n\n\nQuestionTasks\n\n\n\n\n\nNow it’s your turn to run an srun job interactively by exploring the FASTQ files that you have generated during your practical. Follow these steps:\nTask 1: Inspect the data\nWithout uncompressing the FASTQ file, explore the content of the FASTQ files by viewing the first few lines of barcodeX.fastq.gz using zcat and head.\nYou can get the FASTQ file as follows:\n\nIf you are following the Microbial Ecology course:\n\n# Replace X with the barcode you used in the practical\ncp &lt;tba&gt;/barcodeX.fastq.gz data\n\nIf you are following this tutorial independently you can get an example file as follows:\n\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/barcode07.fastq.gz -P data/\n\n\nHave a look at the data and have a look the header, sequence and quality scores.\nTasks 2: Run seqkit interactively\nUse srun to analyze barcodeX.fastq.gz with seqkit stats. Request 1 CPU, 5 GB memory, and 10 minutes of runtime. Save the output in the results/seqkit/ folder.\nTasks 3: Check the results\nOpen the output file and answer the following:\n\nWhat is the average sequence length (avg_len)? Does it match the expected amplicon size from the lab?\nHow many sequences are in the file (num_seqs)? Does this align with your expectations from the experiment?\n\n\nReminder: Run all commands on Crunchomics in your project folder. srun jobs may finish too quickly to appear in squeue.\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Question 1\nzcat data/barcodeX.fastq.gz | head\n\n# Question 2 \nsrun --cpus-per-task=1 --mem=5G seqkit stats data/barcodeX.fastq.gz  \\\n    -Tao results/seqkit/barcodeX_stats.tsv --threads 1\n\n# Question 3\nless -S results/seqkit/barcodeX_stats.tsv\n\nAnswers:\n\nYour average sequence length should be ~ 1400 bp and this should correspond to the amplicon size of your PCR reaction.\nnum_seqs should correspond to the number of sequences in your FASTQ file and will be around 2000-10000 reads. The actual number will depend on how the sequencing went.\n\n\nTip: Use \\ to split long commands across lines for readability.\n\n\n\n\n\n\n\n\n\n5.2.4 sbatch: submitting a long-running job\nWhile srun is great for quick, interactive runs, a lot of analyses on the HPC are submitted with sbatch, which lets jobs run in the background even after you log out.\nUse sbatch when:\n\nYou have long or resource-intensive analyses\nYou want jobs to run unattended\nYou plan to run multiple jobs in sequence or parallel\n\n\n5.2.4.1 Step 1: Create folders for organization\nWe’ll keep our project tidy by creating dedicated folders for scripts and logs:\n\nmkdir -p scripts logs\n\n\n\n5.2.4.2 Step 2: Write a job script\nUse nano (or another editor) to create a script file:\n\nnano scripts/run_seqkit.sh\n\nThen add the following content:\n\n#!/bin/bash\n#SBATCH --job-name=seqkit\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n#SBATCH --time=1:00:00\n\necho \"Seqkit job started on:\" \ndate\n\nsrun --cpus-per-task=1 --mem=5G seqkit stats data/*fasta -Tao results/seqkit/genome_stats_2.tsv --threads 1\n\necho \"Seqkit job finished on:\"\ndate\n\nSave and exit:\n\nPress Ctrl+X to exit, then Y to confirm that you want to safe the file, then Enter.\n\n\n\n5.2.4.3 Step 3: Submit and monitor your job\n\n# Submit \nsbatch scripts/run_seqkit.sh\n\n# Check job status (may run too fast to appear)\nsqueue -u $USER\n\nWhen submitted successfully, you’ll see something like Submitted batch job 754 and new log files will appear in your logs folder:\n\nseqkit_&lt;jobID&gt;.out → standard output (results and messages)\nseqkit_&lt;jobID&gt;.err → error log (check this if your job fails)\n\n\n\n5.2.4.4 Step 4: Understanding your job script\nIn the job script above:\n\n#!/bin/bash: Runs the script with the Bash shell\n#SBATCH --... Define SLURM options: resources, runtime, output names.\n\nThese are the same options that you have used with srun\nThe %j variable in your log filenames automatically expands to the job ID, making it easier to keep track of multiple submissions.\n\necho / date: Prints status messages to track job progress\nsrun ...: The actual analysis command to run on the compute node\n\n\n\n\n\n\n\nTipTip: Debugging your first sbatch jobs\n\n\n\n\n\nIf your job fails:\n\nCheck your .err and .out files in the logs folder, these files usually tell you exactly what went wrong\nConfirm that your input files and paths exist\nTry running the main command interactively with srun first, if it works there, it will work in a script.\n\n\n\n\n\n\n5.2.4.5 Step 5: scancel: Cancelling a job\nSometimes you realize a job is stuck, misconfigured, or using the wrong resources. You can cancel it with:\n\nscancel &lt;jobID&gt;\n\nFor example, if your job ID was 754:\n\nscancel 754\n\n\n\n\n\n\n\nTipTip: Good HPC etiquette\n\n\n\nAlways cancel jobs that are running incorrectly or stuck in a queue too long. This frees resources for others and avoids unnecessary load on the cluster.\n\n\n\n\n\n\n\n\nQuestionTasks\n\n\n\n\n\nTask 1: Write a script to run NanoPlot with sbatch\nNow you’ll write your own sbatch script to generate quality plots from barcodeX.fastq.gz using NanoPlot, which you installed earlier in your conda environment.\nBelow is a template for a script called scripts/nanoplot.sh. Replace all ... with the correct code.\nIn your script, make sure to:\n\nGive your job, output, and error files meaningful names\n\nRequest 2 CPUs, 10 GB of memory, and 30 minutes of runtime\n\nActivate your nanoplot conda environment\n\nCreate an appropriate output directory under results/\n\nUse NanoPlot options to:\n\nIndicate the input is in FASTQ format\n\nOutput a TSV stats file\n\nProduce bivariate dot plots\nSave all results in the new results folder\n\n\n\n#!/bin/bash\n#SBATCH --job-name=nanoplot\n#SBATCH --output=logs/nanoplot_%j.out\n#SBATCH --error=logs/nanoplot_%j.err\n...\n\n# Activate the conda environment\nsource ~/.bashrc # this is needed so that conda is initialized inside the compute node\nconda activate nanoplot\n\n# Add start date (optional)\n...\n\n# Make an output directory \n...\n\n# Run Nanoplot\nNanoPlot ...\n\n# Add end date (optional)\n...\n\nTask 2: Submit the job and inspect the results\nThen:\n\nSubmit your job with sbatch scripts/nanoplot.sh\nUse squeue to check whether it’s running\nInspect your log files (logs/nanoplot_.out and logs/nanoplot_.err)\nUse scp to copy the results folder to your own computer for viewing\nIn your NanoPlot output, open:\n\nNanoStats.txt (text summary)\nLengthvsQualityScatterPlot_dot.html (interactive plot)\n\nRecord:\n\nHow many reads were processed\nThe mean read length\nThe mean read quality\n\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\nThe final content of scripts/nanoplot.sh should look like this:\n\n#!/bin/bash\n#SBATCH --job-name=nanoplot\n#SBATCH --output=logs/nanoplot_%j.out\n#SBATCH --error=logs/nanoplot_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=10G\n#SBATCH --time=00:30:00\n\n# Activate the conda environment\nsource ~/.bashrc # this is needed so that conda is initialized inside the compute node\nconda activate nanoplot\n\n# Add start date (optional)\necho \"Job started: \"\ndate\n\n# Make an output directory\nmkdir -p results/nanoplot/raw\n\n# Run Nanoplot\nNanoPlot --fastq data/barcodeX.fastq.gz \\\n    -t 2 \\\n    --tsv_stats \\\n    --plots dot \\\n    -o results/nanoplot/raw\n\n# Add end date (optional)\necho \"Job ended: \"\ndate\n\nSubmit the script and check your results:\n\n# Submit the job \nsbatch scripts/nanoplot.sh\n\n# Check job status\nsqueue\n\n# Explore the log files \nls -l logs/nanoplot* \ntail logs/nanoplot_*.out\n\n# Copy results to your computer\n# Run this command inside the data_analysis folder on my own computer \nscp -r uvanetid@omics-h0.science.uva.nl:/home/uvanetid/personal/data_analysis/results/nanoplot results\n\n# Explore the stats file \nless -S results/nanoplot/NanoStats.txt\n\nWhat to look for:\n\nMean read length ≈ your expected amplicon size\nMean sequence quality ≈ 12–15 (depends on sequencing kit, the higher the better)\nPlots usually show:\n\nA peak around the expected read length\nA tail of short, incomplete reads\nFewer, unusually long reads (often chimeras)\n\n\nUse these plots to guide quality filtering to removing reads that are too short, too long, or low quality, while keeping the majority of good data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working on an HPC</span>"
    ]
  }
]