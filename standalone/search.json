[
  {
    "objectID": "source/terminal.html",
    "href": "source/terminal.html",
    "title": "1  Setting up a terminal",
    "section": "",
    "text": "1.1 Terminology\nThe command-line interface (CLI) is an alternative to a graphical user interface (GUI), with which you are likely more familiar. Both allow you to interact with your computer’s operating system but in a slightly different way:\nWhen talking about the CLI you might also hear about the shell, terminal, console, or prompt. These terms are related but not identical:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#terminology",
    "href": "source/terminal.html#terminology",
    "title": "1  Setting up a terminal",
    "section": "",
    "text": "In a GUI, you click buttons, open folders, and use menus\nIn the CLI, you type text to issue commands and see text output in the terminal\n\n\n\nThe terminal (or console) is the window or program that lets you type commands\nThe shell is the program that interprets the commands you type inside the terminal and tells the operating system what to do. There are several types of shells — for example, bash or zsh (that use slightly different languages to issue commands). This tutorial was written on a computer that uses bash, which stands for Bourne Again Shell.\n\nA prompt is the text displayed by the shell that indicates that it is ready to accept a command. The prompt often shows useful information, like your username, machine name, and current directory. This is one example for how a prompt can look like: user@machine:~$",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#installation-guides",
    "href": "source/terminal.html#installation-guides",
    "title": "1  Setting up a terminal",
    "section": "1.2 Installation guides",
    "text": "1.2 Installation guides\n\n1.2.1 Windows\nUnlike macOS or Linux, Windows doesn’t include a Unix-style terminal by default, so you will need to install one of the following:\n\nMobaXterm (recommended)\n\nProvides a terminal with Linux-like commands built-in\nInstallation guide: MobaXterm setup instructions\nEasiest option for beginners and lightweight to install\n\nWindows Subsystem for Linux (WSL2)\n\nGives you a full Linux environment directly on Windows\nRecommended if you’re comfortable installing software or already have some command-line experience\nUses Ubuntu by default, which includes Bash and all standard Linux tools\nInstallation guide: Microsoft WSL install instructions\n\n\nOnce installed, open your terminal (MobaXterm or Ubuntu via WSL2) and verify that Bash is available:\n\necho $SHELL\n\n\n\n1.2.2 Mac\nAll Mac computers also come with a built-in terminal and shell. To open the terminal:\n\nIn Finder, go to Go → Utilities, then open Terminal.\nOr use Spotlight Search (⌘ + Space), type Terminal, and press Return.\n\nThe default shell depends on your macOS version:\n\nmacOS Mojave (10.14) or earlier → Bash\nmacOS Catalina (10.15) or later → Zsh\n\nCheck which shell you’re currently using:\n\necho $SHELL\n\n\n\n1.2.3 Linux\nIf you’re using Linux, you already have everything you need and you don’t need to install anything. All Linux systems come with a terminal and a shell, and the default shell is usually Bash.\nYou can open a terminal from your applications menu or by searching for Gnome Terminal, KDE Konsole, or xterm, depending on your desktop environment.\nTo confirm which shell you’re using, type:\n\necho $SHELL",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/terminal.html#sanity-check",
    "href": "source/terminal.html#sanity-check",
    "title": "1  Setting up a terminal",
    "section": "1.3 Sanity check",
    "text": "1.3 Sanity check\nTask 1\nAfter installing a software to open a terminal, open the terminal and type echo $SHELL and press enter. You should see something like this:\n\nIf you see a prompt ending in $ (for example user@machine:~$), your shell is ready to go.\nTask 2\nConfirm that you already have access to the UvA Crunchomics High-Performance Computer by typing the following (exchange user with your UvanetID and enter your UvA password when prompted):\n\nssh -X user@omics-h0.science.uva.nl\n\nIf you see the following, you are could to go.\n\nIf you are not able to successfully login, first control that\n\nyou typed the right user name\nyou entered the correct password when prompted\nyou are logged into the eduroam Wifi and don’t use the open Wifi instead\n\n\nHint: You can log out of Crunchomics by typing exit or closing the terminal window.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up a terminal</span>"
    ]
  },
  {
    "objectID": "source/documentation.html",
    "href": "source/documentation.html",
    "title": "2  Documenting your code",
    "section": "",
    "text": "2.1 Choosing your editor\nDocumenting your code is crucial for both your future self and anyone else who might work with your code. Good documentation helps others (and your future self) understand the purpose, functionality, and usage of your scripts. You want to document your code in the same way that you would write a detailed lab notebook.\nFor more in-depth guidance, see A Guide to Reproducible Code in Ecology and Evolution. While the examples are mainly in R, the principles are general and apply across programming languages.\nTo see how code documentation looks like in a real-world context, have a look at an example workflow I wrote for a previous analysis. It shows how to describe each analysis step, software dependency, and command used in a way that allows someone else to reproduce the entire analysis from scratch and for me to remember what I did a month later:\nThis workflow documents a 16S rRNA gene amplicon analysis from Winogradsky columns, including:\nEven if you don’t understand all commands yet, notice how it reads almost like a lab notebook for code where each step has context, rationale, and description of the results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/documentation.html#choosing-your-editor",
    "href": "source/documentation.html#choosing-your-editor",
    "title": "2  Documenting your code",
    "section": "",
    "text": "2.1.1 Plain text editor\nAvoid visual editors like Word, as they are not designed for code and can change syntax by, for example, replacing backticks (`) with apostrophes (').\nOne of the easiest solutions is to use a plain text editor, such as:\n\nTextEdit (Mac)\nNotepad++ (Windows)\n\nThese editors allow you to write and save code safely, but they lack features like syntax highlighting or integrated code execution.\n\n\n2.1.2 Rmarkdown in RStudio\nRMarkdown combines plain text, code, and documentation in one document. You can write your analysis and explanatory text together, then “knit” the document to HTML, PDF, or Word.\nTo create an RMarkdown file in RStudio:\n\nGo to File → New File → R Markdown\nChoose a title, author, and output format\nWrite your code and text\nClick Knit to render the document\n\nMore info: RMarkdown tutorial\n\n\n2.1.3 Quarto in Rstudio\nQuarto is a next-generation alternative to RMarkdown. It supports R, Python, and other languages, and offers more output formats and customization options. Quarto was also used to generate this tutorial.\nTo create a Quarto document:\n\nGo to File → New File → Quarto Document\nChoose a title, author, and output format\nClick Render to generate your document\n\nMore info: Quarto documentation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/documentation.html#markdown-for-documentation",
    "href": "source/documentation.html#markdown-for-documentation",
    "title": "2  Documenting your code",
    "section": "2.2 Markdown for Documentation",
    "text": "2.2 Markdown for Documentation\nMarkdown is a lightweight language for formatting text in your plain text, Rmarkdown or Quarto document. You can easily add headers, lists, links, code, images, and tables.\nHeaders:\nUse # to add a header and separate different sections of your documentation. The more # symbols you use after each other, the smaller the header will be. When writing a header make sure to always put a space between the # and the header name:\n# Main Header\n## Subheader\nLists:\nUse - or * for unordered lists and numbers for ordered lists.\nOrdered lists are created by using numbers followed by periods. The numbers do not have to be in numerical order, but the list should start with the number one.\n1. First item\n2. Second item\n3. Third item\n4. Fourth item \n1. First item\n2. Second item\n3. Third item\n    1. Indented item\n    2. Indented item\n4. Fourth item \nUnordered lists are created using dashes (-), asterisks (*), or plus signs (+) in front of line items. Indent one or more items to create a nested list.\n- First item\n- Second item\n- Third item\n- Fourth item \n - First item\n- Second item\n- Third item\n    - Indented item\n    - Indented item\n- Fourth item \nYou can also combine ordered with unordered lists:\n1. First item\n2. Second item\n3. Third item\n    - Indented item\n    - Indented item\n4. Fourth item\nCode Blocks:\nEnclose code snippets in triple backticks followed by the computational language, i.e. bash or r, used.\n```bash\ngrep \"control\" downloads/Experiment1.txt\n```\nSome editors also use the following syntax:\n```{bash}\ngrep \"control\" downloads/Experiment1.txt\n```\nLinks:\nYou can easily add links to external resources as follows:\n[Link Text](https://www.example.com)\nEmphasis:\nYou can use * or _ to write italic and ** or __ for bold text.\n*italic*\n**bold**\nPictures\nYou can also add images to your documentation as follows:\n![Alt Text](path/to/your/image.jpg)\nHere, replace Alt Text with a descriptive alternative text for your image, and path/to/your/image.jpg with the actual path or URL of your image.\nTables\nTables can be useful for organizing information. Here’s a simple table:\n| Header 1 | Header 2 |\n| ---------| ---------|\n| Content 1| Content 2|\n| Content 3| Content 4|",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Documenting your code</span>"
    ]
  },
  {
    "objectID": "source/cli.html",
    "href": "source/cli.html",
    "title": "3  Navigating the command line",
    "section": "",
    "text": "3.1 pwd: Find out where we are\nTo get started, open your terminal, and get yourself oriented by typing your first command and then pressing enter:\npwd\nThe command pwd stands for print working directory. It tells you where you currently are in the file system, that is, which folder (directory) your shell is operating on right now. You should see something like this:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#pwd-find-out-where-we-are",
    "href": "source/cli.html#pwd-find-out-where-we-are",
    "title": "3  Navigating the command line",
    "section": "",
    "text": "/Users/YourUserName\n\n\n\n\n\n\nTipTip: Finding the Desktop on Different Systems\n\n\n\n\n\nYour home directory (i.e. the directory you start in whenever you open the terminal) varies slightly across operating systems. Here’s how to locate yourself and connect to familiar locations like the Desktop:\nmacOS\n\nYour home directory is /Users/YourUserName\nTo open the folder you are currently in Finder:open .\nYour desktop is at /Users/YourUserName/Desktop\n\nMobaXterm (Windows)\n\nYour home directory is /home/mobaxterm\nWhen using the portable version of Mobaxterm this directory is temporary and is deleted when you close MobaXterm. To make it permanent:\n\nGo to Settings –&gt; Configuration –&gt; General\nUnder Persistent home directory, choose a folder of your choice\n\nTo open the folder you are currently in the Windows File explorer: explorer.exe .\nYour Desktop is usually at: /mnt/c/Users/YourUserName/Desktop or /mnt/c/Users/YourUserName/OneDrive/Desktop (when using OneDrive)\n\nWSL2 (Windows)\n\nYour home directory is/home/YourUserName\nTo open the folder you are currently in the Windows File explorer: explorer.exe .\nYour Desktop is usually at: /mnt/c/Users/YourUserName/Desktop or /mnt/c/Users/YourUserName/OneDrive/Desktop (when using OneDrive)\n\nIf you want to access the Uva OneDrive folder and/or if your OneDrive folder name includes spaces (like OneDrive - UvA), use quotes around the path:\n\ncd \"/mnt/c/Users/YourUserName/OneDrive - UvA\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#ls-list-the-contents-of-a-directory",
    "href": "source/cli.html#ls-list-the-contents-of-a-directory",
    "title": "3  Navigating the command line",
    "section": "3.2 ls: List the contents of a directory",
    "text": "3.2 ls: List the contents of a directory\nNow that you know where you are, let’s find out what is inside that location, i.e. what files and folders can be found there. The command ls (short for list) shows the files and folders in your current directory. Type the following and press enter:\n\nls\n\nYou should see something like this (your output will vary depending on what’s in your directory):\n\nThe colors and formatting depend on your terminal settings, but typically:\n\nFolders (directories) appear in one color (often green or blue)\nFiles appear in another (often white or bold)\n\nIf your directory contains many items, the output can quickly become overwhelming. To make sense of it, we can use options and arguments to control how commands behave.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#the-structure-of-a-command",
    "href": "source/cli.html#the-structure-of-a-command",
    "title": "3  Navigating the command line",
    "section": "3.3 The structure of a command",
    "text": "3.3 The structure of a command\nA command generally has three parts:\n\nA command name: The program you want to run, i.e. ls\nAn option (or flag): A way to modify how the command behaves, i.e -l (long format)\nAn optional argument: The input, i.e. a file or folder\n\n\nTry the following command in your current directory to “List (ls) the contents of the current folder and show details in long format (-l)”:\n\nls -l\n\nAfter running this you should see a more detail list of the contents of your folder.In the example below you can now see additional information about who owns the files (i.e. access modes), how large the files are, when they were last modified and their name:\n\n\n\n\n\n\n\nTipTip: Using ls in practice\n\n\n\n\n\nThroughout this tutorial, you’ll notice that we will use ls frequently. There is a reason for that:\nWhen working with data, sanity checks are essential, because it is easy to make mistakes, overwrite files, or lose track of where things are. Using simple commands like ls, pwd, wc, or grep help you verify what is happening to your files at every step of an analysis workflow.\nThese habits are not just for beginners, bioinformaticians rely on them constantly.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#getting-help",
    "href": "source/cli.html#getting-help",
    "title": "3  Navigating the command line",
    "section": "3.4 Getting help",
    "text": "3.4 Getting help\nAt some point, you’ll want to know what options a command has or how it works. In this case, you should first check if manual pages (or man pages) are available for the command by typing man followed by the command name:\n\nman ls\n\nThis opens the manual entry for the command ls. You can scroll through it using:\n\n↑ / ↓ arrows or the space bar to move down\nb to move back up\nq to quit the manual\n\nNot all commands come with such a manual. Depending on the program, there are a few common patterns you can try to get help:\n\nman ls\nls --help\nls -h",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#mkdir-make-a-new-folder",
    "href": "source/cli.html#mkdir-make-a-new-folder",
    "title": "3  Navigating the command line",
    "section": "3.5 mkdir: Make a new folder",
    "text": "3.5 mkdir: Make a new folder\nBefore we start moving around, let’s first learn how to create new folders (also called directories). This is something we will do often, for example, to keep raw data, results, and scripts organized in separate places. The command we use for that is mkdir, which stands for make directory.\nFor now, we will use mkdir to create a project folder with the name data_analysis for this tutorial. Don’t worry about how to move into the folder yet, we’ll cover that in the next section.\n\n# Move into the home directory (the starting point of your system)\ncd ~\n\n# Create a new folder called 'data_analysis'\nmkdir data_analysis\n\n# Check that the folder was created successfully\nls\n\nYou should see a new folder called data_analysis appears in the list. We will use this folder as our project folder for all exercises in this tutorial. Next, let’s make a new data folder inside the new data_analysis folder by typing the following:\n\n# Make a data folder inside the data_analysis folder\n# The `-p` option makes the parent directory (here: data), if it does not already exist\n# It is useful to add `-p` when generating a folder inside a folder\nmkdir -p data_analysis/data\n\n# Check that the folder was created successfully\n# Notice here, how we use ls with a flag and also with an optional argument?\nls -l data_analysis\n\n\n\n\n\n\n\nTipTip: Commenting your code\n\n\n\n\n\nNotice how I added # and some notes above each command?\nAnything written after # in Bash is a comment. A comment won’t be executed, but it helps you (and others) understand what the command does.\nIn your own work, add short, meaningful comments above key steps. Avoid restating the obvious, instead, explain why you’re doing something or what it achieves.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#cd-move-around-folders",
    "href": "source/cli.html#cd-move-around-folders",
    "title": "3  Navigating the command line",
    "section": "3.6 cd: Move around folders",
    "text": "3.6 cd: Move around folders\nNow that we have our own project folder, let’s learn how to move around the file system.\nThe file system is structured like a tree that starts from a single root directory (that is also denoted as / in bash). All other folders branch out from this root directory. For example, we can go from the root directory, to the users folder and from there into the john folder.\n\nThere are two ways to specify a path to go the the portfolio folder:\n\nAbsolute path: starts from the root (e.g. cd /users/john/portfolio)\nRelative path: starts from your current location (e.g. cd portfolio if you’re already in /users/john)\n\n\nTip: It is generally recommended to use the relative path from inside your project directory. That makes your code more portable and still allows you to run the code even if your computer setup changes.\n\nLet’s practice moving between folders (at each step, use pwd in case you feel that you get lost):\n\n# Move into the data analysis folder \ncd data_analysis \n\n# Check where you are \npwd\n\nYou now should see something like /Users/Name/data_analysis. You can use the cd command in multiple ways to move around:\n\n# Move into the data folder\ncd data\n\n# Move one level up (..), i.e. go back to the data_analysis folder\ncd ..\n\n# Quickly go back home (you now should be in the home directory)\ncd ~\n\n# Move multiple levels down at once\ncd data_analysis/data\n\n# Move two levels up and go back into the home directory\ncd ../.. \n\n# And go back to the data_analysis folder \ncd data_analysis\n\nIn the code above, the tilde symbol (~) is a shortcut for your home directory. It’s equivalent to typing the full absolute path to your home (e.g. cd /Users/YourName) but it is much faster to type.\n\n\n\n\n\n\nTipTip: Command-line completion\n\n\n\n\n\nHere, are some other tips for faster navigation (and less typos):\n\nUse Tab for autocompletion: type the first few letters of a folder name and press Tab.\nIf there’s more than one match, press Tab twice to see all options.\nUse ↑ / ↓ arrows to scroll through previously entered commands\n\n\nTip: From now on try to use the Tab key once in a while so that you do not have to write everything yourself all the time.\n\n\n\n\n\n\n\n\n\n\nQuestionTask\n\n\n\n\n\nFamiliarize yourself with these first commands and:\n\nCreate a new folder inside your data_analysis directory called results\nMove into the results folder and confirm your location with pwd\nMove back inside the data_analysis folder\nUse ls to confirm both results and data are there\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\nmkdir results \ncd results \npwd \ncd ..\nls",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#wget-download-data",
    "href": "source/cli.html#wget-download-data",
    "title": "3  Navigating the command line",
    "section": "3.7 wget: Download data",
    "text": "3.7 wget: Download data\nNext, let’s download a fasta file that contains the sequence of a 16S rRNA gene. This file will be useful to learn about some other commands. Below, we use the wget command to fetch a fasta file from an online website as follows:\n\n# Download the example fasta file into the current directory\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fasta.gz\n\n\n\n\n\n\n\nImportantWget for Mac users\n\n\n\nFor Mac users wget is not installed by default. If you do not want to install it, you can instead use the curl command:\n\n# -O: save the file under its oroginal name \n# -L: Tells curl that if the server tells it to go somewhere else, follow it until you reach the actual file\ncurl -LO https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fasta.gz\n\n\n\nAfter you download any kind of data, it is always a good idea to do some sanity check to see if the file is present and know how large it is (or whether it is empty and something went wrong during the download):\n\n# List files in long (-l) and human-readable (-h) format\n# combining these two commands becomes -lh\n# You should see that the file exists and is not empty\nls -lh LjRoot303.fasta.gz",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#cp-copy-files",
    "href": "source/cli.html#cp-copy-files",
    "title": "3  Navigating the command line",
    "section": "3.8 cp: Copy files",
    "text": "3.8 cp: Copy files\ncp duplicates files or directories. It is a useful feature to keep our files organized and not have every single file in a single folder but instead to organize your files into folder categories (useful folders can be: data, scripts and results).\nLet’s use cp to copy the downloaded file into data and to organize the data a bit better:\n\n# Copy the file into data \ncp LjRoot303.fasta.gz data/\n\n# Show the content of both locations\nls -l\nls -l data\n\nWhen running the two ls commands, we see that we now have two copies of LjRoot303.fasta.gz, one file is in our working directory and the other one is in our data folder. Having large files in multiple locations is not ideal since we will use unnecessary space. However, we can use another command to move the file into our data folder instead of copying it.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#mv-move-or-rename-files",
    "href": "source/cli.html#mv-move-or-rename-files",
    "title": "3  Navigating the command line",
    "section": "3.9 mv: Move (or rename) files",
    "text": "3.9 mv: Move (or rename) files\nmv moves or renames files without creating a second copy:\n\n# Move the file into data\nmv LjRoot303.fasta.gz data/\n\n# Verify\nls -l\nls -l data\n\nNotice that mv will move a file and, without asking, overwrite the existing file we had in the data folder when we ran cp. This means that if you run mv its best to make sure that you do not overwrite files by mistake.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#rm-remove-files-and-directories",
    "href": "source/cli.html#rm-remove-files-and-directories",
    "title": "3  Navigating the command line",
    "section": "3.10 rm: Remove files and directories",
    "text": "3.10 rm: Remove files and directories\nTo remove files and folders, we use the rm command. For example we could remove the LjRoot303.fasta.gz in case we don’t need it anymore:\n\n# Remove the fasta file from the data folder\nrm data/LjRoot303.fasta.gz\n\n# Check if that worked\nls -l data\n\nIf we want to remove a folder, we need to tell rm that we want to remove folders using an option. To do this, we use -r , which allows us to remove directories and their contents recursively.\n\n\n\n\n\n\nImportant\n\n\n\nUnix does not have an undelete command.\nThis means that if you delete something with rm, it’s gone. Therefore, use rm with care and check what you write twice before pressing enter!\nAlso, NEVER run rm -r or rm -rf on the root / folder or any important path. And yes, we have these combinations online. Therefore, always double check the path or file name before pressing enter when using the rm command.\n\n\n\n\n\n\n\n\nQuestionTask\n\n\n\n\n\nDownload another fasta file:\n\nDownload the fasta file from GCF_000714595 with wget\n\nUse this path https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fasta.gz\n\nMake sure that the new file stored in the data folder\nCheck the file size (hint: use the -h option with ls)\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Download the file\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/GCF_000714595.fasta.gz\n\n# Move the file to the data folder\nmv GCF_000714595.fasta.gz data\n\n# Check the file size\nls -lh data/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#gzip-uncompressing-files",
    "href": "source/cli.html#gzip-uncompressing-files",
    "title": "3  Navigating the command line",
    "section": "3.11 gzip: (Un)compressing files",
    "text": "3.11 gzip: (Un)compressing files\nYou might have noticed that the file we downloaded ends with .gz. This extension is used for files that are compressed to make the file smaller. This is useful for saving files and saving space, but this makes the file unreadable for a human. To be able to learn how to read the content of a file, let’s learn how to uncompress the file first.\n\n# Download another file  \n# Here, we use the -P option to directly download the file into the data folder \nwget -P data https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fasta.gz\n\n# Check that this worked \n# You now should see two files\nls -l data\n\n# Uncompress the file with gzip \n# Here, we use -d to decompress the file \n# (by default gzip will compress a file)\ngzip -d data/LjRoot303.fasta.gz \n\n# Check that this worked \n# We now should see that one file is uncompressed (i.e. it lost the gz extension)\nls data\n\n\n\n\n\n\n\nImportantWget -P for Mac users\n\n\n\nFor Mac users that use curl instead of wget, you can redirect the file to a specific folder like this:\n\ncurl --output-dir data -LO https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fasta.gz",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#exploring-file-contents",
    "href": "source/cli.html#exploring-file-contents",
    "title": "3  Navigating the command line",
    "section": "3.12 Exploring file contents",
    "text": "3.12 Exploring file contents\nNow that we have the uncompressed file we can use different ways to explore the content of a file.\n\n3.12.1 cat: Print the full file\nWe can use cat print the entire file to the screen, this is useful for smaller files, but can get overwhelming for long ones where you would see hundreds of line just flying over the screen.\n\ncat data/LjRoot303.fasta\n\n\n\n3.12.2 head and tail: View parts of a file\nTo only print the first few or last few lines we can use the head and tail commands:\n\n# Show the first 10 lines\nhead data/LjRoot303.fasta\n\n# Show the last 5 lines\n# Here, the option -n allows us to control how many lines get printed\ntail -n 5 data/LjRoot303.fasta\n\n\n\n3.12.3 less: View the full file\nless let’s you view a file’s contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn’t load the entire file but accesses it page by page, resulting in fast loading speeds.\n\nless -S data/LjRoot303.fasta\n\n\nYou can use the arrow Up and Page arrow keys to move through the text file\nTo exit less, type q\n\n\n\n3.12.4 zcat: Decompress and print to screen\nWhen we used gzip above, we decompressed the file and that allowed us to read the content of the fasta file. This is perfect for smaller files, but not ideal for sequence data since these files get large and we might not want to decompress these files as they would clutter our system.\nLuckily, there is one useful tool in bash to decompress the file and print the content to the screen. zcat will print the content of a file to the screen but leave the file as is.\n\n# Check the content of the data folder \n# We should have one compressed and one uncompressed file \nls -l data\n\n# Use zcat on the compressed file to view its content\nzcat data/GCF_000714595.fasta.gz\n\n# Check the content of the data folder \n# We still should have one compressed and one uncompressed file \nls -l data\n\n\n\n\n\n\n\nImportantZcat for Mac users\n\n\n\nFor Mac users using the zsh shell, zcat might not work as expected. Try using gzcat instead:\n\ngzcat data/GCF_000714595.fasta.gz\n\n\n\n\n\n\n\n\n\nTipTip: Editing text files with nano\n\n\n\n\n\nYou can also edit the content of a text file and there are different programs available to do this on the command line, one such tool is nano, which should come with most command line interpreters. You can open any file as follows:\n\nnano data/LjRoot303.fasta\n\nOnce the document is open you can edit it however you want and then\n\nClose the document with Control + X\nType y to save changes and press enter",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#wc-count-things",
    "href": "source/cli.html#wc-count-things",
    "title": "3  Navigating the command line",
    "section": "3.13 wc: Count things",
    "text": "3.13 wc: Count things\nAnother useful tool is the wc (short for wordcount) command that allows us to count the number of lines via -l in a file. It is an useful tool for sanity checking.\n\nwc -l data/LjRoot303.fasta\n\nWe see that this file contains 20 lines of text. For fasta files this is not very informative, but if you for example work with a dataframe with rows and columns where you filter rows using certain conditions this can become very useful for sanity checking.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#grep-print-lines-that-match-patterns",
    "href": "source/cli.html#grep-print-lines-that-match-patterns",
    "title": "3  Navigating the command line",
    "section": "3.14 grep : print lines that match patterns",
    "text": "3.14 grep : print lines that match patterns\nThe grep command searches for patterns in a file. We could for example use this to ask how many 16S rRNA gene sequences are in our fasta file by searching for the fasta header that always starts with a &gt;. The basic grep syntax is grep \"pattern\" file. Here, “pattern” is the text you want to search for, and file is the file you want to search in. The double quotes allow us to include special characters (like &gt;) safely.\n\ngrep \"&gt;\" data/LjRoot303.fasta\n\nThis prints all lines that match the pattern (here: &gt;). In a fasta file, &gt; is used to label the sequence headers. Depending on your terminal, the matches might be highlighted, which makes them easy to spot.\nIf we only want to count the number of headers, we can use the -c option:\n\n# Root303 contains 1 16S rRNA gene sequence\ngrep -c \"&gt;\" data/LjRoot303.fasta\n\n\n\n\n\n\n\nTipTip: The structure of sequence files\n\n\n\n\n\nA sequence FASTA file is a text based format to store DNA or peptide sequences. It should always look something like this:\n\nThe header always starts with a &gt; followed by descriptive information. In a new line the sequence data gets stored in either a single line or multiple lines. A fasta file can contain a single sequence or multiple sequences.\nBy convention the extension .fna is used to store fasta sequences from nucleotides and .faa is used to store fasta sequences from proteins. The extension .fasta can contain either nucleotide or protein sequences.\nAnother common extension is fastq. Fastq files are used to store sequencing data generated by sequencers and contain four lines of information: (i) the sequence header, (ii) the sequence itself, (iii) a separator and (iv) information about the quality of each single base pair in the sequence. They typically look something like this:\n\n\n\n\n\n\n\n\n\n\nQuestionTask\n\n\n\n\n\n\nUncompress the file for GCF_000714595\nCompare the number of 16S rRNA gene sequences for GCF_000714595 and LjRoot303\nWhat could it mean if one microbial species has more 16S rRNA gene sequences than another?\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Uncompress \ngzip -d data/GCF_000714595.fasta.gz\n\n# Count the number of contigs \n# LjRoot303 has 1 and and GCF_000714595 9 16S rRNA gene sequences\ngrep -c \"&gt;\" data/LjRoot303.fasta\ngrep -c \"&gt;\" data/GCF_000714595.fasta\n\nAnswer for question 3:\nBacteria can carry multiple copies of the rRNA operon (16S–23S–5S). A higher copy number is often associated with fast-responding, fast-growing life strategies (more ribosomes → faster protein synthesis under nutrient-rich conditions).\nThis can cause problems in, for example, 16S rRNA gene amplicon analyses:\n\nA single organism may produce multiple slightly distinct 16S sequences, inflating apparent diversity\nSpecies with higher 16S copy number will produce more amplicon reads per cell. So raw read counts overestimate their true cell abundance unless you correct for copy number (which is difficult)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#working-with-multiple-files",
    "href": "source/cli.html#working-with-multiple-files",
    "title": "3  Navigating the command line",
    "section": "3.15 Working with multiple files",
    "text": "3.15 Working with multiple files\nSo far, we have worked with single files. In practice, sequencing data often comes as multiple files, for example, one file per strain or per sample. Let’s practice handling several files at once. Let’s begin by getting more genomes and not only download 16S sequences from strains isolated from Lotus japonicus (files with Lj prefix) but also sequences from strains coming from a human background (files with GCF prefix):\nHint: If you downloaded one of the genomes in the exercise before, then you don’t need to run this wget command.\n\n# Download some more data \nwget -P data https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/GCF_000009425.fasta.gz\nwget -P data https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/GCF_000253315.fasta.gz\nwget -P data https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot59.fasta.gz\n\n# Check what was done\nls -lh data\n\nWe now should have 5 files, 3 of which are still compressed.\n\n\n\n\n\n\nTipHint: Downloading many files with a for loop\n\n\n\n\n\nOnce you understand how to use wget, you can easily scale it up to download multiple files automatically using a for loop.\nFor example, suppose you have a list of all the accessions that were used in the lab experiments in a file called accessions.txt, one accession per line:\nGCF_000009425\nGCF_000253315\nGCF_000714595\nGCF_014058685\nLjRoot303\nLjRoot44\nLjRoot59\nLjRoot60\nRoot401\nRoot68\nRoot935\nYou can then loop through all URLs and download them into the data folder like this:\n\n# make a playground folder\nmkdir playground \n\n# Download all genomes at once\nfor accession in $(cat accessions.txt); do\n    wget -P playground https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/${accession}.fasta.gz\ndone\n\nHere’s what happens:\n\n$(cat accessions.txt) reads all lines from the file\nfor accession in ... goes through each line one by one and stores an accession name in the accession variable\nwget  -P playground github_path/${accession}.fasta.gz downloads the compressed fasta file for each accession into the playground folder. Here, the ${accession} will get replaced with one line in accessions.txt\n\nYou can use this same loop structure to run any command on multiple files automatically. This is one of the main reasons the CLI is so powerful. For example, you could in one command count the number of headers in all your fasta files:\n\n# Download all genomes at once\nfor accession in $(cat accessions.txt); do\n    echo $accession\n    zcat playground/${accession}.fasta.gz | grep -c \"&gt;\"\ndone\n\nYou can also format this more nicely if you really want to but this is outside of the scope of this tutorial. A more detailed explanation about for-loops can be found here.\n\n\n\n\n3.15.1 Wildcards (*): Match multiple files\nImagine we want to uncompress all the new files. Typing every a gzip command for every file will get tedious and you risk mistyping names. Wildcards are another tool that you can use to work with groups of files using pattern matching. One of the most useful wildcard is *, which is used to search for a particular character(s) for zero or more times. For example, *.txt would find all files with the .txt extension.\n\n# List all files inside the data folder that end with gz\n# We should see only 3 files\nls -l data/*gz\n\n# List all uncompressed Lotus fasta files inside the data folder \n# using `Lj*.fasta*` means we look for filenames that \n# start with Lj, are followed by any number of characters and end with .fasta\nls -l data/Lj*.fasta\n\nWe can use Wildcards with every bash command and, for example, use it to unzip every file at once:\n\ngzip -d data/*gz\n\n# Check if that worked\nls -lh data\n\n\n\n3.15.2 cat and &gt; : Combining and saving files\nThe cat command doesn’t just print files, it can also concatenate (join) multiple files into one. For example, we might want to combine all fasta sequences for the Lotus (Lj) strains and store the output in a new file called lotus.fasta in the results folder (a folder you should have generated in an earlier exercise).\n\n# Combine the Lotus fasta files into one \n# and store the output of the cat command in a new file \ncat data/Lj*.fasta &gt; results/lotus.fasta\n\nHere:\n\ncat data/Lj* selects all files in the data folder that start with Lj and end with .fasta.\n&gt; tells the shell to write the combined output into a new file that will be called lotus.fasta\n\n\n\n\n\n\n\nCautionImportant: Be careful with &gt;\n\n\n\n\n\nThe &gt; operator overwrites files without asking. If you want to add (append) to an existing file instead of replacing it, use &gt;&gt;:\n\n\n\nWhenever you combine or modify files it is a good idea to do some sanity checks. Luckily, we already learned about useful ways to do this:\n\n# Check how many 16S sequences are found in individual Lotus strains \ngrep -c \"&gt;\" data/Lj*\n\n# Check how many 16S sequences files are in concatenated file\n# Important: the numbers should add up with the numbers from the command above\ngrep -c \"&gt;\" results/lotus.fasta\n\n\n\n\n\n\n\nQuestionQuestion\n\n\n\n\n\n\nCombine the 16S rRNA gene sequences from human-associated strains (GCF Prefix) into a new file and store this file in the results folder\nCount the total number of 16S sequences in the individual and the combined file\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Combine\ncat data/GCF*.fasta &gt; results/human.fasta\n\n# Count \ngrep -c \"&gt;\" data/GCF*.fasta \ngrep -c \"&gt;\" results/human.fasta",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/cli.html#pipes",
    "href": "source/cli.html#pipes",
    "title": "3  Navigating the command line",
    "section": "3.16 Pipes",
    "text": "3.16 Pipes\nSo far, we’ve run one command at a time. But often, you might want to combine commands so that the output of one becomes the input of another. That’s what the pipe (|) does. It allows us to chain simple commands together to do more complex things.\nFor example, we might want to ask how many sequences all human-associated strains have together but might not want to store a concatenated file to do so. Here, we could do the following instead:\n\n# Combine two files and count the total number of 16S sequences in the combined file\ncat data/GCF*.fasta | grep -c \"&gt;\"\n\nHere:\n\ncat data/GCF*.fasta concatenates fasta files from all human-associated strains\nThe pipe (|) sends the combined contigs directly to the grep command\ngrep -c \"&gt;\" only counts how many headers are found in the combined file",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Navigating the command line</span>"
    ]
  },
  {
    "objectID": "source/hpc_intro.html",
    "href": "source/hpc_intro.html",
    "title": "4  HPC introduction",
    "section": "",
    "text": "If you work at IBED you can get access to the Crunchomics HPC, the Genomics Compute Environment for SILS and IBED. If you need access to Crunchomics, send an email to Wim de Leeuw w.c.deleeuw@uva.nl to get an account set up by giving him your UvA netID.\nUsing an HPC works a bit differently than running jobs on your computer, below you find a simplified schematic:\n\nVery briefly, you can from your laptop log into an HPC where you then have get access to the login node, sometimes also called head node. The purpose of a login node is to prepare to run a program (e.g., moving and editing files as well as compiling and preparing a job script). You then submit a job script from the head to the compute nodes via a job manager called SLURM. The compute nodes are used to actually run a program and SLURM is used to provide the users access to the resources (CPUs, memory) on the compute nodes for a certain amount of time.\n\n\n\n\n\n\nImportant\n\n\n\nCrunchomics etiquette\nYou share the HPC with other people, therefore, take care to only ask for the resources you actually use. Some general rules:\n\nThere are no hard limits on resource usage, instead we expect you to keep in mind that you are sharing the system with other users. Some rules of thumb:\n\nDo NOT run jobs that request many CPUs and lots of memory on the head-node (omics-h0), use the compute nodes (omics-cn001 - omics-cn005) for this\nDo NOT allocate more than 20% (CPU or memory) of the cluster for more than a day.\nDo not leave allocations unused and set reasonable time limits on you jobs\n\nFor large compute jobs a job queuing system (SLURM) is available. Interactive usage is possible but is discouraged for larger jobs. We will learn how to use this system during the tutorial\nClose applications when not in use, i.e. when running R interactively\n\n\n\nOn Crunchomics you:\n\nAre granted a storage of 500 GB. After the duration of your grant, or when your UvAnetID expires, your data will be removed from the HPC. If you need more storage space, contact the Crunchomics team.\nIn your home directory, /home/$USER , you have 25 G of storage\nIn your personal directory, /zfs/omics/personal/$USER , you can store up to 500 GB data\nFor larger, collaborative projects you can contact the Crunchomics team and ask for a shared folder to which several team members can have access\nYou are in charge of backing up your own data and Crunchomics is NOT an archiving system. To learn about data archiving options at UvA visit the website of the computational support team\nA manual with more information and documentation about the cluster can be found here\n\nCrunchomics gives you access to:\n\n5 compute nodes\nEach compute node has 512 GB of memory and 64 CPUs\nAccess to two directories:\n\nThe home directory with 25 GB of storage\nyour personal directory, with 500 GB of storage\n\n\nSome information about snapshots:\n\nSnapshots are made daily at 00.00.00 and kept for 2 weeks. This means that if you accidentally remove a file it can be restored up to 2 weeks after removal. This also means that even if you remove files to make space, these files will still count towards your quota for two weeks\nData on Crunchomics is stored on multiple disks. Therefore, there is protection against disk failure. However, the data is not replicated and you are responsible for backing up and/or archiving your data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HPC introduction</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html",
    "href": "source/hpc_howto.html",
    "title": "5  Working on an HPC",
    "section": "",
    "text": "5.1 Connecting to the HPC\nNow that you are more comfortable with the command line, we will learn how to work on the UVA Crunchomics HPC. In this section, you will go through the following steps:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working on an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#connecting-to-the-hpc",
    "href": "source/hpc_howto.html#connecting-to-the-hpc",
    "title": "5  Working on an HPC",
    "section": "",
    "text": "5.1.1 ssh: Connecting to a sever\nSSH (Secure Shell) is a network protocol that allows you to securely connect to a remote computer such as the Crunchomics HPC. The general command looks like this:\n\nssh -X username@server\n\nHere:\n\nusername is your account name on the HPC, i.e. your UvanetID\nserver is the address of the HPC you want to connect to, for Crunchomics this is omics-h0.science.uva.nl\n-X enables X11 forwarding, which allows graphical applications from the server (like plotting or viewing images) to appear on your local machine. “Untrusted” X11 forwarding means the server can send graphical output, but it has limited access to your local machine\n\nTo log into the Crunchomics at UVA, use the following and enter your UvA password when prompted:\n\nssh -X uvanetid@omics-h0.science.uva.nl\n\n\n\n\n\n\n\nImportant\n\n\n\nImportant tips for connecting:\n\nIf you want to log into Crunchomics while working from UvA use the eduroam network, not the open Amsterdam Science Park network\n\nIf you are connecting from outside UVA, you must be on the VPN. Contact ICT if you have not set it up or encounter issues\nAlways double-check your username and server address to avoid login errors\n\n\n\n\n\n5.1.2 Crunchomics: Preparing your account\nIf this is your first time using Crunchomics, you need to run a small Bash script to set up your account. This script will:\n\nAdd /zfs/omics/software/bin to your PATH variable. This basically allows Bash to locate and use the system-wide installed software available on Crunchomics\nSet up a Python 3 environment with some useful Python packages pre-installed\nCreate a link to your 500 GB personal directory inside your home directory, giving you plenty of space to store data and results\n\nTo set up your account, run the following commands:\n\n# First orient yourself by typing \npwd \nls -lh \n\n# Run the Crunchomics installation script\n/zfs/omics/software/script/omics_install_script\n\n# Check if something changed\n# You now should see the personal folder inside your home directory \nls -lh\n\n\n\n5.1.3 conda: Setting up your own software environment\nMany bioinformatics tools are already installed on Crunchomics, but sometimes you’ll need additional ones (like NanoPlot for analysing long-read data). To manage and install your own tools, we use conda/mamba, an environment manager that lets you create isolated environments for different software.\n\n5.1.3.1 Install conda/mamba\nMany systems already include conda or mamba. Before installing a new copy, check if it’s already available:\n\nwhich conda\nwhich mamba\n\nIf one of these commands returns “command not found”, then you can follow the next steps to install conda/mamba yourself. If you already have conda but not mamba that is fine, just replace all instances of mamba in this tutorial with conda.\nIf you don’t have access to either conda or mamba then you can install Miniforge (which includes conda and mamba by default) by running the following two commands:\n\n# Download the miniforge installation script\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n\n# Execute the miniforge installation script\nbash Miniforge3-$(uname)-$(uname -m).sh\n\nDuring installation:\n\nRead the license (scroll by pressing enter) and say “Yes” when asking for confirmation\nChoose the installation location\n\nImportant: On Crunchomics, your home directory has only 25 GB of space. Therefore install everything into the personal folder that comes with 500 GB: /zfs/omics/personal/uvanetID/miniforge3\n\nSay “yes” when asked if conda should be initialized\nRestart your shell by typing source ~/.bashrc. You now should see a (base) in front of your prompt indicating that you are now inside conda’s base environment\nVerify the installation:\n\n\nconda -h\nmamba -h\n\n\n\n5.1.3.2 Installing NanoPlot\nNanoPlot is a visualization tool for long-read sequencing data. We can install it, and other software, using mamba (or conda if mamba is unavailable).\n\n# Check if NanoPlot is already available\nNanoPlot -h\n\n# If not installed, create a new environment called nanoplot and install it\n# Press \"Y\" when prompted about confirming the installation\nmamba create -n nanoplot -c bioconda nanoplot\n\n# Activate the new environment\nconda activate nanoplot \n\n# Check if the tool is installed\nNanoPlot -h\n\n# Exit the environment\nconda deactivate\n\nRecommendations:\n\nWhenever possible, use mamba instead of conda, it resolves dependencies faster\nKeep your base environment clean (the base environment is the conda environment you start with when you log into the HPC). Always create new environments for each tool (conda create -n toolname …) instead of installing everything in base\nCheck the tool’s documentation for specific installation notes or version requirements\nYou can install specific versions when needed with mamba create -n nanoplot_v1.42 -c bioconda nanoplot=1.42\nYou can remove environments you no longer need with conda env remove -n nanoplot\nYou can list all existing environments with conda env list\n\n\n\n\n5.1.4 Preparing your working directory\nNext, you can start organizing your project folder. You always want to generate project folders inside your personal directory, which provides more storage than the home directory:\n\n# Go into the personal folder\ncd personal \n\n# Make and go into a new project folder\nmkdir data_analysis\ncd data_analysis \n\n# Ensure that you are in the right folder \npwd\n\nYou now have a clean workspace for all the analyses that you will run during this tutorial on Crunchomics. As in your local machine, it’s good practice to keep your raw data, results, and scripts organized in separate folders.\n\n\n5.1.5 scp: Transferring data from/to a server\nTo learn how to transfer data to Crunchomics, we want to transfer the data folder (the one you have generated in previous part of the tutorial) to the HPC. We can do this with the scp(Secure Copy Protocol) command, which securely transfers files or directories between your computer and a remote server.\nThe basic syntax is:\n\nscp [options] SOURCE DESTINATION\nFor connecting to any HPC the syntax is server:file_location\n\nWhen transferring data you must run the command from a terminal session on your own computer, not from a terminal session that runs on the HPC. To keep yourself organized its often useful to have one terminal session open on your laptop and another one Crunchomics as shown in the example below:\n\nTo copy the entire data folder into your Crunchomics project folder, use:\n\n# Run this from your local terminal (not while logged into the HPC) \n# Run this while being inside the data_analysis folder to be able to access the data folder\n# Replace 'username' with your UvAnetID\nscp -r data username@omics-h0.science.uva.nl:/home/username/personal/data_analysis\n\n# Run this command while being logged into the HPC\n# and while being inside the data_analysis folder\n# to check on the HPC that the files arrived:\nll data\n\nNotes:\n\nThe -r option copies directories recursively, i.e. you want to copy everything in the directory and the directory itself (recursively)\nIt’s helpful to keep two terminal windows open: one for your local computer and one for the HPC\nRun this command from inside your local data_analysis folder (the one you created earlier)\n\n\n\n\n\n\n\nTipTip: Moving data from the HPC to our own computer\n\n\n\n\n\nYou can also move results from the HPC to your computer. For example, to copy a single genome file from Crunchomics:\n\nscp username@omics-h0.science.uva.nl:/home/username/personal/data_analysis/data/LjRoot303.fasta .\n\nHere, the . at the end means “copy to the current directory” on your local machine. If you want to copy to another location, replace . with a path.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working on an HPC</span>"
    ]
  },
  {
    "objectID": "source/hpc_howto.html#submitting-jobs-with-slurm",
    "href": "source/hpc_howto.html#submitting-jobs-with-slurm",
    "title": "5  Working on an HPC",
    "section": "5.2 Submitting jobs with SLURM",
    "text": "5.2 Submitting jobs with SLURM\nIn the HPC introduction, you learned that there are two main “sections” on an HPC:\n\nLogin node: where you log in and do light work, such as moving files, editing scripts, or preparing jobs.\nCompute nodes: where the actual analyses run. These nodes have more CPUs and memory and are managed by a scheduling system.\n\nTo use these compute nodes efficiently, we communicate with them through SLURM, a workload manager that decides when, where, and how your jobs run based on the available resources.\n\n5.2.1 squeue: View info about jobs in the queue\nThe command below shows you which jobs are currently running or waiting in the queue:\n\nsqueue\n\nThis displays all jobs submitted to the system, and might look something like this:\n\n\n\n\n\nSome useful columns to know:\n\nJOBID: a unique number assigned to each job; you can use it to check or cancel a job later.\nST: the current state of the job (e.g., R = running, PD = pending).\nA full list of SLURM job state codes can be found here\n\nIf you have already submitted a job, it will appear in this list. To see only your own jobs, you can add the -u flag followed by your username or the USER variable:\n\nsqueue -u $USER\n\n\n\n5.2.2 srun:Submitting a job interactively\nNow that we’ve seen how to check the job queue, let’s learn how to actually run jobs on the compute nodes. There are two main ways to submit jobs: srun and with sbatch. We’ll start with srun, which you typically use when:\n\nYou want to run tasks interactively and see the output directly on your screen\nYou are testing or debugging your commands before writing a job script\nYou have short jobs (usually less than a few hours)\n\n\nImportant: Jobs started with srun will stop if you disconnect from the HPC (unless you use tools like screen or tmux, which we won’t cover in this tutorial).\n\nLet’s start with a very simple example to see how srun works:\n\nsrun --cpus-per-task=1 --mem=1G --time=00:10:00 echo \"Hello world\"\n\nHere’s what each part means:\n\nsrun → communicate to SLURM that you want to run something on the compute node with the following resources:\n--cpus-per-task=1 → request 1 CPU core\n--mem=1G → request 1 GB of memory\n--time=00:10:00 → set a maximum runtime of 10 minutes\necho \"Hello world\" → the actual command to run (echo is a command that prints text to the screen)\n\nSo the arguments after srun and before echo are where you tell SLURM what resources to allocate on the compute node. Note, that after running this command that you might see Hello world twice. This is fine and just some specific behavior of some HPC systems we don’t need to worry about.\n\n\n5.2.3 Running a analysis with Seqkit interactively\nNext, let’s use srun for something more useful. We will analyze the fasta files that we have explored using seqkit, a fast toolkit for inspecting FASTA/FASTQ files. Seqkit comes with different modules, for example the stats module, that generates basic summary statistics from sequence files and can be called with seqkit stats.\n\nTip: Whenever you use a tool for the first time, check its help page (e.g., seqkit -h) to see all available options or check online for a manual or basic usage information.\n\n\n# Create a results folder with a seqkit folder sinide\nmkdir -p results/seqkit \n\n# Run seqkit with srun\nsrun --cpus-per-task=1 --mem=5G seqkit stats data/*fasta -Tao results/seqkit/16s_stats.tsv --threads 1\n\n# View the results (press 'q' to exit less)\nless -S results/seqkit/16s_stats.tsv \n\nHere:\n\nseqkit stats data/*fasta → Use the seqkit stats module to analyse all FASTA files in the data folder ending in fasta\n-Tao → options to store output in tabular format (-T), include all stats (-a), and write to file (-o)\n--threads 1 → run on one thread (should match --cpus-per-task=1)\n\nImportant: Whenever you submit jobs to the compute node, make sure that the number of requested CPUS/threads match with the number of CPUS used by the software\n\nThe results are stored in results/seqkit/16s_stats.tsv\n\nThis job runs very quickly, so fast that it might not even appear in the queue when you type squeue.\nWhen you open the results file, you will, among others, see:\n\nThe number of sequences in each fasta file (num_seqs), this should match with what you saw during the CLI tutorial part\nThe average length of the sequences in bp (avg_len)\nAdditional metrics such as the minimum, maximum or total sequence length\n\n\n\n\n\n\n\nTipTip: Choosing the right amount of resources\n\n\n\n\n\nWhen starting out, choosing the right amount of CPUs, memory, or runtime can be tricky. Here are a few rules of thumb:\n\nStart small, a lot of tools don’t need huge resources for small test runs\nCheck the tool’s documentation, some might list recommended resource settings\nWhen starting out, you can begin by testing on a subset of your data first. It runs faster and helps you debug\nIf your job fails or runs out of memory, increase the resources gradually\nOnce you have a stable workflow, you can scale up\n\nRemember: Over-requesting resources can make jobs wait longer in the queue — and can block others from running.\n\n\n\n\n\n\n\n\n\nQuestionTasks\n\n\n\n\n\nNow it’s your turn to run an srun job interactively by exploring the FASTQ files that you have generated during your practical. Follow these steps:\nTask 1: Inspect the data\nWithout uncompressing the FASTQ file, explore the content of the FASTQ files by viewing the first few lines of barcodeX.fastq.gz using zcat and head. Replace barcodeX with the barcodeID that you prepared during the practical.\nYou can get the FASTQ file as follows:\n\nIf you are following the Microbial Ecology course:\nIf you have prepared more than one barcode during the practical then check the individual file sizes of your barcodes and find the barcode with the largest file size. You will analyse this sample throughout this tutorial to keep things simple. Throughout this tutorial replace barcodeX with that barcode ID.\nIf want to know how to analyse several samples in an efficient manner, you can have a look at an example workflow after you are done with the full tutorial.\n\n# Check the file size of all barcodes \nls -lh /zfs/omics/projects/education/miceco/data/practical/*.fastq.gz\n\n# Copy a fastq file to your data folder\n# Replace X with the barcode ID that you used in the practical\ncp /zfs/omics/projects/education/miceco/data/practical/barcodeX.fastq.gz data\n\nIf you are following this tutorial independently you can get an example file as follows:\n\nwget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/barcode07.fastq.gz -P data/\n\n\nHave a look at the data and have a look the header, sequence and quality scores.\nTasks 2: Run seqkit interactively\nUse srun to analyze barcodeX.fastq.gz with seqkit stats. Request 1 CPU, 5 GB memory, and 10 minutes of runtime. Save the output in the results/seqkit/ folder and name the file barcodeX_stats.tsv.\nTasks 3: Check the results\nOpen the output file and answer the following:\n\nWhat is the average sequence length (avg_len)? Does it match the expected amplicon size from the lab?\nHow many sequences are in the file (num_seqs)? Does this align with your expectations from the experiment?\n\n\nReminder: Run all commands on Crunchomics in your project folder. srun jobs may finish too quickly to appear in squeue.\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Question 1\nzcat data/barcodeX.fastq.gz | head\n\n# Question 2 \nsrun --cpus-per-task=1 --mem=5G seqkit stats data/barcodeX.fastq.gz  \\\n    -Tao results/seqkit/barcodeX_stats.tsv --threads 1\n\n# Question 3\nless -S results/seqkit/barcodeX_stats.tsv\n\nAnswers:\n\nYour average sequence length should be ~ 1400 bp and this should correspond to the amplicon size of your PCR reaction.\nnum_seqs should correspond to the number of sequences in your FASTQ file and will be around 2000-10000 reads. The actual number will depend on how the sequencing went.\n\n\nTip: Use \\ to split long commands across lines for readability. If you do this, be careful and do NOT add a space after the \\\n\n\n\n\n\n\n\n\n\n5.2.4 sbatch: submitting a long-running job\nWhile srun is great for quick, interactive runs, a lot of analyses on the HPC are submitted with sbatch, which lets jobs run in the background even after you log out.\nUse sbatch when:\n\nYou have long or resource-intensive analyses\nYou want jobs to run unattended\nYou plan to run multiple jobs in sequence or parallel\n\n\n5.2.4.1 Step 1: Create folders for organization\nWe’ll keep our project tidy by creating dedicated folders for scripts and logs:\n\nmkdir -p scripts logs\n\n\n\n5.2.4.2 Step 2: Write a job script\nWe will first run seqkit stats again but this time instead of srun we will use sbatch, so that you directly can compare the two.\nUse nano to create a script file:\n\nnano scripts/run_seqkit.sh\n\nThen add the following content:\n\n#!/bin/bash\n#SBATCH --job-name=seqkit\n#SBATCH --output=logs/seqkit_%j.out\n#SBATCH --error=logs/seqkit_%j.err\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=5G\n#SBATCH --time=1:00:00\n\necho \"Seqkit job started on:\" \ndate\n\nseqkit stats data/*fasta -Tao results/seqkit/16s_stats_2.tsv --threads 1\n\necho \"Seqkit job finished on:\"\ndate\n\nSave and exit:\n\nPress Ctrl+X to exit, then Y to confirm that you want to safe the file, then Enter.\n\nNotice how the seqkit stats command looks exactly the same as when we used srun? The main difference between srun and sbatch is how we have to package the instructions for the compute node.\n\n\n5.2.4.3 Step 3: Submit and monitor your job\n\n# Submit by using sbatch and providing it with the script we just wrote\nsbatch scripts/run_seqkit.sh\n\n# Check job status (may run too fast to appear)\nsqueue -u $USER\n\n# Once the job is finished and does not appear in the list, check the outputs \nls -l logs \nls -l results/seqkit\nless -S results/seqkit/16s_stats_2.tsv\n\nWhen submitted successfully, you’ll see something like Submitted batch job 754 and new log files will appear in your logs folder:\n\nseqkit_&lt;jobID&gt;.out → standard output (results and messages)\nseqkit_&lt;jobID&gt;.err → error log (check this if your job fails)\n\nThe content of results/seqkit/16s_stats_2.tsv should look exactly the same as when you ran it with srun.\n\n\n5.2.4.4 Step 4: Understanding your job script\nIn the job script above:\n\n#!/bin/bash: Runs the script with the Bash shell, this is something you need to add on top of every job script\n#SBATCH --... Allows us to define the SLURM options: resources, runtime, output names.\n\nThese are the same options that you have used with srun\nThe %j variable in your log filenames automatically expands to the job ID, making it easier to keep track of multiple submissions.\n\necho / date: Prints status messages to track job progress\nseqkit stats ...: The actual analysis command to run on the compute node\n\n\n\n\n\n\n\nTipTip: Debugging your first sbatch jobs\n\n\n\n\n\nIf your job fails:\n\nCheck your .err and .out files in the logs folder, these files usually tell you exactly what went wrong\nConfirm that your input files and paths exist\nTry running the main command interactively with srun first, if it works there, it will work in a script.\n\n\n\n\n\n\n5.2.4.5 Step 5: scancel: Cancelling a job\nSometimes you realize a job is stuck, misconfigured, or using the wrong resources. You can cancel it with:\n\nscancel &lt;jobID&gt;\n\nFor example, if your job ID was 754:\n\nscancel 754\n\n\n\n\n\n\n\nTipTip: Good HPC etiquette\n\n\n\nAlways cancel jobs that are running incorrectly or stuck in a queue too long. This frees resources for others and avoids unnecessary load on the cluster.\n\n\n\n\n\n\n\n\nQuestionTasks\n\n\n\n\n\nTask 1: Write a script to run NanoPlot with sbatch\nNow you’ll write your own sbatch script to generate quality plots from barcodeX.fastq.gz using NanoPlot, which you installed earlier in your conda environment. For this, first have a look at the tools manual and read what the tools does and what the different options are.\nBelow is a template for a script that you should store in the file scripts/nanoplot.sh using nano. You should replace all ... with the correct code.\nThe script already ensures that the job:\n\nGives your job, output, and error files meaningful names\n\nRequests 2 CPUs, 10 GB of memory, and 30 minutes of runtime\n\nActivates your nanoplot conda environment\n\nCreates an appropriate output directory under results/\n\nYour task is to use Nanoplot and use the right options (check the tools manual for those) and:\n\nIndicate the input is in FASTQ format\nUse 2 threads\nOutput the stats file as a properly formatted TSV\nSpecify that we want bivariate dot plots\nSave all results in the new results folder\n\n\n#!/bin/bash\n#SBATCH --job-name=nanoplot\n#SBATCH --output=logs/nanoplot_%j.out\n#SBATCH --error=logs/nanoplot_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=10G\n#SBATCH --time=00:30:00\n\n# Activate the conda environment\nsource ~/.bashrc # this is needed so that conda is initialized inside the compute node\nconda activate nanoplot # whenever activating an environment, make sure that the environment name is correct\n\n# Add start date (optional)\ndate\n\n# Make an output directory\nmkdir -p results/nanoplot/raw\n\n# Run Nanoplot \n# and replace the ... with the right options for sub-tasks 1-5\nNanoPlot \\\n    --fastq ... \\\n    -t ... \\\n    --... \\\n    --plots ... \\\n    -o results/nanoplot/raw\n\n# Add end date (optional)\ndate\n\nTask 2: Submit the job and inspect the results\nThen:\n\nSubmit your job with sbatch scripts/nanoplot.sh\nUse squeue to check whether it’s running\nInspect your log files (logs/nanoplot_.out and logs/nanoplot_.err) and check how long the job ran\nUse scp to copy the results folder to your own computer for viewing\nIn your NanoPlot output, open:\n\nNanoStats.txt (text summary)\nLengthvsQualityScatterPlot_dot.html (interactive plot that you can open on your computer in your internet browser)\n\nRecord:\n\nHow many reads were processed\nThe mean read length\nThe mean read quality\n\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\nThe final content of scripts/nanoplot.sh should look like this:\n\n#!/bin/bash\n#SBATCH --job-name=nanoplot\n#SBATCH --output=logs/nanoplot_%j.out\n#SBATCH --error=logs/nanoplot_%j.err\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=10G\n#SBATCH --time=00:30:00\n\n# Activate the conda environment\nsource ~/.bashrc # this is needed so that conda is initialized inside the compute node\nconda activate nanoplot\n\n# Add start date (optional)\necho \"Job started: \"\ndate\n\n# Make an output directory\nmkdir -p results/nanoplot/raw\n\n# Run Nanoplot\nNanoPlot \\\n    --fastq data/barcode07.fastq.gz \\\n    -t 2 \\\n    --tsv_stats \\\n    --plots dot \\\n    -o results/nanoplot/raw\n\n# Add end date (optional)\necho \"Job ended: \"\ndate\n\nSubmit the script and check your results:\n\n# Submit the job \nsbatch scripts/nanoplot.sh\n\n# Check job status\nsqueue\n\n# Explore the log files once the job is done\n# The job typically should run for &lt; 1 minute\nls -l logs/nanoplot* \ntail logs/nanoplot_*.out\n\n# Copy results to your computer\n# Run this command inside the data_analysis folder on your own computer \n# (you should already have a results folder inside that folder)\nscp -r uvanetid@omics-h0.science.uva.nl:/home/uvanetid/personal/data_analysis/results/nanoplot results\n\n# If you are using a Windows computer, then you can use explorer.exe to open the folder you are in\n# Run this command inside the data_analysis folder on your own computer \n# Mac users instead can use: `open .`\nexplorer.exe .\n\n# Explore the stats file \nless -S results/nanoplot/NanoStats.txt\n\n# On your own computer, find the nanoplot folder and open this file in your browser:\n# results/nanoplot/raw/LengthvsQualityScatterPlot_dot.html\n\nWhat to look for:\n\nMean read length ≈ your expected amplicon size\nMean sequence quality ≈ 12–15 (depends on sequencing kit, the higher the better)\nPlots usually show:\n\nA peak around the expected read length\nA tail of short, incomplete reads\nFewer, unusually long reads (often chimeras)\n\n\nUse these plots to guide quality filtering to removing reads that are too short, too long, or low quality, while keeping the majority of good data.\n\n\n\n\n\n\n\n\n\n\n\n\nTipViewing files without scp\n\n\n\n\n\nCrunchomics has an Application Server that can be used to run the RStudio app on the server. You can use this to run R code but also to view files. To do this:\n\nGo to https://omics-app01.science.uva.nl/\nEnter your UvAnetID and password\nThe File Viewer on the right side shows the files and folders that you have access to on Crunchomics\nYou can view your NanoPlot HTML by going to personal/data_analysis/results/nanoplot/raw, clicking on the HTML you want to view and selecting View in Web Browser",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Working on an HPC</span>"
    ]
  },
  {
    "objectID": "source/applied_workflow.html",
    "href": "source/applied_workflow.html",
    "title": "6  Amplicon long-read analysis",
    "section": "",
    "text": "6.1 chopper: Quality cleaning\nNow that we have learned how to use the command line to issue commands and use an HPC we can now analyse our long-read amplicon data and:\nImportant: All of this should be run on the Crunchomics HPC, so be sure that you connected with ssh -X user@omics-h0.science.uva.nl.\nIn this section, you will learn how to perform quality filtering of your amplicon long-read data using chopper. Quality control is an essential first step to remove low quality reads that introduce errors in downstream analyses such as sequence alignment. Typical things you want to do are:\nchopper is designed for filtering and trimming of long-read data and you can install chopper with mamba:\nmamba create -n chopper_0.11.0 -c bioconda chopper=0.11.0\nIn this tutorial, you will always receive example code as a starting point, with sections containing ... for you to fill in. If you get stuck in this section, first check chopper’s documentation or use the built-in help via the CLI (chopper -h).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Amplicon long-read analysis</span>"
    ]
  },
  {
    "objectID": "source/applied_workflow.html#chopper-quality-cleaning",
    "href": "source/applied_workflow.html#chopper-quality-cleaning",
    "title": "6  Amplicon long-read analysis",
    "section": "",
    "text": "Remove sequence adapters, barcodes, primer sequences (this already has been done for your data)\nRemove reads with a too low quality score\nRemove reads that are too long or too short\nTrim read ends if they have low quality scores\n\n\n\n\n\n\n\n\n\n\nQuestionTasks\n\n\n\n\n\nUse your NanoPlot quality plots from the previous section and choose some quality and length cutoffs.\nTask 1: Perform quality filtering using chopper\nDo the following:\n\nCreate a new folder to store the results and name the output file barcodeX_filtered.fastq.gz\nRequest 2 CPUs and 20G of memory with srun, and ensure chopper itself also uses 2 threads\nInvestigate the Nanoplot LengthvsQualityScatterPlot.html that you generated in the last exercise and decide on the right thresholds to use with chopper to:\n\nDiscard sequences that are too long or too short\nDiscard reads with a too low phred score\n\nCheck the tools manual carefully for how to deal with .gz files. Hint: Closely check the example section\n\n\nIMPORTANT: After chopper finished you will see something like “Kept 1985conda de reads out of 3785 reads”. If you see less than 50% of the reads remaining use less strict length and quality filters. You need enough reads for the next steps\n\nTask 2: Assess the quality of the filtering step\nInvestigate the quality of the filtered FASTQ file by running:\n\nNanoPlot\nseqkit\n\nImportant: Be sure that you name the output file something like barcodeX_filtered.tsv, we need this file later!\n\n\nAfterwards open NanoPlots and seqkit. Investigate the outputs and record:\n\nThe total number of sequences that went into the analysis\nThe total number of sequences that passed the quality filtering\n\n\n# Generate an output folder for the results\n... \n\n# Activate the chopper conda environment\n...\n\n# Run chopper\nsrun --cpus-per-task 2 --mem=20G chopper ...\n\n# Deactivate the conda environment \n...\n\n# Make a folder for the Nanoplot results and run Nanoplot \n# (don't forget to activate the right conda environment first and deactivate it after)\nsrun ...\n\n# Run seqkit\n# Make sure that the output file is something like path/barcodeX_filtered.tsv\nsrun ...\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Generate an output folder for the results\nmkdir results/chopper\n\n# Activate the chopper conda environment\nconda activate chopper_0.11.0\n\n# Run chopper\nsrun --cpus-per-task 2 --mem=20G chopper \\\n    -i data/barcodeX.fastq.gz \\\n    -q 12 \\\n    --minlength 1000 --maxlength 1650 \\\n    --threads 2 | gzip \\\n    &gt; results/chopper/barcodeX_filtered.fastq.gz\n\n# Deactivate the conda environment \nconda deactivate\n\n# Run Nanoplot\nconda activate nanoplot\n\nmkdir -p results/nanoplot/cleaned\n\nsrun --cpus-per-task 2 --mem=10G NanoPlot \\\n    --fastq results/chopper/barcodeX_filtered.fastq.gz \\\n    -t 2 \\\n    --tsv_stats \\\n    --plots dot \\\n    -o results/nanoplot/cleaned\n\nconda deactivate\n\n# Run seqkit \nsrun --cpus-per-task=1 --mem=5G seqkit stats results/chopper/barcodeX_filtered.fastq.gz  \\\n    -Tao results/seqkit/barcodeX_filtered.tsv --threads 1\n\nBelow are example results from one of my analyses. These numbers will be different for you, however, this process will make you more familiar for what to look for in your own analyses.\n\nInput: 3785 reads\nPassed filtering: 1985 reads (~50%). You ideally want to keep 50%-90% of the reads for the down-stream analyses depending on the quality of the data and how many reads you generated.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Amplicon long-read analysis</span>"
    ]
  },
  {
    "objectID": "source/applied_workflow.html#minimap2-map-reads-to-reference-database",
    "href": "source/applied_workflow.html#minimap2-map-reads-to-reference-database",
    "title": "6  Amplicon long-read analysis",
    "section": "6.2 Minimap2: Map reads to reference database",
    "text": "6.2 Minimap2: Map reads to reference database\nNanopore long-read sequencing can produce reads that span nearly the entire 16S rRNA gene, making it useful for identifying microbial species. However, these long reads come with a trade-off: they have a higher error rate compared to short-read technologies.\nAssume that your average read quality is Q10. The Phred score (Q) is a measure of how confident the sequencer is in each base call. The probability of a base being called incorrectly can be calculated as:\n\\[P=10^{-Q/10}\\]\nIn R we can compute this with:\n\nQ &lt;- 10\nP_error &lt;- 10^(-Q/10)\npercent_error &lt;- P_error * 100\nmessage(\"Error rate: \", round(percent_error, 2), \"%\")\n\nError rate: 10%\n\n\nThis shows that with a mean Phred quality of 10, each base has roughly a 10% chance of being incorrect. For a 1,500 bp 16S amplicon we expect 150 errors per sequence with such a quality score. While Nanopore long reads are useful because they capture the full 16S rRNA gene, a high level of errors can make species-level identification unreliable.\nTo classify our filtered Nanopore reads, we use Minimap2 to map each read against a reference database of 16S rRNA sequences from all the strains used in your lab experiments. Mapping identifies the reference sequence most similar to each read. The read is then aligned base-by-base to the corresponding reference sequence to check for exact matches or mismatches. Depending on the similarity, one or more hits may be recorded for each read.\nMapping tools like Minimap2 are well-suited for Nanopore data because they tolerate mismatches as well as small insertions and deletions, which are common in error-prone long reads. The output of Minimap2 is a PAF (Pairwise mApping Format) file, which contains information about how well each read maps to the reference sequences.\nSome reads may map equally well to multiple reference sequences (multi-mappers), particularly if the strains have highly similar 16S sequences (for example the different Pseudomonas strains you worked with in the lab). These multi-mapping reads are important to keep in mind when summarizing taxonomic abundances.\nMinimap2 is installed by default on Crunchomics.\n\n\n\n\n\n\nQuestionTasks\n\n\n\n\n\nTask 1: Get the reference database\nWe have already prepared a non-redundant database of 16S rRNA gene sequences. Download it to your data/ folder using:\n\nwget https://raw.githubusercontent.com/ndombrowski/MicEco2025/refs/heads/main/data/amplicons_nr.fasta -P data\n\nThis file (amplicons_nr.fasta) contains one to two representative 16S rRNA sequences per strain used in the lab. View the file with head and use grep to count how many sequences are in the file.\nTask 2: Run minimap2\nNext, you will map your filtered reads against the reference database using minimap2.\nBefore running, make sure to check all the available options with minimap2 -h and then make sure tha tyou:\n\nCreate a folder to store your results\nUse srun with 2 CPUs and 20G of memory\nUse -cx map-ont for accurate Nanopore vs reference mapping\nEnsure that the output is in PAF format\nProvide both\n\ndata/amplicons_nr.fasta (the reference)\nbarcodeX_filtered.fastq.gz (the cleaned fastq sequences)\n\nUse &gt; results/minimap2/barcodeX.paf to store the output\n\nTask 3: Explore the output\nAfter the alignment finishes:\n\nCount the number of alignments with wc\nView the file with less and scroll through the first few lines to inspect the structure.\nCheck the minimap2 manual to find out what the different columns mean.\n\nAnswer the following questions:\n\nHow many alignments are there? Is this number higher or lower than your total number of input reads? Why might that be?\nLook at the first 12 columns of the PAF file. Which of these could help you filter unreliable alignments or identify multi-mapping reads?\n\n\n# Count the number of reference sequences\n... \n\n# Generate a suitable output folder \n...\n\n# Run minimap2 \n...\n\n# Count the number of alignments\n...\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Count the number of reference sequences\ngrep -c \"&gt;\"  data/amplicons_nr.fasta\n\n# Generate a suitable output folder \nmkdir results/minimap2\n\n# Run minimap2 \nsrun --cpus-per-task 2 --mem=20G minimap2 \\\n        -cx map-ont -t 2 \\\n        data/amplicons_nr.fasta \\\n        results/chopper/barcodeX_filtered.fastq.gz \\\n        &gt; results/minimap2/barcodeX.paf\n\n# Count number of alignments: 2059 (-1 because of the header)\nwc -l results/minimap2/barcodeX.paf\n\n# Explore the file itself\nless -S results/minimap2/barcodeX.paf\n\nAnswer:\n\nThere are 2059 alignments for 2976 reads. If you see more alignments then there are reads, there might be different reasons for that:\n\nIf your experiment included Pseudomonas strains you likely will have more alignments than reads. The higher number of alignments occurs because some reads might align equally well to multiple reference 16S sequences, these are multi-mapping reads.\nSome reads might only partially or weakly align to highly similar regions in the 16S rRNA gene\n\nUseful PAF columns for assessing alignment reliability include:\n\nColumn 10: number of matching bases (can be use to calculate % identity)\nColumn 11: total length of the alignment (including gaps; can be use to calculate % identity)\nColumn 12: mapping quality. 0 is used to mark and thereby identify multi-mappers\nColumns 3–4: read start and end positions (can be used to calculate how much of the read aligned)\n\n\nInformation on the PAF output:\n\n\n\nCol\nDescription\n\n\n\n\n1\nQuery sequence name\n\n\n2\nQuery sequence length\n\n\n3\nQuery start coordinate (0-based)\n\n\n4\nQuery end coordinate (0-based)\n\n\n5\n‘+’ if query/target on the same strand; ‘-’ if opposite\n\n\n6\nTarget sequence name\n\n\n7\nTarget sequence length\n\n\n8\nTarget start coordinate on the original strand\n\n\n9\nTarget end coordinate on the original strand\n\n\n10\nNumber of matching bases in the mapping\n\n\n11\nNumber bases, including gaps, in the mapping\n\n\n12\nMapping quality (0-255 with 255 for missing)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Amplicon long-read analysis</span>"
    ]
  },
  {
    "objectID": "source/applied_workflow.html#generate-a-count-table",
    "href": "source/applied_workflow.html#generate-a-count-table",
    "title": "6  Amplicon long-read analysis",
    "section": "6.3 Generate a count table",
    "text": "6.3 Generate a count table\nNow that we have our alignment results, our goal is to summarize them into a count table, a matrix showing how many reads aligned to each reference 16S rRNA sequence. Before creating the count table, we first need to filter our results to make sure we only count high-confidence matches by:\n\nRemoving low-confidence hits by filtering out alignments with:\n\nLow sequence identity (many mismatches)\nLow coverage (only part of the read mapped to the reference)\nLow mapping quality (reads could map equally well elsewhere)\n\nSelecting the single best alignment for each read (to handle multi-mappers)\n\nAfterwards, we can count how many reads map to each reference sequence. These counts can then be summarized at higher taxonomic levels (e.g., genus) to account for the uncertainty due to sequencing errors.\nInstead of using a single pre-built program, we will use a custom Python script to perform this step. So why use a tool like Python or R for this?\nTools like Minimap2 produce large, text-based alignment files. While command-line tools are great for specific tasks (like counting lines or filtering by quality), programming languages like Python and R allow you to:\n\nCombine multiple filtering criteria (e.g. identity, coverage, quality)\nParse complex data formats\nSummarize and visualize results reproducible\n\n\n\n\n\n\n\nQuestionTasks\n\n\n\n\n\nTask 1: Install the necessary software\nTo run the required software, we first need to install some packages to parse the data. You can do this by running the following:\n\nmamba create -n python_3.10.18 -c conda-forge python=3.10.18 matplotlib pandas\n\nTask 2: Get the python scripts\nDownload the two required files:\n\nwget https://raw.githubusercontent.com/ndombrowski/MicEco2025/refs/heads/main/scripts/paf_to_matrix.py -P scripts\nwget https://raw.githubusercontent.com/ndombrowski/MicEco2025/refs/heads/main/data/accession_to_genus.tsv -P data\n\n\npaf_to_matrix.py is the python script that filters alignment results and generates count tables\naccession_to_genus.tsv is a two-column tab-delimited file linking each 16S rRNA gene accession to its genus\n\nYou can inspect the script’s help menu to understand its required inputs by typing python scripts/paf_to_matrix.py -h. The main arguments are:\n\n-i: path to the folder containing PAF alignment files\n-o: output folder\n-t: path to two-column TSV file linking accessions to genus names\n-s: (optional) seqkit stats file generated with _Tao, used to calculate unmapped reads\nFiltering thresholds for identity, coverage, and mapping quality\n\nTask 3: Generate the count table\nNow, use paf_to_matrix.py to convert your alignment results into a count table. You will need to:\n\nMake sure to activate the conda environment we setup in Task 1\nProvide the path to your PAF alignment folder (-i)\nAdd your seqkit stats file (-s)\nInclude the accession-to-genus mapping file (-t)\nChoose the folder in which you want to save your results (-o)\n\n\nHint: Python scripts like this one are not computationally demanding, you can safely run it on the login node without using srun.\n\n\n# Activate the python conda environment \n... \n\n# Make folder to store the results \n...\n\n# Run the python script\npython scripts/paf_to_matrix.py \\\n  -i ... \\\n  -s ... \\\n  -t ... \\\n  -o ...\n\nTask 4: Explore the count table\npaf_to_matrix.py will:\n\nPrint a brief summary of the filtering to the screen\nGenerate several tables:\n\notu_table_multimappers.tsv: A count table based on all alignments, including multi-mappers\notu_table.tsv: A count table based on only best (primary) hits per read\notu_table_genus.tsv: A count table summarized at the genus rank\n\n\nInspect the outputs and answer the following:\n\nHow many reads were lost due to the filtering? Check the options of python scripts/paf_to_matrix.py -h and think about whether there are any parameters you would change in case too many reads were removed?\nAre the taxa listed in your count tables consistent with the strains used in your experiment?\nDo the relative abundances of taxa match your expectations? If not, what biological or technical factors could explain the differences?\n\n\n\n\n\n\n\nAnswerClick to see the answer\n\n\n\n\n\n\n# Activate the python conda environment \nconda activate python_3.10.18\n\n# Make folder to store the results \nmkdir results/tables\n\n# Run the python script\npython scripts/paf_to_matrix.py \\\n  -i results/minimap2/ \\\n  -s results/seqkit/barcodeX_filtered.tsv \\\n  -t data/accession_to_genus.tsv \\\n  -o results/tables\n\nconda deactivate\n\n\nAbout 8% of reads were lost during the filtering. If you had more alignments than reads, the filtered reads were likely secondary (less confident) alignments. If too many reads are removed, try lowering: the sequence identity threshold (id), mapping threshold (-mapq) or allowed coverage threshold (-cov)\nMost reads map to Streptococcus and Lactococcus, consistent with the experiment. A few unassigned reads may come from contamination or poor-quality reads.\nStreptococcus had ~10× more reads than Lactococcus. This may indicate stronger growth due to better adaptation to liquid media or competitive advantage in the co-culture.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Amplicon long-read analysis</span>"
    ]
  }
]