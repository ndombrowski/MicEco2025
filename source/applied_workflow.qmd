---
execute:
  eval: false
engine: knitr

---

# Amplicon long-read analysis

Sections that are still in progress:

- Work on sections quality cleaning (consider testing fastplong), consensus generation (ngspeciesID), blast for taxonomic assignment and minimap for counts --> will be added to Chapter 6 (From raw reads to results)


## `chopper`: Quality cleaning 

In this section, you will learn how to perform quality filtering of your amplicon long-read data using chopper. Quality control is an essential first step to remove low quality reads that introduce errors in downstream analyses such as consensus generation and taxonomic assignment. Typical things you want to do are:

- Removing sequence adapters, barcodes, primer sequences. This already has been done for your data.
- Removing reads with a too low quality score
- Removing reads that are too long or too short
- Trimming read ends if they have low quality scores

`chopper` is designed for filtering and trimming of long-read data and you can install `chopper` with mamba:

```{bash}
mamba create -n chopper_0.11.0 -c bioconda chopper=0.11.0
```

In this tutorial, you will always receive example code as a starting point, with sections containing `...` for you to fill in. If you get stuck, check [chopper's documentation](https://github.com/wdecoster/chopper) or use the built-in help via the CLI (`chopper -h`).

::: {.callout-question .callout-warning collapse=false}
# Tasks 

Use your NanoPlot quality plots from the previous section and choose some quality and length cutoffs. Specifically, you want to:

1. Perform quality filtering with chopper:
    - Store the output of chopper in a separate results folder
    - Request 2 CPUs and 20G of memory with `srun`, and ensure chopper itself also uses 2 threads
    - Discard sequences that are too long or too short
    - Discard reads, where the read's average quality score is lower than a quality score of Q15
    - Check the [tools manual](https://github.com/wdecoster/chopper) carefully with how to deal with `.gz` files. Hint: Closely check the example section
2. Investigate the quality of the filtered FASTQ file with NanoPlot 
    - use srun, 2 CPUs and 10G of memory
3. afterwards open NanoPlots LengthvsQualityScatterPlot_dot.html and NanoStats.txt outputs and answer:
    - The total number of sequences that went into the analysis
    - The total number of sequences that passed the quality filtering
    - At which step most reads were lost (and how many)
    - How many basepairs were lost due to the trimming

```{bash}
# Generate an output folder for the results
... 

# Activate the chopper conda environment
...

# Run chopper
srun --cpus-per-task 2 --mem=20G chopper ...

# Deactivate the conda environment 
...

# Make a folder for the Nanoplot results and run Nanoplot
...
```


::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

```{bash}
# Generate an output folder for the results
mkdir results/chopper

# Activate the chopper conda environment
conda activate chopper_0.11.0

# Run chopper
srun --cpus-per-task 2 --mem=20G chopper \
    -i data/barcodeX.fastq.gz \
    -q 15 \
    --minlength 1300 --maxlength 1600 \
    --threads 2 | gzip \
    > results/chopper/barcodeX_filtered.fastq.gz

# Deactivate the conda environment 
conda deactivate

# Run Nanoplot
conda activate nanoplot

mkdir -p results/nanoplot/cleaned

srun --cpus-per-task 2 --mem=10G NanoPlot \
    --fastq results/chopper/barcodeX_filtered.fastq.gz \
    -t 2 \
    --tsv_stats \
    --plots dot \
    -o results/nanoplot/cleaned

conda deactivate
```

Below are example results from one of my analyses. These numbers will be different for you, however, this process will make you more familiar for what to look for in your own analyses.

- Input: 43,170 reads
- Passed filtering: 31,102 reads (72%). For this data this is reasonable given the relatively high Phred score we use for quality filtering


:::
:::



## `NGSpeciesID`: Read clustering 

Nanopore long-read data generates sequence reads that are almost as long as the 16S rRNA gene sequence, which is great for identifying species. However, long reads come with a tradeoff: the have a much higher error rate. 

In our dataset the average read quality is Q15. The phred score (Q) is a measure of the probability that a base is called correctly. We can calculate this with:

$$P=10^{-Q/10}$$

In R we can compute this with:

```{r}
#| eval: true
Q <- 16
P_error <- 10^(-Q/10)
percent_error <- P_error * 100
message("Error rate: ", round(percent_error, 2), "%")
```

With Q15, each base has a ~3.2% chance of being incorrect. In a 1,500 bp amplicon that would mean 48 bases would be incorrect and this can make it difficult to directly assign sequences to a species.

To deal with this we can cluster similar reads together in groups of sequences that likely come from the same species. Tools like NGSpeciesID build consensus sequences from each cluster in order to correct these randomly occurring errors by comparing all the reads in a cluster.

You can install NGSpeciesID with mamba:

```{bash}
# Installation a base environment
mamba create -n NGSpeciesID_0.3.1  python=3.11 pip

# Inside the base environment install all required tools
conda activate NGSpeciesID_0.3.1
mamba install --yes -c conda-forge -c bioconda medaka==2.0.1 openblas==0.3.3 spoa racon minimap2  samtools
pip install NGSpeciesID

conda deactivate
```


::: {.callout-question .callout-warning collapse=false}
# Task 

Familiarize yourself with the [NGSpeciesID documentation] and finish the following sbatch script that you can store in `scripts/ngspeciesid.sh`. 

Inside the script do the following:

- Run the script with 5 CPUs, 50G of memory for 1 hour
- For NGSpeciesID:
    - Provide the path to the fastplong cleaned FASTQ file
    - Ensure that NGSpeciesID runs with 5 cores 
    - Ensure that the output is stored in a separate new folder
    - Tell that you work with ONT data
    - Tell that you want to cluster the data with `--consensus` and clean the data further with `--medaka`
    - Lower the abundance ratio to 0.01 to detect potential contamination with few reads
    - Increase the minimum aligned fraction of read to be included in cluster to 0.95 so that highly similar reads align. I.e. 95% identity is needed to merge reads into a cluster 


Submit the job and then answer the following:

- 


```{bash}
#!/bin/bash
#SBATCH --job-name=ngspeciesid
#SBATCH --output=logs/ngspeciesid_%j.out
#SBATCH --error=logs/ngspeciesid_%j.err
#SBATCH --cpus-per-task=...
#SBATCH --mem=...
#SBATCH --time=...

# Activate the conda environment
source ~/.bashrc 
conda activate NGSpeciesID_0.3.1

# Run NGSpeciesID
NGSpeciesID ...
```

::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

The `scripts/ngspeciesid.sh` should look as follows:

```{bash}
#!/bin/bash
#SBATCH --job-name=ngspeciesid
#SBATCH --output=logs/ngspeciesid_%j.out
#SBATCH --error=logs/ngspeciesid_%j.err
#SBATCH --cpus-per-task=5
#SBATCH --mem=50G
#SBATCH --time=1:00:00

# Activate the conda environment
source ~/.bashrc # this is needed so that conda is initialized inside the compute node
conda activate NGSpeciesID_0.3.1

echo "Job started: "
date

NGSpeciesID \
    --ont \
    --fastq results/fastplong/barcodeX.fastq \
    --outfolder results/ngspeciesid \
    --consensus --medaka --t 5 \
    --aligned_threshold 0.95 \
    --abundance_ratio 0.01 

echo "Job ended: "
date
```


:::
:::