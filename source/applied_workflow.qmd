---
execute:
  eval: false
engine: knitr

---

# Amplicon long-read analysis

Sections that are still in progress:

- Work on sections quality cleaning (consider testing fastplong), consensus generation (ngspeciesID), blast for taxonomic assignment and minimap for counts --> will be added to Chapter 6 (From raw reads to results)


## `fastplong`: Quality cleaning 

In this section, you will learn how to perform quality filtering and adapter trimming of your amplicon long-read data using fastplong. Quality control is an essential first step to ensure that only high-quality reads are used for downstream analyses such as consensus generation and taxonomic assignment.

You can install fastplong with mamba:

```{bash}
mamba create -p fastplong_0.4.1 -c bioconda fastplong=0.4.1
```

In this tutorial, you will always receive example code as a starting point, with sections containing `...` for you to fill in. If you get stuck, check [the tools official documentation](https://github.com/OpenGene/fastplong) or use the built-in help via the CLI (`fastplong -h`).

::: {.callout-question .callout-warning collapse=false}
# Task 

Use your NanoPlot quality plots from the previous section and choose some quality and length cutoffs. Specifically, you want to:

- Store the output of fastplong in a separate results folder 
- Request 2 CPUs with `srun`, and ensure fastplong also uses 2 threads
- Discard sequences that are too long or too short
- Discard reads, where the read's average quality score is lower than a quality score of Q8.
- Move a sliding window from front (5') to tail, drop the bases in the window if its mean quality < Q8

```{bash}
# Generate an output folder for the results
... 

# Activate the fastplong conda environment
...

# Run fastplong
srun --mem=20G --cpus-per-task 2 fastplong ...

# Deactivate the conda environment 
...
```

After the analysis finished check the output and record:

- The total number of sequences that went into the analysis
- The total number of sequences that passed the quality filtering
- At which step most reads were lost (and how many)
- How many basepairs were lost due to the trimming


::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

```{bash}
# Generate an output folder for the results
mkdir results/fastplong

# Activate the fastplong conda environment
conda activate fastplong_0.4.1

# Run fastplong
srun --mem=20G --cpus-per-task 2 fastplong \
    -i data/barcode01_merged.fastq.gz \
    -l 1400 --length_limit 1700 \
    --mean_qual 8 --cut_front 8 \
    --thread 2 \
    -o results/fastplong/barcode01_filtered.fastq.gz

# Deactivate the conda environment 
conda deactivate
```

**Parameter overview:**
- `--mean_qual 8`: Removes reads with an average Phred quality below 8
- `-l 1400 --length_limit 1700`: Keeps reads roughly within the expected amplicon size range
- `--cut_front 8`: Removes low-quality bases using a sliding window approach
- `--thread 2`: Uses two CPU threads for parallel processing


Below are example results from one of my analyses. These numbers will be different for you, however, this process will make you more familiar for what to look for in your own analyses.

- Input: 17,121 reads
- Passed filtering: 8,433 reads (49%). For my data this is reasonable given the left-tail seen in the NanoPlot quality plot
- Most reads were lost because they were too short (4,627) or had low quality (4,084)
- The dataset initially contained 24,089,162 bp, of which 12,339,313 bp (~51%) were retained

:::
:::



## `NGSpeciesID`: Read clustering 

Nanopore long-read data generates sequence reads that are almost as long as the 16S rRNA gene sequence, which is great for identifying species. However, long reads come with a tradeoff: the have a much higher error rate. 

In our dataset the average read quality is Q15. The phred score (Q) is a measure of the probability that a base is called correctly. We can calculate this with:

$$P=10^{-Q/10}$$

In R we can compute this with:

```{r}
#| eval: true
Q <- 16
P_error <- 10^(-Q/10)
percent_error <- P_error * 100
message("Error rate: ", round(percent_error, 2), "%")
```

With Q15, each base has a ~3.2% chance of being incorrect. In a 1,500 bp amplicon that would mean 48 bases would be incorrect and this can make it difficult to directly assign sequences to a species.

To deal with this we can cluster similar reads together in groups of sequences that likely come from the same species. Tools like NGSpeciesID build consensus sequences from each cluster in order to correct these randomly occurring errors by comparing all the reads in a cluster.

You can install NGSpeciesID with mamba:

```{bash}
# Installation a base environment
mamba create -n NGSpeciesID_0.3.1  python=3.11 pip

# Inside the base environment install all required tools
conda activate NGSpeciesID_0.3.1
mamba install --yes -c conda-forge -c bioconda medaka==2.0.1 openblas==0.3.3 spoa racon minimap2  samtools
pip install NGSpeciesID

conda deactivate
```


::: {.callout-question .callout-warning collapse=false}
# Task 

Familiarize yourself with the [NGSpeciesID documentation] and finish the following sbatch script that you can store in `scripts/ngspeciesid.sh`. 

Inside the script do the following:

- Run the script with 5 CPUs, 50G of memory for 1 hour
- For NGSpeciesID:
    - Provide the path to the fastplong cleaned FASTQ file
    - Ensure that NGSpeciesID runs with 5 cores 
    - Ensure that the output is stored in a separate new folder
    - Tell that you work with ONT data
    - Tell that you want to cluster the data with `--consensus` and clean the data further with `--medaka`
    - Lower the abundance ratio to 0.01 to detect potential contamination with few reads
    - Increase the minimum aligned fraction of read to be included in cluster to 0.95 so that highly similar reads align. I.e. 95% identity is needed to merge reads into a cluster 


Submit the job and then answer the following:

- 


```{bash}
#!/bin/bash
#SBATCH --job-name=ngspeciesid
#SBATCH --output=logs/ngspeciesid_%j.out
#SBATCH --error=logs/ngspeciesid_%j.err
#SBATCH --cpus-per-task=...
#SBATCH --mem=...
#SBATCH --time=...

# Activate the conda environment
source ~/.bashrc 
conda activate NGSpeciesID_0.3.1

# Run NGSpeciesID
NGSpeciesID ...
```

::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

The `scripts/ngspeciesid.sh` should look as follows:

```{bash}
#!/bin/bash
#SBATCH --job-name=ngspeciesid
#SBATCH --output=logs/ngspeciesid_%j.out
#SBATCH --error=logs/ngspeciesid_%j.err
#SBATCH --cpus-per-task=5
#SBATCH --mem=50G
#SBATCH --time=1:00:00

# Activate the conda environment
source ~/.bashrc # this is needed so that conda is initialized inside the compute node
conda activate NGSpeciesID_0.3.1

echo "Job started: "
date

NGSpeciesID \
    --ont \
    --fastq results/fastplong/barcodeX.fastq \
    --outfolder results/ngspeciesid \
    --consensus --medaka --t 5 \
    --aligned_threshold 0.95 \
    --abundance_ratio 0.01 

echo "Job ended: "
date
```


:::
:::