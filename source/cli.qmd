---
engine: knitr
toc-expand: 2
---

# Navigating the command line

## `pwd`: Find out where we are

Once your terminal is open, let’s get oriented by typing your first command and then pressing enter: 

```{bash}
pwd
```

The command `pwd` stands for **print working directory**. It tells you where you currently are in the file system, that is, which folder (directory) your shell is operating on right now. You should see something like:

```
/Users/YourUserName
```

::: {.callout-tip title="Tip: Finding the Desktop on Different Systems" collapse="true"}
Your home directory path varies slightly across operating systems. Here’s how to locate yourself and connect to familiar locations like the Desktop:

**macOS**

-   Your home directory is `/Users/YourUserName`
-   To open the folder you are currently in Finder:`open .`
-   Your desktop is at `/Users/YourUserName/Desktop`

**MobaXterm (Windows)**

-   Your home directory is `/home/mobaxterm`
-   By default, this is temporary and is deleted when you close MobaXterm. To make it permanent:
    -   Go to Settings --\> Configuration --\> General
    -   Under Persistent home directory, choose a folder of your choice
-   To open the folder you are currently in the Windows File explorer: `explorer.exe .`
-   Your Desktop is usually at: `/mnt/c/Users/YourUserName/Desktop` or `/mnt/c/Users/YourUserName/OneDrive/Desktop` (when using OneDrive)

**WSL2 (Windows)**

-   Your home directory is`/home/YourUserName`
-   To open the folder you are currently in the Windows File explorer: `explorer.exe .`
-   Your Desktop is usually at: `/mnt/c/Users/YourUserName/Desktop` or `/mnt/c/Users/YourUserName/OneDrive/Desktop` (when using OneDrive)

If you want to access the Uva OneDrive folder:

If your OneDrive folder name includes spaces (like OneDrive - UvA), use quotes around the path: 

```{bash}
cd "/mnt/c/Users/YourUserName/OneDrive - UvA"
```

:::


## `ls`: List the contents of a directory

Now that we know where we are, let's find out what is inside that location, i.e. what files and folders can be found there. The command `ls` (short for *list*) shows the files and folders in your current directory. Type the following and press enter:

```{bash}
ls
```

You should see something like this (your output will vary depending on what’s in your directory):

![](../img/ls.png)

The colors and formatting depend on your terminal settings, but typically:

- Folders (directories) appear in one color (often green or blue)
- Files appear in another (often white or bold)

If your directory contains many items, the output can quickly become overwhelming. To make sense of it, we can use options and arguments to control how commands behave.


## The structure of a command

A command generally has three parts:

- A command: The program you want to run, i.e. `ls`
- An option (or flag): A way to modify how the command behaves, i.e `-l` (long format)
- An optional argument: The input, i.e. a file or folder

![](../img/unix_command.png){width=80%}

Try the following command in your current directory to "List (`ls`) the contents of the current folder and show details in long format (`-l`)":

```{bash}
ls -l
```

After running this you should see a more detail list of the contents of your folder.In the example below we can see that we now print additional information about who owns the files (i.e. access modes), how large the files are, when they were last modified and of course the name:

![](../img/ls2.png){width="80%"}


::: {.callout-tip title="Tip: Using ls in practice" collapse="false"}

Throughout this tutorial, you’ll notice that we use `ls` frequently. There’s a reason for that:

When working with data, sanity checks are essential, because it is easy to make mistakes, overwrite files, or lose track of where things are. Using simple commands like `ls`, `wc`, or `grep` helps you verify what’s happening to your files at every step of an analysis workflow.

These habits are not just for beginners, also experienced bioinformaticians rely on them constantly. For newcomers, practicing these checks early will help you build confidence and intuition when working on the command line.

:::

## Getting help

At some point, you’ll want to know what options a command has or how it works. In this case, you can always check the **manual pages** (or *man pages*) by typing `man` followed by the command name:

```{bash}
man ls
```

This opens the manual entry for the command ls. You can scroll through it using:

- ↑ / ↓ arrows or the space bar to move down
- b to move back up
- q to quit the manual

Not all commands come with such a manual. Depending on the program, there are a few common patterns you can try to get help:

-   `man ls`
-   `ls --help`
-   `ls -h`


## `mkdir`: Make a new folder

Before we start moving around, let's first learn how to create new folders (also called directories).This is something we will do often, for example, to keep raw data, results, and scripts organized in separate places. The command we use for that is `mkdir`, which stands for *make directory*.

For now, we will use `mkdir` to create a shared working folder with the name `data_analysis` for this tutorial. Don’t worry about how to move into the folder yet, we’ll cover that in the next section.

```{bash}
# Move into the home directory (the starting point of your system)
cd ~

# Create a new folder called 'data_analysis'
mkdir data_analysis

# Check that the folder was created successfully
ls
```

You should see a new folder called data_analysis appear in the list. We will use this folder as our project space for all exercises in this tutorial. You can easily make a new `data` folder inside the new data_analysis folder by typing the following:

```{bash}
# Make a data folder inside the data_analysis folder
# The `-p` option makes the parent directory, if it does not already exist
# It is useful to by default add `-p` when generating a folder inside a folder
mkdir -p data_analysis/data

# Check that the folder was created successfully
# Notice here, how we use ls with a flag and also with an optional argument?
ls -l data_analysis
```


::: {.callout-tip title="Tip: Commenting your code" collapse="false"}
Notice how we added # and some notes above each command?

Anything written after `#` in Bash is a comment. A comment won't be executed, but it helps you (and others) understand what the command does.

In your own work, add short, meaningful comments above key steps. Avoid restating the obvious, instead, explain why you’re doing something or what it achieves.
:::


## `cd`: Move around folders

Now that we have our own project folder, let’s learn how to move around the file system.

The file system is structured like a tree that starts from a single root directory (that is also denoted as `/` in bash). All other folders branch out from this root directory. For example, we can go from the root directory, to the users folder and from there into the john folder.

![](../img/filesystem.png)

There are two ways to specify a path to go the the portfolio folder:

- **Absolute path**: starts from the root (e.g. `cd /users/john/portfolio`)
- **Relative path**: starts from your current location (e.g. `cd portfolio` if you’re already in `/users/john`)

It is generally recommended to use the relative path from inside your project directory. That makes your code more portable and still allows you to run the code even if your computer setup changes.

Let’s practice moving between folders (at each step, use `pwd` in case you feel that you get lost):

```{bash}
# Move into the data analysis folder 
cd data_analysis 

# Check where we are 
pwd
```

We now should see that we are in something like `/Users/Name/data_analysis`. We can use the `cd` command in multiple ways to move around:

```{bash}
# Move into the data folder
cd data

# Move one level up, i.e. go back to the data_analysis folder
cd ..

# Move multiple levels at once
cd data_analysis/data

# Move two levels up
cd ../.. 

# Quickly go back home
cd ~

# And go back to the data_analysis folder 
cd data_analysis
```

In the code above, the tilde symbol (`~`) is a shortcut for your home directory. It's equivalent to typing the full absolute path to your home (e.g. `cd /Users/YourName`) but it is much faster to type.


::: {.callout-tip title="Tip: Command-line completion" collapse="false"}
Here, are some other tips for faster navigation (and less typos):

- Use Tab for autocompletion: type the first few letters of a folder name and press Tab.
- If there’s more than one match, press Tab twice to see all options.
- Use ↑ / ↓ arrows to scroll through previously entered commands

**Hint**: From now on try to use the Tab key once in a while so that you do not have to write everything yourself all the time.
:::


::: {.callout-question .callout-warning collapse=false}
# Question 

Familiarize yourself with these first commands and:

- Create a new folder inside your data_analysis directory called results
- Move into the results folder and confirm your location with `pwd`
- Move back inside the data_analysis folder
- Use `ls` to confirm both results and data are there

::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

```{bash}
mkdir results 
cd results 
pwd 
cd ..
ls 
```

:::
:::




## `wget`: Download data

Next, let’s download a genome fasta file to go through some other useful commands. We can use the `wget` command to fetch a genome fasta from an online website as follows:

```{bash}
# Download the example fasta file into the current directory
wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fna.gz
```

::: {.callout-tip title="Tip: Where does our data come from?" collapse="true"}

The genome comes from the [Plant-associated bacterial culture collections database](https://www.at-sphere.com/). This database contains information from microbial strains isolated from the roots and leaves of *Arabidopsis thaliana* and from roots and nodules from *Lotus japonicus*. Some of these strains are also used in your lab practicals. 

For convenience (and security reasons) you will for this tutorial will download the data from Github. However, in practice you will often download data from institutional websites such as the one above. 

:::

After you download data, it is always a good idea to do some sanity check to see if the file is present and know how large it is:

```{bash}
# List files in long (-l) and human-readable (-h) format
# combining these two commands becomes -lh
ls -lh LjRoot303.fna.gz
```


## `cp`: Copy files

`cp` duplicates files or directories. It is a useful feature to keep our files organized and not have every single file in a single folder but instead to organize your files into folder categories (useful folders can be: data, scripts and results). 

Let's use `cp` to copy the downloaded file into `data` and to organize the data a bit better:

```{bash}
# Copy file into data 
cp LjRoot303.fna.gz data/

# Show content of both locations
ls -l
ls -l data
```

When running the two `ls` commands, we see that we now have two copies of `LjRoot303.fna.gz`, one file is in our working directory and the other one is in our data folder. Having large files in multiple locations is not ideal since we will use unneccassary space. However, we can use another command to move the file into our data folder instead of copying it.


## `mv`: Move (or rename) files

`mv` moves or renames files without creating a second copy:

```{bash}
# Move the file into data
mv LjRoot303.fna.gz data/

# Verify
ls -l
ls -l data
```

Notice that `mv` will move a file and, without asking, overwrite the existing file we had in the data folder when we ran `cp`. This means that if you run `mv` its best to make sure that you do not overwrite files by mistake.



## `rm`: Remove files and directories

To remove files and folders, we use the `rm` command. For example we could remove the `LjRoot303.fna.gz` in case we don't need it anymore:

```{bash}
# Remove the genome file from the data folder
rm data/LjRoot303.fna.gz

# Check if that worked
ls -l data
```

If we want to remove a folder, we need to tell `rm` that we want to remove folders using an option. To do this, we use `-r` , which allows us to remove directories and their contents recursively.

::: callout-important
**Unix does not have an undelete command.**

This means that if you delete something with rm, it's gone. Therefore, use `rm` with care and check what you write twice before pressing enter!

Also, NEVER run `rm -r` or `rm -rf` on the root `/` folder or any important path. And yes, we have seen recommendations to use these combinations online. Therefore, always double check the path or file name before pressing enter when using the `rm` command.
:::

::: {.callout-question .callout-warning collapse=false}
# Question 

Download another genome:

- Download another genome (not LjRoot303) from this url: https://github.com/ndombrowski/MicEco2025/tree/main/data. To get the link, 
    - Click on the genome you want to download
    - Right click on `view raw` and select `copy link`
    - Use wget to download that file
- Make sure that the new genome stored in the data folder
- Check the file size (hint: use the -h option with ls)

::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

```{bash}
# Download the genome
wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot44.fna.gz

# Move the genome file to the data folder
mv LjRoot44.fna.gz data

# Check the file size
ls -lh data/
```

:::
:::


## `gzip`: (Un)compressing files

You might have noticed that the file we downloaded ends with `.gz`. This extension is used for files that are compressed to make the file smaller. This is useful for saving files, but this makes the file unreadable for a human. To be able to learn how to read the content of a file, let's learn how to uncompress the file first.

```{bash}
# Download another genome  
# Here, we use the -P option to directly download the file into the data folder 
wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot303.fna.gz -P data

# Check that this worked 
# You now should see two genomes
ls data

# Uncompress the file with gzip 
# Here, we use -d to decompress the file 
# (by default gzip will compress a file)
gzip -d data/LjRoot303.fna.gz 

# Check that this worked 
# We now should see that one file is uncompressed (i.e. it lost the gz extension)
ls data
```



## Exploring file contents

Now that we have the uncompressed file we can use different ways to explore the content of a file.

### `cat`: Print the full file

We can use `cat` print the entire file to the screen, this is fine for short files, but overwhelming for long ones, which for our file is the case as we will see the nucleotides just running across the screen.

```{bash}
cat data/LjRoot303.fna
```


### `head` and `tail`: View parts of a file

To only print the first few or last few lines we can use the `head` and `tail` commands:

```{bash}
# Show the first 10 lines
head data/LjRoot303.fna

# Show the last 5 lines
# Here, the option -n allows us to control how many lines get printed
tail -n 5 data/LjRoot303.fna
```


### `less`: View the full file

`less` let's you view a file's contents one screen at a time. This is useful when dealing with a large text file (such as a sequence data file) because it doesn't load the entire file but accesses it page by page, resulting in fast loading speeds.

```{bash}
less -S data/LjRoot303.fna
```

-   You can use the arrow Up and Page arrow keys to move through the text file
-   To exit less, type `q`


### `zcat`: Decompress and print to screen

When we used gzip above, we decompressed the file and that allowed us to read the content of the fasta file. This is perfect for smaller files, but not ideal for sequence data since these files get large and we might not want to decompress these files as they would clutter our system.

Luckily, there is one useful tool in bash to decompress the file and print the content to the screen. `zcat` will print the content of a file to the screen but leave the file as is. Since the file is quite large, we see a lot of content flying across the screen, that is fine, we will see how to improve in a bit.

```{bash}
# Check the content of the data folder 
# We should have one compressed and one uncompressed file 
ls data

# Use zcat on the compressed file 
zcat data/LjRoot44.fna.gz

# Check the content of the data folder 
# We still should have one compressed and one uncompressed file 
ls data
```



::: {.callout-tip title="Tip: Editing text files" collapse="true"}
You can also edit the content of a text file and there are different programs available to do this on the command line, the most commonly used tool is `nano`, which should come with most command line interpreters. You can open any file as follows:

```{bash}
nano data/LjRoot303.fna
```

Once the document is open you can edit it however you want and then

-   Close the document with `control + X`
-   Type `y` to save changes and press enter
:::



## `wc`: Count things

Another useful tool is the `wc` (short for *wordcount*) command that allows us to count the number of lines via `-l` in a file. It is an useful tool for sanity checking.

```{bash}
wc -l data/LjRoot303.fna
```

We see that this file does contain 112,262 lines of text. For fasta files this is not very informative, but if you for example work with a dataframe with rows and columns where you filter rows using certain conditions this can become very useful for sanity checking.



## `grep` : print lines that match patterns

The `grep` command searches for patterns in a file. We could for example use this to ask how many contigs are in our fasta file by searching for the fasta header that always starts with a `>`. The basic grep syntax is `grep "pattern" file`. Here, "pattern" is the text you want to search for, and file is the file you want to search in. The double quotes allow us to include special characters (like `>`) safely.

```{bash}
grep ">" data/LjRoot303.fna
```

This prints all lines that match the pattern. In a fasta file, this means you see all the contig headers. Depending on your terminal, the matches might be highlighted, which makes them easy to spot.. 

If we only care about the number of contigs, we can use the `-c` option:

```{bash}
# Root303 contains 85 contigs
grep -c ">" data/LjRoot303.fna
```


::: {.callout-tip title="The structure of a fasta file" collapse="false"}

A sequence FASTA file is a text based format to store DNA or peptide sequences.  It should always look something like this:

![](../img/fasta.png)

The header always starts with a `>` followed by descriptive information. In a new line the sequence data gets stored in either a single line or multiple lines. A fasta file can continue single sequence or multiple sequences.

By convention the extensions `.fna` is used to store fasta sequences from nucleotides and `.faa` is used to store fasta sequences from proteins.

:::



::: {.callout-question .callout-warning collapse=false}
# Question 

1. Uncompress the file for LjRoot44
2. Compare the number of contigs for the LjRoot44 and LjRoot303 genomes
3. What does it mean if one genome has more contigs than another? Should this raise any concerns when analyzing the genome?

::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

```{bash}
# Uncompress 
gzip -d data/LjRoot44.fna.gz

# Count the number of contigs 
# We work with genomes with 85 and 12 contigs
grep -c ">" data/LjRoot303.fna
grep -c ">" data/LjRoot44.fna
```

Answer for question 3:

LjRoot44 appears more fragmented (more contigs) than LjRoot303. A fragmented genome can split interesting genes across contigs making them harder to find and annotate. When evaluating genomes, consider not just contig counts but also file size, genome size, and completeness.  

:::
:::


## Pipes 

So far, we’ve run one command at a time, for example. But often, you might want to combine commands so that the output of one becomes the input of another. That’s what the pipe (`|`) does. It allows us to chain simple commands together to do more complex operations.

```{bash}
# Example: show only the first 100 lines and then
# count how often a sequence motif occurs in these lines
head -n 100 data/LjRoot303.fna | grep -c ">" 
```

Here:

- `head -n 100 data/GCF_000005845.2_ASM584v2_genomic.fna` prints the first 100 lines
- The pipe (`|`) sends these 100 lines directly to the `grep` command
- `grep -c ">"` only counts how many headers are found in the first 100 lines of the fasta file



## Working with multiple files

So far, we’ve worked with single files. In practice, sequencing data often comes as multiple filesm for example, one FASTQ file per barcode. Let’s practice handling several files at once. Let's begin by getting more genomes and not only get genomes from *Lotus japonicus* (files with Lj prefix) but also some genomes from *Arabidopsis thaliana* (files with Root prefix):

**Hint**: If you downloaded one of the genomes in the exercise before, then you don't need to run this wget command.

```{bash}
# Download some more data 
wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/Root401.fna.gz -P data
wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/Root68.fna.gz -P data
wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/LjRoot59.fna.gz -P data

# Check what was done
ls -lh data
```

We now should have 5 files, 3 of which are still compressed. 

::: {.callout-tip title="Hint: Downloading many files with a for loop" collapse="true"}
Once you understand how to use `wget`, you can easily scale it up to download multiple files automatically using a simple **for loop**.

For example, suppose you have a list of genome IDs in a file called `genomes.txt`, one ID per line:

```
LjRoot303
LjRoot44
LjRoot59
LjRoot60
Root401
Root68
Root935
```

You can then loop through all URLs and download them into the `data` folder like this:

```{bash}
# make a playground folder
mkdir playground 

# Download all genomes at once
for genome in $(cat genomes.txt); do
    wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/${genome}.fna.gz -P playground
done
```

Here’s what happens:

- `$(cat genomes.txt)` reads all lines from the file
- `for genome in ...` goes through each line one by one
- `wget github_path/${genome}.fna.gz -P playground` downloads each genome into the playground folder. Here, the `${genome}` will get replaced with one line in `genomes.txt`

You can use this same loop structure to run any command on multiple files automatically. This is one of the main reasons the CLI is so powerful. For example, you could in one command count the number of contigs in all your genomes:

```{bash}
# Download all genomes at once
for genome in $(cat genomes.txt); do
    echo $genome
    zcat playground/${genome}.fna.gz | grep -c ">"
done
```

You can also format this more nicely if you really want to but this is outside of the scope of this tutorial. A more detailed explanation about for-loops can be found [here](https://scienceparkstudygroup.github.io/ibed-bioinformatics-page/source/core_tools/bash-for-loops.html).

:::



### Wildcards (`*`): Match multiple files

Imagine we want to uncompress all the new files. Typing every filename can get tedious. **Wildcards** are another tool that you can use to work with groups of files using pattern matching.

```{bash}
# List all files inside the data folder that end with gz
# We should see only 3 files
ls data/*gz

# List all Lotus fasta files inside the data folder 
# using `Lj*.fna*` means we look for filenames that 
# Start with Lj, followed by any number of characters before and ending with .fna
ls data/Lj*.fna
```

We can use Wildcards with every bash command and use it to unzip every file at once with the following command:

```{bash}
gzip -d data/*gz

# Check if that worked
ls -lh data
```


### `cat` and `>` : Combining and saving files 

The cat command doesn’t just print files, it can also concatenate (join) multiple files into one. For example, we might want to combine all contigs for the Lotus (`Lj`) genome and store the output in a new file called `lotus.fna` in the results folder (a folder you should have generated in an earlier exercise).

```{bash}
# Combine the Lotus fasta files into one
cat data/Lj*.fna > results/lotus.fna
```

Here:

- `cat data/Lj*` selects all files in the data folder that start with `Lj` and end with `.fna`. 
- `>` tells the shell to write the combined output into a new file called lotus.fna


::: {.callout-caution title="Important: Be careful with >" collapse="false"}
The `>` operator overwrites files without asking.
If you want to add (append) to an existing file instead of replacing it, use `>>`:
:::

Whenever you modify files it is a good idea to do some sanity checks. Luckily, we already learned about useful ways to do this:

```{bash}
# Check how many protein files are in the individual faa files 
grep -c ">" data/Lj*

# Check how many protein files are in concatenated file
# Hopefully the numbers add up
grep -c ">" results/lotus.fna
```



::: {.callout-question .callout-warning collapse=false}
# Question 

1. Combine the Arabidopsis contigs (Root Prefix) into a new file that should be stored in the results folder
2. Count the total number of contigs in the individual and the combined file

::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

```{bash}
# Combine
cat data/Root*.fna > results/arabidopsis.fna

# Count 
grep -c ">" data/Root*.fna 
grep -c ">" results/arabidopsis.fna
```

:::
:::