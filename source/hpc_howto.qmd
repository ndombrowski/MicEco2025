---
execute:
  eval: false
engine: knitr

---

# Working on an HPC

Now that you are more comfortable with the command line, we will start working on the UVA Crunchomics HPC. In this section, you will go through the following steps:

- Connect to the HPC and transfer your sequencing files
- Install software using conda
- Learn how to run analyses using SLURM job submission scripts
    - `srun` for shorter interactive analyses
    - `sbatch` for longer running, more resource intensive analyses


## Connecting to the HPC

### `ssh`: Connecting to a sever

**SSH** (Secure Shell) is a network protocol that allows you to securely connect to a remote computer such as the Crunchomics HPC. The general command looks like this:

```{bash}
ssh -X username@server
```

Here:

- username is your account name on the HPC, i.e. your UvanetID
- server is the address of the HPC you want to connect to, for Crunchomics this is `omics-h0.science.uva.nl`
- `-X` enables X11 forwarding, which allows graphical applications from the server (like plotting or viewing images) to appear on your local machine. "Untrusted" X11 forwarding means the server can send graphical output, but it has limited access to your local machine

For Crunchomics at UVA, you would use:

```{bash}
ssh -X uvanetid@omics-h0.science.uva.nl
```

::: callout-important

**Important tips for connecting:**

- If you want to log into Crunchomics while working from UvA use the eduroam network, not the open Amsterdam Science Park network  
- If you are connecting from outside UVA, you must be on the VPN. Contact ICT if you have not set it up or encounter issues
- Always double-check your username and server address to avoid login errors
:::


### Crunchomics: Preparing your account

If this is your **first time using Crunchomics**, you need to run a small Bash script to set up your account. This script will:

-   Add `/zfs/omics/software/bin` to your PATH variable. This basically allows Bash to locate and use the system-wide installed software available on Crunchomics
-   Set up a Python 3 environment with some useful Python packages pre-installed
-   Create a link to your 500 GB personal directory inside your home directory, giving you plenty of space to store data and results

To set up your account, run the following commands:

```{bash}
# First orient yourself by typing 
pwd 
ls -lh 

# Run the Crunchomics installation script
/zfs/omics/software/script/omics_install_script

# Check if something changed
# You now should see the personal folder inside your home directory 
ls -lh
```


### `conda`: Setting up your own software environment

Many bioinformatics tools are already installed on Crunchomics, but sometimes you’ll need additional ones (like NanoPlot for analysing long-read data). To manage and install your own tools, we use conda/mamba, an environment manager that lets you create isolated environments for different software.

#### Install conda/mamba

Many systems already include conda or mamba. Before installing a new copy, check if it’s already available:

```{bash}
which conda
which mamba
```

If one of these commands returns “command not found”, then you can follow the next steps to install conda/mamba yourself.

To install Miniforge (which includes conda and mamba by default), run:

```{bash}
# Download the miniforge installation script
curl -L -O "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"

# Execute the miniforge installation script
bash Miniforge3-$(uname)-$(uname -m).sh
```

During installation:

- Read the license (scroll by pressing enter) and say "Yes" when asking for confirmation
- Choose the installation location
    - On Crunchomics, your home directory has only 25 GB of space. Therefore install everything into the personal folder that comes with 500 GB: `/zfs/omics/personal/uvanetID/miniforge3`
- Say "yes" when asked if conda should be initialized
- Restart your shell by typing `source ~/.bashrc`. You now should see a `(base)` in front of your prompt indicating that you are now inside conda's base environment
- Verify the installation:

```{bash}
conda -h
mamba -h
```



#### Installing NanoPlot 

[NanoPlot](https://github.com/wdecoster/NanoPlot) is a visualization tool for long-read sequencing data. We can install it, and other software, using mamba (or conda if mamba is unavailable). 

```{bash}
# Check if NanoPlot is already available
NanoPlot -h

# If not installed, create a new environment called nanoplot and install it
# Press "Y" when prompted about confirming the installation
mamba create -n nanoplot -c bioconda nanoplot

# Activate the new environment
conda activate nanoplot 

# Check if the tool is installed
NanoPlot -h

# Exit the environment
conda deactivate
```

**Recommendations**:

- Whenever possible, use mamba instead of conda, it resolves dependencies faster
- Keep your base environment clean (the base environment is the conda environment you start with when you log into the HPC). Always create new environments for each tool (`conda create -n toolname …`) instead of installing everything in base
- Check the tool’s documentation for specific installation notes or version requirements
- You can install specific versions when needed with `mamba create -n nanoplot_v1.42 -c bioconda nanoplot=1.42`
- You can remove environments you no longer need with `conda env remove -n nanoplot`


### Preparing your working directory

Next, you can start organizing your project folder. You always want to generate project folders inside your personal directory, which provides more storage than the home directory:

```{bash}
# Go into the personal folder
cd personal 

# Make and go into a new project folder
mkdir data_analysis
cd data_analysis 

# Ensure that you are in the right folder 
pwd
```

You now have a clean workspace for all the analyses that you will run during this tutorial on Crunchomics. As in your local machine, it’s good practice to keep your raw data, results, and scripts organized in separate folders.


### `scp`: Transferring data from/to a server

To learn how to transfer data to Crunchomics, we want to transfer the `data` folder (the one you have generated in previous part of the tutorial) to the HPC. We can do this with the `scp `(Secure Copy Protocol) command, which securely transfers files or directories between your computer and a remote server.

The basic syntax is:

- `scp [options] SOURCE DESTINATION`
- For connecting to any HPC the syntax is `server:file_location`

When transferring data you must **run the command from a terminal session on your own computer**, not from a terminal session that runs on the HPC.

To copy the entire data folder into your Crunchomics project folder, use:

```{bash}
# Run this from your local terminal (not while logged into the HPC)
# Replace 'username' with your UvAnetID
scp -r data username@omics-h0.science.uva.nl:/home/username/personal/data_analysis

# Then check on the HPC that the files arrived:
ll data/*.fasta
```

Notes:

- The `-r` option copies directories recursively, i.e. you want to copy everything in the directory and the directory itself (recursively)
- It’s helpful to keep two terminal windows open: one for your local computer and one for the HPC
- Run this command from inside your local `data_analysis` folder (the one you created earlier)


::: {.callout-tip title="Tip: Moving data from the HPC to our own computer" collapse="true"}

You can also move results from the HPC to your computer. For example, to copy a single genome file from Crunchomics:

```{bash}
scp username@omics-h0.science.uva.nl:/home/username/personal/data_analysis/data/LjRoot303.fasta .
```

Here, the `.` at the end means “copy to the current directory” on your local machine. If you want to copy to another location, replace `.` with a path.

:::


## Submitting jobs with SLURM

In the HPC introduction, you learned that there are two main "sections" on an HPC:

- **Login node**: where you log in and do light work, such as moving files, editing scripts, or preparing jobs.
- **Compute nodes**: where the actual analyses run. These nodes have more CPUs and memory and are managed by a scheduling system.

To use these compute nodes efficiently, we communicate with them through **SLURM**, a workload manager that decides *when*, *where*, and *how* your jobs run based on the available resources.


### `squeue`: View info about jobs in the queue

The command below shows you which jobs are currently running or waiting in the queue:

```{bash}
squeue
```

This displays all jobs submitted to the system, and might look something like this:

![](../img/squeue.png){width="90%" fig-align="left"}

Some useful columns to know:

- JOBID: a unique number assigned to each job; you can use it to check or cancel a job later.
- ST: the current state of the job (e.g., R = running, PD = pending).
- A full list of SLURM job state codes can be found [here](https://curc.readthedocs.io/en/latest/running-jobs/squeue-status-codes.html)

If you have already submitted a job, it will appear in this list. To see only your own jobs, you can add the `-u` flag followed by your username:

```{bash}
squeue -u $USER
```


### `srun`:Submitting a job interactively

Now that we’ve seen how to check the job queue, let’s learn how to actually run jobs on the compute nodes. There are two main ways to submit jobs: `srun` and with `sbatch`. We'll start with `srun`, which you typically use when:

- You want to run tasks **interactively** and see the output directly on your screen
- You are **testing or debugging** your commands before writing a job script
- You have **short jobs** (usually less than a few hours)

> **Important:** Jobs started with `srun` will stop if you disconnect from the HPC (unless you use tools like `screen` or `tmux`, which we won’t cover in this tutorial).

Let’s start with a very simple example to see how `srun` works:

```{bash}
srun --cpus-per-task=1 --mem=1G --time=00:10:00 echo "Hello world"
```

Here’s what each part means:

- `srun` → communicate that you want to run something on the compute node
- `--cpus-per-task=1` → request 1 CPU core
- `--mem=1G` → request 1 GB of memory
- `--time=00:10:00` → set a maximum runtime of 10 minutes
- `echo "Hello world"` → the actual command to run (prints text to the screen)

So the arguments after `srun` and before `echo` are where you tell SLURM what resources to allocate on the compute node.


### Running a analysis with Seqkit interactively

Next, let’s use srun for something more useful. First, we will analyze the fasta files that we have explored using [`seqkit`](https://scienceparkstudygroup.github.io/ibed-bioinformatics-page/source/core_tools/seqkit.html), a fast toolkit for inspecting FASTA/FASTQ files.

> **Tip**: Whenever you use a tool for the first time, check its help page (e.g., `seqkit -h`) to see all available options.

```{bash}
# Create a results folder
mkdir -p results/seqkit 

# Run seqkit interactively
srun --cpus-per-task=1 --mem=5G seqkit stats data/*fasta -Tao results/seqkit/16s_stats.tsv --threads 1

# View the results (press 'q' to exit less)
less -S results/seqkit/16s_stats.tsv 
```

Here:

- `seqkit stats data/*fasta` → analyzes all FASTA files in the data folder ending in fna
- `-Tao` → output in tabular format (-T), include all stats (-a), and write to file (-o)
- `--threads 1` → run on one thread (should match `--cpus-per-task=1`)
- The results are stored in results/seqkit/genome_stats.tsv

This job runs very quickly — so fast that it might not even appear in the queue when you type `squeue`.

When you open the results file, you'll see:

- The number of sequences (`num_seqs`), this should match what we saw earlier
- The average length of the sequences in bp (`avg_len`)
- Additional metrics such as the minimum, maximum or total sequence length


::: {.callout-tip title="Tip: Choosing the right amount of resources" collapse="true"}

When starting out, choosing the right amount of CPUs, memory, or runtime can be tricky.
Here are a few rules of thumb:

- Start small, a lot of tools don't need huge resources for small test runs
- Check the tool's documentation, many list recommended resource settings
- Test on a subset of your data first. It runs faster and helps you debug
- If your job fails or runs out of memory, increase the resources gradually
- Once you have a stable workflow, you can scale up

Remember: Over-requesting resources can make jobs wait longer in the queue — and can block others from running.

:::


::: {.callout-question .callout-warning collapse=false}
# Tasks 

Now it’s your turn to run an srun job interactively by exploring the FASTQ files that you have generated during your practical. Follow these steps:

**Task 1: Inspect the data**

Without uncompressing the FASTQ file, explore the content of the FASTQ files by viewing the first few lines of `barcodeX.fastq.gz` using `zcat` and `head`. 

You can get the FASTQ file as follows:

1.  If you are following the Microbial Ecology course:

    ```{bash}
    # Replace X with the barcode you used in the practical
    cp /zfs/omics/projects/education/miceco/data/practical/barcodeX.fastq.gz data
    ```

1.  If you are following this tutorial independently you can get an example file as follows:

    ```{bash}
    wget https://github.com/ndombrowski/MicEco2025/raw/refs/heads/main/data/barcode07.fastq.gz -P data/
    ```

Have a look at the data and have a look the header, sequence and quality scores.

**Tasks 2: Run seqkit interactively**

Use `srun` to analyze `barcodeX.fastq.gz` with `seqkit stats`. Request 1 CPU, 5 GB memory, and 10 minutes of runtime. Save the output in the results/seqkit/ folder.

**Tasks 3: Check the results**

Open the output file and answer the following:

- What is the average sequence length (avg_len)? Does it match the expected amplicon size from the lab?
- How many sequences are in the file (num_seqs)? Does this align with your expectations from the experiment?

> **Reminder**: Run all commands on Crunchomics in your project folder. srun jobs may finish too quickly to appear in squeue.


::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

```{bash}
# Question 1
zcat data/barcodeX.fastq.gz | head

# Question 2 
srun --cpus-per-task=1 --mem=5G seqkit stats data/barcodeX.fastq.gz  \
    -Tao results/seqkit/barcodeX_stats.tsv --threads 1

# Question 3
less -S results/seqkit/barcodeX_stats.tsv
```

Answers:

- Your average sequence length should be ~ 1400 bp and this should correspond to the amplicon size of your PCR reaction.
- num_seqs should correspond to the number of sequences in your FASTQ file and will be around 2000-10000 reads. The actual number will depend on how the sequencing went.

> **Tip**: Use `\` to split long commands across lines for readability.

:::
:::



### `sbatch`: submitting a long-running job

While `srun` is great for quick, interactive runs, a lot of analyses on the HPC are submitted with **`sbatch`**, which lets jobs run in the background even after you log out.

Use `sbatch` when:

- You have **long or resource-intensive** analyses
- You want jobs to run **unattended**
- You plan to **run multiple jobs** in sequence or parallel

#### Step 1: Create folders for organization

We'll keep our project tidy by creating dedicated folders for scripts and logs:

```{bash}
mkdir -p scripts logs
```

#### Step 2: Write a job script

Use nano (or another editor) to create a script file:

```{bash}
nano scripts/run_seqkit.sh
```

Then add the following content:

```{bash}
#!/bin/bash
#SBATCH --job-name=seqkit
#SBATCH --output=logs/seqkit_%j.out
#SBATCH --error=logs/seqkit_%j.err
#SBATCH --cpus-per-task=1
#SBATCH --mem=5G
#SBATCH --time=1:00:00

echo "Seqkit job started on:" 
date

srun --cpus-per-task=1 --mem=5G seqkit stats data/*fasta -Tao results/seqkit/genome_stats_2.tsv --threads 1

echo "Seqkit job finished on:"
date
```

Save and exit:

- Press `Ctrl+X` to exit, then `Y` to confirm that you want to safe the file, then Enter.


#### Step 3: Submit and monitor your job

```{bash}
# Submit 
sbatch scripts/run_seqkit.sh

# Check job status (may run too fast to appear)
squeue -u $USER
```


When submitted successfully, you’ll see something like `Submitted batch job 754` and new log files will appear in your logs folder:

- `seqkit_<jobID>.out` → standard output (results and messages)
- `seqkit_<jobID>.err` → error log (check this if your job fails)


#### Step 4: Understanding your job script

In the job script above:

- `#!/bin/bash`:	Runs the script with the Bash shell
- `#SBATCH --...`	Define SLURM options: resources, runtime, output names. 
    - These are the same options that you have used with `srun`
    - The `%j` variable in your log filenames automatically expands to the job ID, making it easier to keep track of multiple submissions.
- `echo / date`:	Prints status messages to track job progress
- `srun ...`: The actual analysis command to run on the compute node


::: {.callout-tip title="Tip: Debugging your first sbatch jobs" collapse=false}

If your job fails:

- Check your .err and .out files in the logs folder, these files usually tell you exactly what went wrong
- Confirm that your input files and paths exist
- Try running the main command interactively with srun first, if it works there, it will work in a script.

:::


#### Step 5: `scancel`: Cancelling a job

Sometimes you realize a job is stuck, misconfigured, or using the wrong resources. You can cancel it with:

```{bash}
scancel <jobID>
```

For example, if your job ID was 754:

```{bash}
scancel 754
```

::: {.callout-tip title="Tip: Good HPC etiquette"}

Always cancel jobs that are running incorrectly or stuck in a queue too long. This frees resources for others and avoids unnecessary load on the cluster.

:::


::: {.callout-question .callout-warning collapse=false}
# Tasks

**Task 1: Write a script to run NanoPlot with sbatch**

Now you'll write your own `sbatch` script to generate quality plots from `barcodeX.fastq.gz` using **NanoPlot**, which you installed earlier in your conda environment.

Below is a template for a script called `scripts/nanoplot.sh`.  Replace all `...` with the correct code.

In your script, make sure to:

- Give your job, output, and error files meaningful names  
- Request **2 CPUs**, **10 GB of memory**, and **30 minutes** of runtime  
- Activate your `nanoplot` conda environment  
- Create an appropriate **output directory** under `results/`  
- Use NanoPlot options to:  
    - Indicate the input is in **FASTQ** format  
    - Output a **TSV stats file**  
    - Produce bivariate dot plots
    - Save all results in the new results folder  

```{bash}
#!/bin/bash
#SBATCH --job-name=nanoplot
#SBATCH --output=logs/nanoplot_%j.out
#SBATCH --error=logs/nanoplot_%j.err
...

# Activate the conda environment
source ~/.bashrc # this is needed so that conda is initialized inside the compute node
conda activate nanoplot

# Add start date (optional)
...

# Make an output directory 
...

# Run Nanoplot
NanoPlot ...

# Add end date (optional)
...
```

**Task 2: Submit the job and inspect the results**

Then:

1. Submit your job with sbatch `scripts/nanoplot.sh`
2. Use `squeue` to check whether it’s running
3. Inspect your log files (logs/nanoplot_*.out and logs/nanoplot_*.err)
4. Use scp to copy the results folder to your own computer for viewing
5. In your NanoPlot output, open:
    - NanoStats.txt (text summary)
    - LengthvsQualityScatterPlot_dot.html (interactive plot)
6. Record:
    - How many reads were processed
    - The mean read length
    - The mean read quality

::: {.callout-answer .callout-warning collapse=true}
# Click to see the answer 

The final content of `scripts/nanoplot.sh` should look like this:

```{bash}
#!/bin/bash
#SBATCH --job-name=nanoplot
#SBATCH --output=logs/nanoplot_%j.out
#SBATCH --error=logs/nanoplot_%j.err
#SBATCH --cpus-per-task=2
#SBATCH --mem=10G
#SBATCH --time=00:30:00

# Activate the conda environment
source ~/.bashrc # this is needed so that conda is initialized inside the compute node
conda activate nanoplot

# Add start date (optional)
echo "Job started: "
date

# Make an output directory
mkdir -p results/nanoplot/raw

# Run Nanoplot
NanoPlot --fastq data/barcodeX.fastq.gz \
    -t 2 \
    --tsv_stats \
    --plots dot \
    -o results/nanoplot/raw

# Add end date (optional)
echo "Job ended: "
date
```

Submit the script and check your results:

```{bash}
# Submit the job 
sbatch scripts/nanoplot.sh

# Check job status
squeue

# Explore the log files 
ls -l logs/nanoplot* 
tail logs/nanoplot_*.out

# Copy results to your computer
# Run this command inside the data_analysis folder on my own computer 
scp -r uvanetid@omics-h0.science.uva.nl:/home/uvanetid/personal/data_analysis/results/nanoplot results

# Explore the stats file 
less -S results/nanoplot/NanoStats.txt
```

What to look for:

- Mean read length ≈ your expected amplicon size
- Mean sequence quality ≈ 12–15 (depends on sequencing kit, the higher the better)
- Plots usually show:
    - A peak around the expected read length
    - A tail of short, incomplete reads
    - Fewer, unusually long reads (often chimeras)

Use these plots to guide quality filtering to removing reads that are too short, too long, or low quality, while keeping the majority of good data.

:::
:::

