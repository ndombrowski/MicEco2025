---
execute:
  eval: false
engine: knitr

---

# Using an HPC

Now that you are more comfortable with the command line, we will start working on the UVA Crunchomics HPC. In this section, you will learn how to work with sequencing data on a High-Performance Computer.

You will go through the following steps:

- Connect to the HPC and transfer your sequencing files
- Learn how to run analyses in batch mode using job submission scripts
- Assess sequencing quality and remove low-quality reads
- Build consensus sequences from your cleaned reads
- Compare your sequences to reference databases to identify taxa
- Align reads to consensus sequences to check coverage and quality

## Connecting to the HPC

### `ssh`: Connecting to a sever

**SSH** (Secure Shell) is a network protocol that allows you to securely connect to a remote computer. On an HPC, you use SSH to log in and run commands on the server. The general command looks like this:

```{bash}
ssh -X username@server
```

Here:

- username is your account name on the HPC
- server is the address of the HPC you want to connect to
- `-X` enables X11 forwarding, which allows graphical applications from the server (like plotting or viewing images) to appear on your local machine. “Untrusted” X11 forwarding means the server can send graphical output, but it has limited access to your local machine

For Crunchomics at UVA, you would use:

```{bash}
ssh -X uvanetid@omics-h0.science.uva.nl
```

::: callout-important

**Important tips for connecting:**

- If you want to log into Crunchomics while working from UvA use the eduroam network, not the open Amsterdam Science Park network.  
- If you are connecting from outside UVA, you must be on the VPN. Contact ICT if you have not set it up or encounter issues.
- Always double-check your username and server address to avoid login errors.
:::


### Crunchomics: Preparing your account

If this is your first time using Crunchomics, you need to run a small Bash script to set up your account. This script will:

-   Add `/zfs/omics/software/bin` to your PATH. This basically allows Bash to locate and use the system-wide installed software available on Crunchomic
-   Set up a Python 3 environment with some useful Python packages pre-installed
-   Create a link to your 500 GB personal directory inside your home directory, giving you plenty of space to store data and results

To set up your account, run the following command:

```{bash}
/zfs/omics/software/script/omics_install_script
```

Once the script has completed, you can start organizing your project files. We recommend creating a dedicated project folder inside your personal directory, which provides more storage than the home directory:

```{bash}
# Go into the personal folder
# You want generally generate data here, since the personal folder comes with more space than the home directory
cd personal 

# Make and go into a project folder 
mkdir data_analysis
cd data_analysis 

# Ensure that we are in the right folder 
pwd
```

You now have a clean workspace for all your analyses on Crunchomics. As in your local machine, it’s good practice to keep your raw data, results, and scripts organized in separate folders.


### `scp`: Transferring data from/to a server

To start analysing data on Crunchomics, we first need to copy our local `data` folder (from your personal computer) to the HPC. We can do this with the `scp `(Secure Copy Protocol) command, which securely transfers files or directories between your computer and a remote server.

The basic syntax is:

`scp [options] SOURCE DESTINATION`

When transferring data you must **run the command from your own computer**, not from inside the HPC login session.

To copy the entire data folder into your Crunchomics project folder, use:

```{bash}
# Run this from your local terminal (not while logged into the HPC)
# Replace 'username' with your UvAnetID
scp -r data username@omics-h0.science.uva.nl:/home/username/personal/data_analysis

# Then check on the HPC that the files arrived:
ll data/*.fna
```

Notes:

- The -r option copies directories recursively, i.e. you want to copy everything in the directory(recursively)
- It’s helpful to keep two terminal windows open: one for your local computer and one for the HPC
- Run this command from inside your local `data_analysis` folder (the one you created earlier)


::: {.callout-tip title="Tip: Moving data from the HPC to our own computer" collapse="true"}

You can also move results back to your computer. For example, to copy a single genome file from Crunchomics:

```{bash}
scp username@omics-h0.science.uva.nl:/home/username/personal/data_analysis/data/LjRoot303.fna .
```

Here, the `.` at the end means “copy to the current directory” on your local machine. If you want to copy to another location, replace `.` with a path.

:::


## Submitting jobs with SLURM

### `squeue`: View info about jobs in the queue

The following command gives us some information about how busy the HPC is:

```{bash}
squeue
```

After running this, you can see all jobs scheduled on the HPC, which might look something like this:

![](../img/squeue.png){width="50%" fig-align="left"}

-   JOBID: every job gets a number and you can manipulate jobs via this number
-   ST: Job state codes that describe the current state of the job. The full list of abbreviations can be found [here](https://curc.readthedocs.io/en/latest/running-jobs/squeue-status-codes.html)

If we would have submitted a job, we also should see the job listed there.


### `srun`:Submitting a job interactively

`srun` is used when you want to run tasks interactively or want to have more control over the execution of a job. You directly issue `srun` commands in the terminal and you at the same time are able to specify the tasks to be executed and their resource requirements. For example, you might want to run softwareX and request that this job requires 10 CPUs and 5 GB of memory.

Use `srun` when:

-   You want to run tasks interactively and need immediate feedback printed to the screen
-   You are testing or debugging your commands before incorporating them into a script
-   You need more control over the execution of tasks
-   Typically, you use srun for smaller jobs that do not run for too long, i.e. a few hours

Let's submit a very simple example for which we would not even need to submit a job, but just to get you started.

```{bash}
srun echo "Hello interactively"
```

You should see the output of echo printed to the screen and if you would run `squeue` you won't even see your job since everything ran so fast. But congrats, you communicated the first time with the compute node.

Now assume you want to run a more complex interactive task with `srun` that might run longer and benefit from using more CPUs. In this case you need to specify the resources your job needs by adding flags, i.e. some of which you see here:

```{bash}
srun --cpus-per-task=1 --mem=1G --time=00:10:00 echo "Hello interactively"
```

The different flags mean the following:

-   `--cpus-per-task=1`: Specifies the number of CPUs per task. Adjust this based on the computational requirements of your task
-   `--mem=1G`: Sets the memory requirement for the task. Modify this based on your task's memory needs
-   `--time=00:10:00` Sets the time limit to 10 minutes
-   `echo "Hello interactively`: The actual command you want to run interactively

Other flags:

-   `--nodes=1`: Specifies the number of nodes. In this case, it's set to 1 and tells slurm that we want to use a full node. Only use this if you make use of all resources on that node, otherwise omit. In our case this definitely is over-kill to request a full node with 64 CPUs to print a single line of text to the screen.
-   `--ntasks=1`: Defines the number of tasks to run. Here, we would set it to 1 since we want to use echo once


### Run FastQC with srun <-- adjust for long-read

[FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) is a quality control tool for high throughput sequence data that is already installed on Crunchomics. This will be the actual software that we now want to run to look more closely at the quality of the sequencing data that we just uploaded to Crunchomics.

By the way: Whenever running a software for a first time, it is useful to check the manual, for example with `fastqc -h.`

Let's start by setting up a clean folder structure to keep our files organized. I start with making a folder in which I want to store all results generated in this analysis and by using the `-p` argument I generate a fastqc folder inside the results folder at the same time. Then, we can submit our job using `srun`.

```{bash}
mkdir -p results/fastqc 

srun --cpus-per-task=1 --mem=5G fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1

ll results/fastqc/*
```

Since we work with little data this will run extremely fast despite only using 1 CPU (or thread, in our case these two words can be used interchangeably). However, if you would be logged into Crunchomics via a second window and run `squeue` you should see that your job is actively running (in the example below, the job we submitted is named after the software and got the jobid 746):

![](../img/squeue2.png){width="50%" fig-align="left"}

Additionally, after the run is completed, you should see that several HTML files were generated in our fastqc folder.


::: {.callout-tip title="Tip: Choosing the right amount of resources" collapse="true"}

When you're just starting, deciding on the right resources to request for your computational job can be a bit challenging. The resource requirements can vary significantly based on the specific tool or workflow you are using. Here are some general guidelines to help you make informed choices:

-   Use default settings: Begin by working with the default settings provided by the HPC cluster or recommended by the tool itself. These are often set to provide a balanced resource allocation for a wide range of tasks
-   Check the software documentation: Consult the documentation of the software or tool you are using. Many tools provide recommendations for resource allocation based on the nature of the computation.
-   Test with small datasets: For initial testing and debugging especially when working with large datasets, consider working with a smaller subset of your data. This allows for faster job turnaround times, helping you identify and resolve issues more efficiently.
-   Monitor the resources usage:
    -   Use `sacct` to check what resources a finished job has used and see whether you can optimize a run if you plan to run similar jobs over and over again. An example command would be `sacct -j 419847 --format=User,JobID,Jobname,state,start,end,elapsed,MaxRss,ncpus.` In the report, look for columns like MaxRSS (maximum resident set size) to check if the amount of memory allocated (--mem) was appropriate.
    -   Ensuer that the job used the resources you requested. For instance, if you would have used `--cpus-per-task=4 --mem=4G`, you would expect to use a total of 16 GB of memory (4 CPUs \* 4 GB). You can verify this with `sacct` to ensure your job's resource requirements align with its actual usage.
-   Fine-Tuning Resource Requests: If you encounter performance issues or your jobs are not completing successfully, consider iteratively adjusting resource requests. This might involve increasing or decreasing the number of CPUs, memory allocation, or other relevant parameters.

:::


### `sbatch`: submitting a long-running job

`sbatch` is your go-to command when you have a script (i.e. a batch script) that needs to be executed without direct user interaction.

Use `sbatch` when:

-   You have long-running or resource-intensive tasks
-   You want to submit jobs that can run independently without your immediate supervision
-   You want to submit multiple jobs at once

To run a job script, you:

-   create a script that contains all the commands and configurations needed for your job
-   use sbatch to submit this script to the Slurm scheduler, and it takes care of the rest.

Let's start with generating some new folders to keep our project folder organized:

```{bash}
mkdir scripts 
mkdir logs
```

To get started, assume we have created a script in the scripts folder named `run_fastqc.sh` with the content that is shown below. Notice, how in this script I added some additional `echo` commands? I just use these to print some information about the progress which could be printed to a log file but if you have several commands that should be executed after each other that is how you would do it.

```{bash}
#!/bin/bash
#SBATCH --cpus-per-task=1
#SBATCH --mem=5G      
#SBATCH --time=1:00:00

echo "Start fastqc"

fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1

echo "fastqc finished"
```

In the job script above:

-   `#!/bin/bash` . This so-called Shebang line tells the shell to interpret and run the Slurm script using the bash shell. This line should always be added at the very top of your SBATCH/Slurm script.
-   The lines that follow and start with `#` are the lines in which we define the amount of resources required for our job to run. In our case, we request 1 CPU, 5G of memory and 1 hour time limit.
-   If your code needs any dependencies, such as conda environments, you would add these dependencies here. We do not need this for our example here, but you might need to add something like this `conda activate my_env` if you have installed your own software. We will touch upon conda environments and installing software a bit later.
-   The lines afterwards are the actually commands that we want to run on the compute nodes, We also call this the job steps.

To prepare the script and run it:

-   Run `nano scripts/run_fastqc.sh` to generate an empty jobscript file
-   Add the code from above into the file we just opened
-   Press `ctrl+x` to exit nano
-   Type `Y` when prompted if the changes should be saved
-   Confirm that the file name is good by pressing enter

Afterwards, you can submit `run_fastqc.sh` as follows:

```{bash}
#submit job: 754
sbatch scripts/run_fastqc.sh

#check if job is running correctly
squeue
```

After running this, you should see that the job was submitted and something like this printed to the screen `Submitted batch job 754`. You will also see that a new file is generated that will look something like this `slurm-754.out`.

When you submit a batch job using sbatch, Slurm redirects the standard output and standard error messages, which you have seen printed to the screen when you used `srun`, to a file named in the format slurm-JOBID.out, where JOBID is the unique identifier assigned to your job.

This file is useful as it:

-   Captures the output of our batch scripts and stores them in a file
-   Can be used for debugging, since if something goes wrong with your job, examining the contents of this file can provide valuable insights into the issue. Error messages, warnings, or unexpected outputs are often recorded here

Feel free to explore the content of the log file, do you see how the echo commands are used as well?

::: {.callout-tip title="Tip: sbatch and better log files" collapse="true"}
We have seen that by default `sbatch` redirects the standard output and error to our working directory and that it decides itself how to name the files. Since file organization is very important especially if you generate lots of files, you find below an example to:

-   Store the standard output and error in two separate files
-   Redirect the output into another folder, the logs folder
-   In the code below, the `%j` is replaced with the job allocation number once the log files are generated

```{bash}
#!/bin/bash
#SBATCH --job-name=our_fastqc_job
#SBATCH --output=logs/fastqc_%j.out
#SBATCH --error=logs/fastqc_%j.err
#SBATCH --cpus-per-task=1
#SBATCH --mem=5G
#SBATCH --time=1:00:00

echo "Start fastqc"

fastqc data/seq_project/*/*gz -o results/fastqc  --threads 1

echo "fastqc finished"
```
:::


## Installing software

There might be cases where the software you are interested in is not installed on the HPC you are working with (or on your own computer).

In the majority of cases, you should be able to install software by using a package management system, such as conda or mamba. These systems allow you to find and install packages in their own environment without administrator privileges. Let's have a look at a very brief example:

### Install conda/mamba

A lot of systems already come with conda/mamba installed, however, if possible we recommend working with mamba instead of conda. mamba is a replacement and uses the same commands and configuration options as conda, however, it tends to be much faster. A useful thing is that if you find documentation for conda then you can swap almost all commands between conda & mamba.

If you have conda installed and do not want to install anything else, that is fine. Just replace all instances of mamba with conda below.

This command should work in most cases to setup conda together with mamba:

```{bash}
curl -L -O "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
bash Miniforge3-$(uname)-$(uname -m).sh
```

When running the bash command, you get asked a few questions:

1. Read the license and use arrow down to scroll down. Don't scroll too fast, so that you see the next question
2. Decide where conda gets installed. You can say yes to the default location in your home. But don't forget that for Crunchomics your home only has 25G of space. You could also install the miniforge/mambaforge folder in your personal folder instead.
3. Say yes, when you get asked whether conda should be initialized during start up
4. Restart the shell (exit the terminal and use ssh to log back in) for the changes to take effect
5. Check if conda is accessible by running `conda -h`



### Setting up an environment

Let's assume we want to install seqkit, a tool that allows us to calculate some statistics for sequencing data such as the number of sequences or average sequence length per sample.

We can install seqkit into a separate environment, which we can give a descriptive name, as follows:

```{bash}
#check if the tool is installed (should return "command not" found if the software is not installed)
seqkit -h

#create an empty environment and name it seqkit and we add the version number to the name
#this basically sets up seqkit separate from our default working environment
#this is useful whenever software require complicated dependencies allowing us to have a separate install away from software that could conflict with each other
mamba create -n seqkit_2.6.1

#install seqkit, into the environment we just created
mamba install -n seqkit_2.6.1 -c bioconda seqkit=2.6.1

#to run seqkit, we need activate the environment first
mamba activate seqkit_2.6.1

#check if tool is installed, 
#if installed properly this should return some detailed information on how to run seqkit
seqkit -h

#run the tool via srun
mkdir results/seqkit
srun --cpus-per-task 2 --mem=4G seqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2
less -S results/seqkit/seqkit_stats.tsv 

#close the environment
conda deactivate
```

When installing seqkit, we:

-   specify the exact version we want to download with `=2.6.1`. We could also install the newest version that conda/mamba can find by running `mamba install -n seqkit -c bioconda seqkit`.
-   specify that we want to look for seqkit in the bioconda channel with the option `-c`. Channels are the locations where packages are stored. They serve as the base for hosting and managing packages. Conda packages are downloaded from remote channels, which are URLs to directories containing conda packages. If you are unable to find a package it might be that you need to specify a channel.

Forgot what conda environments you installed in the past? You can run `conda env list` to generate a list of all existing environments.

Unsure if a software can be installed with conda? Google conda together with the software name, which should lead you do a conda web page. This page should inform you whether you need to add a specific channel to install the software as well as the version numbers available.

A full set of mamba/conda commands can be found [here](https://docs.conda.io/projects/conda/en/latest/commands/index.html.).

::: {.callout-caution collapse="false" title="Exercise"}
1.  Download and view the file `results/seqkit/seqkit_stats.tsv` on your own computer
2.  Run the seqkit again but this time submit the job via a sbatch script instead of using srun. Notice, that you need to tell SLURM how it can activate the conda environment that has seqkit installed. You might need to google how to do that, since this requires some extra line of code that we have not covered yet but see this as a good exercise for how to handle error messages that you see in the log files for your own analyses

<details>

<summary>Click me to see an answer</summary>

```{bash}
#question 1
scp username@omics-h0.science.uva.nl:/home/ndombro/personal/projectX/results/seqkit/seqkit_stats.tsv .

#question 2 
sbatch scripts/seqkit.sh
```

Content of `scripts/seqkit.sh`:

```{bash}
#!/bin/bash
#SBATCH --job-name=seqkit_job
#SBATCH --output=logs/seqkit_%j.out
#SBATCH --error=logs/seqkit_%j.err
#SBATCH --cpus-per-task=2
#SBATCH --mem=5G

#activate dependencies
source ~/.bashrc
mamba activate seqkit_2.6.1

#run seqkit
echo "Start seqkit"

seqkit stats -a -To results/seqkit/seqkit_stats.tsv data/seq_project/*/*.gz --threads 2

echo "seqkit finished"
```

In the script above, we see that we need to add two lines of code to activate the seqkit conda environment:

```         
source ~/.bashrc
mamba activate seqkit_2.6.1
```

When you run a script or a command, it operates in its own environment. The source command is like telling the script to look into another file, in this case, `~/.bashrc`, and execute the commands in that file as if they were written directly into the script.

Here, `source ~/.bashrc` is telling the script to execute the commands in the `~/.bashrc` file. This is typically done to set up environment variables, paths, and activate any software or tools that are required for the script to run successfully. In our case this tells Slurm where we have installed conda and thus enables Slurm to use conda itself.

This allows slurm to, after executing `source ~/.bashrc`, activates a Conda environment using `mamba activate seqkit_2.6.1`. This ensures that the SeqKit tool and its dependencies are available and properly configured for use in the subsequent part of the script.

</details>
:::