---
author: Nina Dombrowski
date-modified: last-modified
date: 12/10/2025

format:
  html:
    toc: true
    embed-resources: true

execute:
  eval: false

engine: knitr
---

# Workflow to analyse MicEco 2025 data

## Project description

Different combinations of microbial strains were mixed to test their interactions in liquid culture. After growth, DNA was isolated, the full-length 16S rRNA gene amplified and sequenced using Nanopore sequencing. The goal of this workflow is to:

- Assess the quality of sequence reads
- Perform quality cleaning
- Map the filtered read to a reference 16S database 
- Generate a count table

Sample info:

- 24+24 samples were generated from different combinations of human and soil-related bacterial strains.
- Note: there might be a barcode mix up (see mail from Peter from 09122024)
- Compared to the test data, these samples still contain the barcode. To align with the practical, these will be removed with porechop and these files will be provided to the students

## Dependencies

Analyses were executed on AlmaLinux 8.10 (x86_64), that info was extracted via `uname -a` and `cat /etc/os-release`. The used software is:

- sed (GNU sed) 4.5
- seqkit v2.7.0
- nanoplot v1.42.0
- chopper v0.11.0
- minimap2 2.22-r1105-dirty
- Custom scripts that are available [via github](https://github.com/ndombrowski/MicEco2025/tree/refs/heads/main/scripts)
    - combine_nanoplot_html.py
    - paf_to_matrix.py
    - Dependencies: Python 3.10.18, pandas v2.3.0, matplotlib v3.9.2 (determined via `pip show package`)


## Project setup

Data was analysed on the Uva Crunchomics HPC:

```{bash}
# Define project directory
wdir="/home/ndombro/personal/miceco2025_da"
cd $wdir
```



## Prepare input files

We work with the following data:

- Sequencing data was provided as zipped folder called `Microbiocourse2025.zip` by Peter Kuperus December 10, 2025 via SurfDrive and stored in the data folder 

To analyse the data, individual fastq files per sample were first combined into a single file.

```{bash}
mkdir -p data/combined scripts results filelists logs

# Prepare the raw data
unzip data/Microbio2025.zip -d data

# Combine barcodes per study type
for barcode_dir in data/Microbio2025/Soil/fastq_pass/barcode*/; do 
    barcode=$(basename "$barcode_dir")
    short=${barcode/barcode/bc}
    echo "Combine data for $barcode"
    cat "$barcode_dir"/*fastq.gz > data/combined/s_"$short".fastq.gz
done

for barcode_dir in data/Microbio2025/Human/fastq_pass/barcode*/; do 
    barcode=$(basename "$barcode_dir")
    short=${barcode/barcode/bc}
    echo "Combine data for $barcode"
    cat "$barcode_dir"/*fastq.gz > data/combined/h_"$short".fastq.gz
done

ll data/combined/* | wc -l
ll -h data/combined

# Cleanup 
rm -r data/Microbio2025
```

Notes:

- We work with 39 files. We don't have the following barcodes:
  - human: bc07, bc13, bc15, bc21, bc22, bc24
  - soil: bc10, bc12, bc20
- Some barcodes have smaller file sizes
  - h_bc05, s_bc04, s_bc18, s_bc24


## Assess read quality 

```{bash}
mkdir -p results/seqkit results/nanoplot/raw 

# Run seqkit and summarize data
srun --cpus-per-task 10 --mem=50G seqkit stats \
    -a -To results/seqkit/seqkit_raw.tsv data/combined/*fastq.gz --threads 10

awk 'NR>1 {sum+=$4} END {print sum}' results/seqkit/seqkit_raw.tsv
awk 'NR>1 {sum+=$4; count++} END {print sum/count}' results/seqkit/seqkit_raw.tsv
awk 'NR==2 {min=$4; max=$4} NR>1 {if ($4<min) min=$4; if ($4>max) max=$4} END {print min, max}' results/seqkit/seqkit_raw.tsv 

awk 'NR>1 {sum+=$7; count++} END {print sum/count}' results/seqkit/seqkit_raw.tsv
awk 'NR>1 {sum+=$10; count++} END {print sum/count}' results/seqkit/seqkit_raw.tsv

awk 'NR>1 {sum+=$16; count++} END {print sum/count}' results/seqkit/seqkit_raw.tsv

# Flag samples with too few reads and remove
awk 'NR>1 && $4 < 100 {print $1}' results/seqkit/seqkit_raw.tsv > filelists/low_count_samples

for i in `cat filelists/low_count_samples`; do
    rm $i 
done

ll data/combined/* | wc -l

# Run Nanoplot
conda activate nanoplot_1.42.0

for file in data/combined/*fastq.gz; do
    barcode=$(basename $file .fastq.gz)
    echo "Start analysis for $barcode"

    srun --cpus-per-task 6 --mem=10G NanoPlot \
        --fastq ${file} \
        -t 6 \
        --tsv_stats \
        --plots dot \
        -o results/nanoplot/raw/${barcode}
done

conda deactivate

## Combine Nanoplot HTMLs
## Was edited to run with s_bc and h_bc and run twice
## barcode_folders = [f for f in os.listdir(base_path) if f.startswith("s_bc")]
python scripts/combine_nanoplot_html.py --base_path results/nanoplot/raw \
    --output_html results/nanoplot/raw/raw_nanoplot_s.html

python scripts/combine_nanoplot_html.py --base_path results/nanoplot/raw \
    --output_html results/nanoplot/raw/raw_nanoplot_h.html
```

Notes:

- Total number of reads: 952,522 (min:1, avg: 24423, max:39075)
- Samples with few reads 
  - h_bc05, s_bc04, s_bc18, s_bc24
- The average read length and median (Q2) is 1546 and 1584 bp
- The phred score is around 15
- Notes from the combined Nanoplots:
    - Most reads around 1600 bp and longer than expected
    - The length is due to the barcode still being part of the read


### Remove barcodes with porechop 

```{bash}
mkdir results/porechop results/nanoplot/porechop 

conda activate porechop_0.2.4

for file in data/combined/*fastq.gz; do
    barcode=$(basename $file .fastq.gz)
    echo "Start analysis for $barcode"

    srun --cpus-per-task 10 --mem=20G porechop \
        --input $file \
        --output results/porechop/"$barcode".fastq.gz \
        --threads 10 \
        --discard_middle
done 

conda deactivate

# Run seqkit 
srun --cpus-per-task 10 --mem=50G seqkit stats \
    -a -To results/seqkit/seqkit_porechop.tsv results/porechop/*fastq.gz --threads 10

awk 'NR>1 {sum+=$4} END {print sum}' results/seqkit/seqkit_porechop.tsv
awk 'NR>1 {sum+=$4; count++} END {print sum/count}' results/seqkit/seqkit_porechop.tsv
awk 'NR==2 {min=$4; max=$4} NR>1 {if ($4<min) min=$4; if ($4>max) max=$4} END {print min, max}' results/seqkit/seqkit_porechop.tsv 

awk 'NR>1 {sum+=$7; count++} END {print sum/count}' results/seqkit/seqkit_porechop.tsv
awk 'NR>1 {sum+=$10; count++} END {print sum/count}' results/seqkit/seqkit_porechop.tsv

awk 'NR>1 {sum+=$16; count++} END {print sum/count}' results/seqkit/seqkit_porechop.tsv

# Run Nanoplot 
conda activate nanoplot_1.42.0

for file in results/porechop/*fastq.gz; do
    barcode=$(basename $file .fastq.gz)
    echo "Start analysis for $barcode"

    srun --cpus-per-task 6 --mem=10G NanoPlot \
        --fastq ${file} \
        -t 6 \
        --tsv_stats \
        --plots dot \
        -o results/nanoplot/porechop/${barcode}
done

conda deactivate

python scripts/combine_nanoplot_html.py --base_path results/nanoplot/porechop \
    --output_html results/nanoplot/porechop/porechop_nanoplot_s.html

python scripts/combine_nanoplot_html.py --base_path results/nanoplot/porechop \
    --output_html results/nanoplot/porechop/porechop_nanoplot_h.html
```

Notes:

- Total number of reads: 947,147 (min:1008, avg: 27,061, max:38,848)
- Samples with fewest reads 
  - s_bc02 (1008), h_bc01 (6795)
- The average read length and median (Q2) is 1496 and 1496 bp
- The phred score is around 18
- Notes from the combined Nanoplots:
    - Most reads around 1500 bp
    - There is sometimes a smaller cloud of longer but lower quality reads
      -  --> will try to get a threshold to discard these since the quality is consistently lower across all plots
    - Few reads that are longer, more reads that are a bit shorter
    - Phread score evenly distributed around Phred score 10-35

Settings I chose for cleaning based on the NanoPlots:

- Phred 12
- Min length 1400
- Max length 1540



### Quality reads with chopper

```{bash}
# Run chopper
conda activate chopper_0.11.0

for file in results/porechop/*fastq.gz; do
    barcode=$(basename "$file" .fastq.gz)
    outdir="results/chopper/" 
    mkdir -p "$outdir"

    echo "Start analysis for ${barcode}"

    srun --cpus-per-task 10 --mem=50G chopper -i "$file" \
        -q 12 \
        --trim-approach trim-by-quality --cutoff 12 \
        -l 1400 --maxlength 1540 \
        --threads 10 | gzip > "${outdir}/${barcode}.fastq.gz"
done

conda deactivate
```

Notes from this analysis (table generated by using the standard output from chopper and converting it to markdown via chatgpt):

| barcode | input_reads | output_reads | perc_kept |
| ------- | ----------- | ------------ | --------- |
| h_bc01  | 6795        | 6009         | 88.43     |
| h_bc02  | 29468       | 24548        | 83.3      |
| h_bc03  | 15181       | 13159        | 86.68     |
| h_bc04  | 30927       | 26614        | 86.05     |
| h_bc06  | 8675        | 7703         | 88.8      |
| h_bc08  | 28093       | 23820        | 84.79     |
| h_bc09  | 23210       | 20325        | 87.57     |
| h_bc10  | 30476       | 25789        | 84.62     |
| h_bc11  | 18829       | 16561        | 87.95     |
| h_bc12  | 29962       | 23065        | 76.98     |
| h_bc14  | 21650       | 18922        | 87.4      |
| h_bc16  | 25502       | 22725        | 89.11     |
| h_bc17  | 25185       | 22372        | 88.83     |
| h_bc18  | 24704       | 19636        | 79.49     |
| h_bc19  | 21506       | 19087        | 88.75     |
| h_bc20  | 32735       | 28077        | 85.77     |
| h_bc23  | 17229       | 15369        | 89.2      |
| s_bc01  | 27972       | 25175        | 90        |
| s_bc02  | 1008        | 907          | 89.98     |
| s_bc03  | 31668       | 27707        | 87.49     |
| s_bc05  | 29156       | 25974        | 89.09     |
| s_bc06  | 31612       | 27935        | 88.37     |
| s_bc07  | 38186       | 33722        | 88.31     |
| s_bc08  | 25214       | 22225        | 88.15     |
| s_bc09  | 33018       | 29429        | 89.13     |
| s_bc11  | 29620       | 26694        | 90.12     |
| s_bc13  | 31721       | 27992        | 88.24     |
| s_bc14  | 37025       | 30473        | 82.3      |
| s_bc15  | 33519       | 30123        | 89.87     |
| s_bc16  | 38848       | 34147        | 87.9      |
| s_bc17  | 30428       | 27673        | 90.95     |
| s_bc19  | 30741       | 27209        | 88.51     |
| s_bc21  | 37846       | 31969        | 84.47     |
| s_bc22  | 37565       | 32774        | 87.25     |
| s_bc23  | 31873       | 29181        | 91.55     

: {.striped .responsive .sm}

Will keep all samples for downstream analyses.


### Check quality of the cleaned data

```{bash}
# Get overaching summary with seqkit and summarize with awk
srun --cpus-per-task=10 --mem=5G seqkit stats results/chopper/*fastq.gz  \
    -Tao results/seqkit/seqkit_chopper.tsv --threads 10

# Run seqkit 
awk 'NR>1 {sum+=$4} END {print sum}' results/seqkit/seqkit_chopper.tsv
awk 'NR>1 {sum+=$4; count++} END {print sum/count}' results/seqkit/seqkit_chopper.tsv
awk 'NR==2 {min=$4; max=$4} NR>1 {if ($4<min) min=$4; if ($4>max) max=$4} END {print min, max}' results/seqkit/seqkit_chopper.tsv 

awk -F '\t'  'NR>1 {sum+=$10; count++} END {print sum/count}' results/seqkit/seqkit_chopper.tsv

awk 'NR>1 {sum+=$16; count++} END {print sum/count}' results/seqkit/seqkit_chopper.tsv

# Run Nanoplot 
conda activate nanoplot_1.42.0

for file in results/chopper/*fastq.gz; do
    barcode=$(basename $file .fastq.gz)
    echo "Start analysis for $barcode"

    srun --cpus-per-task 6 --mem=10G NanoPlot \
        --fastq ${file} \
        -t 6 \
        --tsv_stats \
        --plots dot \
        -o results/nanoplot/chopper/${barcode}
done

conda deactivate

python scripts/combine_nanoplot_html.py --base_path results/nanoplot/chopper \
    --output_html results/nanoplot/chopper/chopper_nanoplot_s.html

python scripts/combine_nanoplot_html.py --base_path results/nanoplot/chopper \
    --output_html results/nanoplot/chopper/chopper_nanoplot_h.html
```

- We work with a total of 825,090 reads
- Total number of reads is unevenly distributed, from 907-34147 reads/sample (mean 23,574)
- The median read length (Q2) is around 1496 bp and the second low quality cloud in the nanoplots is mostly gone
- Note that some samples have a small dual-peak (1470 and 1500), i.e. s_bc03, s_bc05, s_bc06, but there is no difference in sequence quality, so that could be due natural sequence length variation
- The phred score is around 19


### Read mapping 

The reference sequences were generated as is discussed in `source/analysis_test_data.qmd`. Note that the file `amplicons_nr_final.fasta` that is mentioned in this workflow was named `amplicons_nr.fasta` but both are the same files.

```{bash}
mkdir results/mapping_counts results/tables

# Get reference database and accession to genus mapping file
wget https://raw.githubusercontent.com/ndombrowski/MicEco2025/refs/heads/main/data/amplicons_nr.fasta -P data
wget https://raw.githubusercontent.com/ndombrowski/MicEco2025/refs/heads/main/data/accession_to_genus.tsv -P data

# Run minimap 
for file in results/chopper/*.fastq.gz; do
    ref_db="data/amplicons_nr.fasta"
    barcode=$(basename $file .fastq.gz)
    
    echo "----------------------------------------------------------------"
    echo "Mapping $barcode to reference database..."
    echo "----------------------------------------------------------------"

    # Map reads to reference and save PAF
    srun --cpus-per-task 2 --mem=20G minimap2 \
        -cx map-ont -t 2 ${ref_db} ${file} \
        > results/mapping_counts/${barcode}.paf
done

# Generate count table 
python scripts/paf_to_matrix.py \
    -i results/mapping_counts/ \
    -o results/tables/ \
    -t data/accession_to_genus.tsv \
    -s results/seqkit/seqkit_chopper.tsv 

# Test with single sample 
mkdir test 
cp results/mapping_counts/s_bc05.paf test/

python scripts/paf_to_matrix.py \
    -i test/ \
    -o test/ \
    -t data/accession_to_genus.tsv \
    -s results/seqkit/seqkit_chopper.tsv 

# Some more exploration of the paf file in py showed low sequence identity in blastn showed hits against Blautia 
# In total we have 56,966 out of 825,090 reads that are unassigned (ca 6%)
# To make it easier to explore this further, these hits were extracted into a fastq/fasta file
wc -l results/tables/bad_hit_read_ids.txt

seqtk subseq <(cat results/chopper/*gz) results/tables/bad_hit_read_ids.txt > results/mapping_counts/bad_hits.fastq

seqtk seq -a -l 100 results/mapping_counts//bad_hits.fastq > results/mapping_counts//bad_hits.fasta

awk '
    /^>/ {
        split($0, f, " ");
        readid = substr(f[1],2);
        bcid = "";
        for (i=2; i<=NF; i++) {
            if (f[i] ~ /^barcode=/) {
                split(f[i], b, "=");
                bcid = b[2];
            }
        }
        print ">" readid "-" bcid;
        next
    }
    { print }' \
 results/mapping_counts//bad_hits.fasta > results/mapping_counts//bad_hits2.fasta

grep -c ">" results/mapping_counts/bad_hits2.fasta

# Blastn search against a random nr of sequences returns hits to
# Blautia VII, Stapylococcus, Anaerostipes, Limosilactobacillus 
# Blautia IX Anaerostipes (bc09)
# Blautia V, Limosilactobacillus III, Anaerostipes, Bacteroidetes (bc11)
seqtk sample results/mapping_counts/bad_hits2.fasta 10 |  seqtk seq -l 100  > random10.fasta

awk '/^>.*barcode09/{p=1; print; next} p{if (/^>/){p=0} else print}' results/mapping_counts/bad_hits2.fasta \
  | seqtk sample - 10 \
  | seqtk seq -l 100 > random10_barcode09.fasta

awk '/^>.*barcode11/{p=1; print; next} p{if (/^>/){p=0} else print}' results/mapping_counts/bad_hits2.fasta \
  | seqtk sample - 10 \
  | seqtk seq -l 100 > random10_barcode11.fasta
```

Notes: 

- In some samples a lot of reads were filtered due to the quality thresholds, i.e. h_bc19 --> 
  - I found GCF_000714595 in there which had more than one rrn, so that could result in reads mapping more to the mismatches to the second copy 
- I investigated identity_hist.png (output of paf_to_matrix) and that showed a clear peak around 0.7-0.8 indicating some lower identity mapping results in the paf files. These will be removed during the filtering step considering the 0.9 threshold used by default by the script
- Notice that there is a small coverage peak at ~0.97 (possibly the second GCF_000714595 rrna sequence?)
- During the run with the default thresholds I noticed that in the human samples there was a large fraction of unassigned reads
  - these are not due to unmapped reads (tested by extracting these)
  - these are due to reads with lower mapping/identity that were lost in the filtering process and affected some taxa more than others:
    - Streptococcus, Lactobacillus



```
Per-sample filtering summary:
        before   after  removed  removed_frac_perc
sample
h_bc01   14044    6425     7619          54.250926
h_bc02   45221   39610     5611          12.407952
h_bc03   32383    7915    24468          75.558163
h_bc04   27122   26549      573           2.112676
h_bc06   14610   12347     2263          15.489391
h_bc08   43854   38784     5070          11.561089
h_bc09   53161   18645    34516          64.927296
h_bc10   48240   42296     5944          12.321725
h_bc11   44836    8251    36585          81.597377
h_bc12   27517   25874     1643           5.970854
h_bc14   38320   27346    10974          28.637787
h_bc16   42358   37363     4995          11.792341
h_bc17   58831   15518    43313          73.622750
h_bc18   35304   30739     4565          12.930546
h_bc19   53345    8901    44444          83.314275
h_bc20   28432   28141      291           1.023495
h_bc23   15477   15334      143           0.923952
s_bc01   35268   34886      382           1.083135
s_bc02    2609    2564       45           1.724799
s_bc03   41693   37460     4233          10.152783
s_bc05   49539   48712      827           1.669392
s_bc06   47569   46542     1027           2.158969
s_bc07  101071   99105     1966           1.945167
s_bc08   66445   65142     1303           1.961020
s_bc09   45118   44523      595           1.318764
s_bc11   36428   33385     3043           8.353464
s_bc13   44303   43632      671           1.514570
s_bc14   53944   52637     1307           2.422883
s_bc15   90250   88482     1768           1.959003
s_bc16  102220  100161     2059           2.014283
s_bc17   42534   42083      451           1.060328
s_bc19   38752   35183     3569           9.209847
s_bc21   64051   54349     9702          15.147304
s_bc22   48454   43754     4700           9.699922
s_bc23   87391   69881    17510          20.036388
```



## Analyse PAF files in more detail

```{python}
import pandas as pd
from pathlib import Path
import argparse
import sys
import matplotlib.pyplot as plt


def read_paf_with_AS(paf_file):
    """
    Reads a PAF file, keeping the first 12 standard columns and the AS:i: optional field.
    Returns a DataFrame with AS as a separate column.
    """
    rows = []
    with open(paf_file) as f:
        for line in f:
            if line.startswith("#") or not line.strip():
                continue
            parts = line.strip().split("\t")
            # First 12 standard columns
            std_cols = parts[:12]
            # Look for AS:i:<value> in optional fields
            AS = None
            for opt in parts[12:]:
                if opt.startswith("AS:i:"):
                    AS = int(opt.split(":")[-1])
                    break
            # Append row (first 12 + AS)
            rows.append(std_cols + [AS])

   # Create DataFrame
    colnames = [
        "qname",
        "qlen",
        "qstart",
        "qend",
        "strand",
        "tname",
        "tlen",
        "tstart",
        "tend",
        "nmatch",
        "alen",
        "mapq",
        "AS",
    ]
    df = pd.DataFrame(rows, columns=colnames)
    return df

# Define data paths
input_folder = Path("data_analysis/practical_data//results/mapping_counts/")
taxon_path = Path("data/accession_to_genus.tsv")
stats_path = Path("data_analysis/practical_data/results/seqkit/seqkit_chopper.tsv")
output_folder = Path("data_analysis/practical_data/results")
coverage_threshold = 0.9 
identity_threshold = 0.9 
mapq_threshold = 30

# Read in data
taxon_df = pd.read_csv(taxon_path, sep="\t")
if "tname" not in taxon_df.columns:
    taxon_df.columns = ["tname", "genus"]

all_files = list(input_folder.glob("*.paf"))
dfs = []

# On all files
for paf_file in all_files:
    barcode = paf_file.stem
    df = read_paf_with_AS(paf_file)
    df["sample"] = barcode
    dfs.append(df)

df = pd.concat(dfs, ignore_index=True)

df = df.copy()
numeric_cols = [
    "qlen",
    "qstart",
    "qend",
    "tlen",
    "tstart",
    "tend",
    "nmatch",
    "alen",
    "mapq",
    "AS",
]
df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors="coerce")

# Calculate query coverage and identity
df["qcov"] = (df.qend - df.qstart) / df.qlen
df["identity"] = df["nmatch"] / df["alen"].replace(0, pd.NA)

# add tax
df_tax = df.merge(taxon_df, left_on = "tname", right_on = 'tname')

# remove dubs 
df_best = df_tax.sort_values(
        ["qname", "identity", "AS", "mapq", "qcov", "alen"],
        ascending=[True, False, False, False, False, False],
    ).drop_duplicates("qname")

strepto_data = df_best[df_best['genus'] == 'Streptococcus']

# Search individual seqa 
# zcat results/chopper/*gz | grep -A2 "af394f78-8fc9-4075-aad8-e8284c38071e"
# grep "0000521d-cfa9-4914-b76c-3ff885795a61" results/mapping_counts/*paf
```

Notes:

- Streptomyces have a calculated coverage from 0.7 - 0.98 but the majority of hits have a coverage identity score of 0.8. Did some blastn searches
  - d3f03318-362f-4644-af15-c9c3c71a7083: 98% similar to Streptococcus salvarius ATCC 49124 (which makes sense with the calculated identity)
  - 000177cd-140f-4d44-87e5-d4e9db9a5c6f : 80% similar to stretococcus, blastn all hits to Blautia (at high id)
  - 00045f5e-d1fc-4b74-a827-834253091336: again, all hits to Blautia


## Analyse count table  (not started yet)

For this analysis,  relevant data, such as the `otu_table_genus.tsv` was moved into `../data/` in order to interactively analyse the data inside this notebook

### Read in the data

```{r}
library(tidyverse)
library(patchwork) 

df_wide <- read_tsv("data/otu_table_genus.tsv")
mapping <- read_csv("data/metadata.csv") 

#expected <- read_csv("data/expected.csv")
```

### Data wrangling

```{r}
df <- df_wide |>
    pivot_longer(
        cols = contains("bc"),
        names_to = "barcode",
        values_to = "counts"
    )

df <- df |> 
    group_by(barcode) |>
    mutate(rel_abundance = (counts / sum(counts)) * 100) |>
    ungroup() |>
    mutate(genus = fct_reorder(genus, rel_abundance, .fun = sum)) |>
    mutate(genus = fct_relevel(genus, "unassigned", after = 0))

df_all <- merge(df, mapping, by.x = "barcode", by.y = "unique_barcode", all.y = TRUE) 

# df_filtered <- df |>
#     inner_join(expected, by = c("culture", "genus")) |>
#     group_by(sample) |>
#     mutate(rel_abundance = (counts / sum(counts)) * 100) 

```


### Plotting

```{r}
df_h <- df_all |> filter(microbes_used == "h") 
df_s <- df_all |> filter(microbes_used == "s")

sample_counts_h <- df_h |>
    group_by(barcode, community, d) |>
    summarise(total = sum(counts), .groups = "drop")

sample_counts_s <- df_s |>
    group_by(barcode, community, d) |>
    summarise(total = sum(counts), .groups = "drop")


p1 <- ggplot(df_h, aes(x = barcode, y = rel_abundance)) +
    geom_col(aes(fill = genus), width = 0.95) +
    scale_fill_brewer(palette = "Paired", na.translate = FALSE, direction = -1) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.12)) ) +
    geom_text(
    data = sample_counts_h, 
    aes(x = barcode, y = 101, label = total),  # y = 1 puts labels above 100% stacked bars
    vjust = 0, hjust = 0, size = 3, angle = 80
    ) +
    labs(
        title = "Human-related strain interactions",
        y = "Relative abundance (%)",
        x = "",
        fill = "Genus"
    ) +
    facet_grid(cols = vars(community, d), scales = "free") +
    theme_classic()    +
    theme(
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    ) +
    guides(fill = guide_legend(reverse = TRUE))

p2 <- ggplot(df_s, aes(x = barcode, y = rel_abundance)) +
    geom_col(aes(fill = genus), width = 0.95) +
    scale_fill_brewer(palette = "Paired", na.translate = FALSE, direction = -1) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.12)) ) +
    geom_text(
    data = sample_counts_s, 
    aes(x = barcode, y = 101, label = total),  # y = 1 puts labels above 100% stacked bars
    vjust = 0, hjust = 0, size = 3, angle = 80
    ) +
    labs(
        title = "Human-related strain interactions",
        y = "Relative abundance (%)",
        x = "",
        fill = "Genus"
    ) +
    facet_grid(cols = vars(community, d), scales = "free") +
    theme_classic()    +
    theme(
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    ) +
    guides(fill = guide_legend(reverse = TRUE))

p1 / p2
```
